polars/docs/source/index.md
---
![logo](https://raw.githubusercontent.com/pola-rs/polars-static/master/banner/polars_github_banner.svg)
Blazingly Fast DataFrame Library    Rust docs latest   Rust crates Latest Release   PyPI Latest Release   DOI Latest Release  
Polars is a blazingly fast DataFrame library for manipulating structured data. The core is written
in Rust, and available for Python, R and NodeJS.
## Key features
- **Fast**: Written from scratch in Rust, designed close to the machine and without external
dependencies.
- **I/O**: First class support for all common data storage layers: local, cloud storage & databases.
- **Intuitive API**: Write your queries the way they were intended. Polars, internally, will
determine the most efficient way to execute using its query optimizer.
- **Out of Core**: The streaming API allows you to process your results without requiring all your
data to be in memory at the same time.
- **Parallel**: Utilises the power of your machine by dividing the workload among the available CPU
cores without any additional configuration.
- **Vectorized Query Engine**
- **GPU Support**: Optionally run queries on NVIDIA GPUs for maximum performance for in-memory
workloads.
- **[Apache Arrow support](https://arrow.apache.org/)**: Polars can consume and produce Arrow data
often with zero-copy operations. Note that Polars is not built on a Pyarrow/Arrow implementation.
Instead, Polars has its own compute and buffer implementations.
!!! info "Users new to DataFrames"
A DataFrame is a 2-dimensional data structure that is useful for data manipulation and analysis. With labeled axes for rows and columns, each column can contain different data types, making complex data operations such as merging and aggregation much easier. Due to their flexibility and intuitive way of storing and working with data, DataFrames have become increasingly popular in modern data analytics and engineering.
## Philosophy
The goal of Polars is to provide a lightning fast DataFrame library that:
- Utilizes all available cores on your machine.
- Optimizes queries to reduce unneeded work/memory allocations.
- Handles datasets much larger than your available RAM.
- A consistent and predictable API.
- Adheres to a strict schema (data-types should be known before running the query).
Polars is written in Rust which gives it C/C++ performance and allows it to fully control
performance-critical parts in a query engine.
## Example
{{code_block('home/example','example',['scan_csv','filter','group_by','collect'])}}
A more extensive introduction can be found in the [next chapter](user-guide/getting-started.md).
## Community
Polars has a very active community with frequent releases (approximately weekly). Below are some of
the top contributors to the project:
--8<-- "docs/assets/people.md"
## Contributing
We appreciate all contributions, from reporting bugs to implementing new features. Read our
[contributing guide](development/contributing/index.md) to learn more.
## License
This project is licensed under the terms of the
[MIT license](https://github.com/pola-rs/polars/blob/main/LICENSE).
---
polars/docs/source/_build/snippets/under_construction.md
---
!!! warning ":construction: Under Construction :construction:"
This section is still under development. Want to help out? Consider contributing and making a [pull request](https://github.com/pola-rs/polars) to our repository.
Please read our [contributing guide](https://docs.pola.rs/development/contributing/) on how to proceed.
---
polars/docs/source/polars-cloud/cli.md
---
# CLI
Polars cloud comes with a command line interface (CLI) out of the box. This allows you to interact
with polars cloud resources from the terminal.
```bash
pc --help
```
```
usage: pc [-h] [-v] [-V] {login,workspace,compute} ...
positional arguments:
{login,workspace,compute}
login Authenticate with Polars Cloud by logging in through the browser
workspace Manage Polars Cloud workspaces.
compute Manage Polars Cloud compute clusters.
options:
-h, --help show this help message and exit
-v, --verbose Output debug logging messages.
-V, --version Display the version of the Polars Cloud client.
```
### Authentication
You can authenticate with Polars Cloud from the CLI using
```bash
pc login
```
This refreshes your access token and saves it to disk.
### Workspaces
Create and setup a new workspace
```bash
pc workspace setup
```
List all workspaces
```bash
pc workspace list
```
```
NAME ID STATUS
test-workspace 0194ac0e-5122-7a90-af5e-b1f60b1989f4 Active
polars-ci-2025â€¦ 0194287a-e0a5-7642-8058-0f79a39f5b98 Uninitialized
```
### Compute
---
polars/docs/source/polars-cloud/connect-cloud.md
---
# Connect cloud environment
To use Polars Cloud, you must connect your workspaces to a cloud environment.
If you log in to the Polars Cloud dashboard for the first time with an account that isnâ€™t connected
to a cloud environment, you will see a blue bar at the top of the screen. You can explore Polars
Cloud in this state, but you wonâ€™t be able to execute any queries yet.
![An overview of the Polars Cloud dashboard showing a button to connect your cloud environment](https://raw.githubusercontent.com/pola-rs/polars-static/refs/heads/master/polars_cloud/connect-cloud/dashboard.png)
When you click the blue bar you will be redirected to the start of the set up flow.
## 1. Set workspace name
In the first step of the setup flow, youâ€™ll name your workspace. You can keep the default name
"Personal Workspace" or use the name of your team or department. This workspace name will be used by
the compute context to run your queries remotely.
!!! tip "Naming your workspace"
If youâ€™re unsure, you can use a temporary name. You can change the workspace name later under the workspace settings.
![Connect your cloud screen where you can input a workspace name](https://raw.githubusercontent.com/pola-rs/polars-static/refs/heads/master/polars_cloud/connect-cloud/workspace-naming.png)
## 2. Deploy to AWS
After naming your workspace, click Deploy to Amazon. This opens a screen in AWS with a
CloudFormation template. This template installs the necessary roles in your AWS environment.
![CloudFormation stack image as step of the setupflow](https://raw.githubusercontent.com/pola-rs/polars-static/refs/heads/master/polars_cloud/connect-cloud/cloudformation.png)
If you want to learn more about what Polars Cloud installs in your environment, you can read more on
[the AWS Infrastructure page](providers/aws/infra.md).
!!! info "No permissions to deploy the stack in AWS"
If you don't have the required permissions to deploy CloudFormation stacks in your AWS environment, you can copy the URL and share it with your operations team or someone with the permissions. With the URL they can deploy the stack for you.
## 3. Deploying the environment
After you click Create stack, the CloudFormation stack will be deployed in your environment. This
process usually takes around 5 minutes. You can monitor the progress in your AWS environment or in
the Polars setup flow.
![Progress screen in the set up flow](https://raw.githubusercontent.com/pola-rs/polars-static/refs/heads/master/polars_cloud/connect-cloud/progress-page.png)
When the CloudFormation stack deployment completes, youâ€™ll see a confirmation message.
![Final screen of the set up flow indication successful deployment](https://raw.githubusercontent.com/pola-rs/polars-static/refs/heads/master/polars_cloud/connect-cloud/successful-setup.png)
If you click "Start exploring", you will be redirected to the Polars Cloud dashboard.
You can now run your Polars query remotely in the cloud. Go to the
[getting started section](quickstart.md) to your first query in minutes,
[learn more how to run queries remote](run/compute-context.md) or manage your workspace to invite
your team.
!!! info "Only connect a workspace once"
You only have to connect your workspace once. If you invite your team to a workspace that is connected to a cloud environment they can immediately run queries remotely.
---
polars/docs/source/polars-cloud/faq.md
---
# FAQ
On this page you can find answers to some frequently asked questions around Polars Cloud.
## Who is behind Polars Cloud?
Polars Cloud is built by the organization behind the open source Polars project. We are committed to
improve Polars open source for all single machine workloads. Polars Cloud will extend Polars
functionalities for remote and distributed compute.
## Where does the compute run?
All compute runs in your own cloud environment. The main reason is that this ensures that your data
never leaves your environment and that the compute is always close to your data.
You can learn more about how this setup in
[the infrastructure section of the documentation](providers/aws/infra.md).
## Can you run Polars Cloud on-premise?
Currently, Polars Cloud is only available to organizations that are on AWS. Support for on-premise
infrastructure is on our roadmap and will become available soon.
## What does Polars Cloud offer me beyond Polars?
Polars Cloud offers a managed service that enables scalable data processing with the flexibility and
expressiveness of the Polars API. It extends the open source Polars project with the following
capabilities:
- Distributed engine to scale workloads horizontally.
- Cost-optimized serverless architecture that automatically scales compute resources
- Built-in fault tolerance mechanisms ensuring query completion even during hardware failures or
system interruptions
- Comprehensive monitoring and analytics tools providing detailed insights into query performance
and resource utilization.
## What are the main use cases for Polars Cloud?
Polars Cloud offers both a batch as an interactive mode to users. Batch mode can be used for ETL
workloads or one-off large scale analytic jobs. Interactive mode is for users that are looking to do
data exploration on a larger scale data processing that requires more compute than their own machine
can offer.
## How can Polars Cloud integrate with my workflow?
One of our key priorities is ensuring that running remote queries feels as native and seamless as
running them locally. Every user should be able to scale their queries effortlessly.
Polars Cloud is completely environment agnostic. This allows you to run your queries from anywhere
such as your own machine, Jupyter/Marimo notebooks, Airflow DAGs, AWS Lambda functions, or your
servers. By not tying you to a specific platform, Polars Cloud gives you the flexibility to execute
your queries wherever it best fits your workflow.
## What is the pricing model of Polars Cloud?
Polars Cloud is available at no additional cost in this early stage. You only pay for the resources
you use in your own cloud environment. We are exploring different usage based pricing models that
are geared towards running queries as fast and efficient as possible.
## Will the distributed engine be available in open source?
The distributed engine is only available in Polars Cloud. There are no plans to make it available in
the open source project. Polars is focused on single node compute, as it makes efficient use of the
available resources. Users already report utilizing Polars to process hundreds of gigabytes of data
on single (large) compute instance. The distributed engine is geared towards teams and organizations
that are I/O bound or want to scale their Polars queries beyond single machines.
---
polars/docs/source/polars-cloud/index.md
---
![Image showing the Polars Cloud logo](https://raw.githubusercontent.com/pola-rs/polars-static/refs/heads/master/polars_cloud/polars-cloud.svg)
# Introducing Polars Cloud
!!! tip "Polars Cloud is in alpha stage"
Polars Cloud is currently available to a select group of individuals and companies for early-stage testing. You can learn more about Polars Cloud and its goals in [our recent announcement post](https://pola.rs/posts/polars-cloud-what-we-are-building/).
DataFrame implementations always differed from SQL and databases. SQL could run anywhere from
embedded databases to massive data warehouses. Yet, DataFrame users have been forced to choose
between a solution for local work or solutions geared towards distributed computing, each with their
own APIs and limitations.
Polars is bridging this gap with **Polars Cloud**. Build on top of the popular open source project,
Polars Cloud enables you to write DataFrame code once and run it anywhere. The distributed engine
available with Polars Cloud allows to scale your Polars queries beyond a single machine.
## Key Features of Polars Cloud
- **Unified DataFrame Experience**: Run a Polars query seamlessly on your local machine and at scale
with our new distributed engine. All from the same API.
- **Serverless Compute**: Effortlessly start compute resources without managing infrastructure with
options to execute queries on both CPU and GPU.
- **Any Environment**: Start a remote query from a notebook on your machine, Airflow DAG, AWS
Lambda, or your server. Get the flexibility to embed Polars Cloud in any environment.
## Install Polars Cloud
Simply extend the capabilities of Polars with:
```bash
pip install polars polars_cloud
```
## Example query
To run your query in the cloud, simply write Polars queries like you are used to, but call
`LazyFrame.remote()` to indicate that the query should be run remotely.
{{code_block('polars-cloud/index','index',['ComputeContext','LazyFrameExt'])}}
## Sign up today and start for free
Polars Cloud is still in an early development stage and available at no additional cost. You only
pay for the resources you use in your own cloud environment.
## Cloud availability
Currently, Polars Cloud is available on AWS. Other cloud providers and on-premise solutions are on
the roadmap and will become available in the upcoming months.
![AWS logo](https://raw.githubusercontent.com/pola-rs/polars-static/refs/heads/master/polars_cloud/aws-logo.svg)
---
polars/docs/source/polars-cloud/quickstart.md
---
# Getting started
Polars Cloud is a managed compute platform for your Polars queries. It allows you to effortlessly
run your local queries in your cloud environment, both in an interactive setting as well as for ETL
or batch jobs. By working in a 'Bring your own Cloud' model the data never leaves your environment.
## Installation
Install the Polars Cloud python library in your environment
```bash
$ pip install polars polars-cloud
```
Create an account and login by running the command below.
```bash
$ pc login
```
## Connect your cloud
Polars Cloud currently exclusively supports AWS as a cloud provider.
Polars Cloud needs permission to manage hardware in your environment. This is done by deploying our
cloudformation template. See our [infrastructure](providers/aws/infra.md) section for more details.
To connect your cloud run:
```bash
$ pc setup workspace -n 
```
This redirects you to the browser where you can connect Polars Cloud to your AWS environment.
Alternatively, you can follow the steps in the browser and create the workspace there.
## Run your queries
Now that we are done with the setup, we can start running queries. The general principle here is
writing Polars like you're always used to and calling `.remote()` on your `LazyFrame`. The following
example shows how to create a compute cluster and run a simple Polars query.
{{code_block('polars-cloud/quickstart','general',['ComputeContext','LazyFrameExt'])}}
Let us go through the code line by line. First we need to define the hardware the cluster will run
on. This can be provided in terms of cpu & memory or by specifying the the exact instance type in
AWS.
```python
ctx = pc.ComputeContext(memory=8, cpus=2 , cluster_size=1)
```
Then we write a regular lazy Polars query. In this simple example we compute the maximum of column
`a` over column `b`.
```python
df = pl.LazyFrame({
"a": [1, 2, 3],
"b": [4, 4, 5]
})
lf = df.with_columns(
c = pl.col("a").max().over("b")
)
```
Finally we are going to run our query on the compute cluster. We use `.remote()` to signify that we
want to run the query remotely. This gives back a special version of the `LazyFrame` with extension
methods. Up until this point nothing has executed yet, calling `.write_parquet()` sends the query to
g
---
polars/docs/source/polars-cloud/release-notes.md
---
# Release Notes
## Polars Cloud 0.0.7 (2025-04-11)
### Enhancements
- We now default to the streaming engine for distributed workloads A big benefit is that query
results are now streamed directly to final S3/external storage instead of being collected into
memory first. This enables handling much larger results without causing out-of-memory errors.
- Polars Cloud now supports organizations. An organization can contain multiple workspaces and will
eventually serve as the central entity for managing user roles and billing. During migration, an
organization was automatically created for each of your workspaces. If you have multiple
workspaces with the same name, you can now select one by specifying the organization:
```python
workspace = Workspace("my_workspace", organization="my_organization")
context = ComputeContext(workspace=workspace)
```
- Polars Cloud login tokens are now automatically refreshed for up to 8 hours after the initial
login. No more need to login every 15 minutes.
- A clear error message is now shown when connecting to Polars Cloud with an out-of-date client
version.
- The portal now has a feedback button in the bottom right corner so you can easily report feedback
to us. We greatly appreciate it, so please use it as much as you would like.
- Distributed queries now display the physical plan in addition to the logical plan. The logical
plan is the optimized query and how it would be run on a single node. The physical plan represents
how we chop this up into stages and tasks so that it can be run on multiple nodes in a distributed
manner.
- The workspace compute listing page has been reworked for improved clarity and shows more
information.
### Bug fixes
- Improved out-of-memory reporting
- Ctrl-C now works to exit during login
- Workspace members can now be re-invited if they were previously removed
## Polars Cloud 0.0.6 (2025-03-31)
This version of Polars Cloud is only compatible with the 1.26.x releases of Polars Open Source.
### Enhancements
- Added support for [Polars IO plugins](https://docs.pola.rs/user-guide/plugins/io_plugins/).
- These allow you to register different file formats as sources to the Polars engines which allows
you to benefit from optimizations like projection pushdown, predicate pushdown, early stopping
and support of our streaming engine.
- Added `.show()` for remote queries
- You can now call `.remote().show()` instead of `remote().limit().collect().collect()`
- All Polars features that rely on external Python dependencies are now available in Polars Cloud
such as:
- `scan_iceberg()`
- `scan_delta()`
- `read_excel()`
- `read_database()`
### Bug fixes
- Fixed an issue where specifying CPU/memory minimums could select an incompatible legacy EC2
instance.
- The API now returns clear errors when trying to start a query on an uninitialized workspace.
- Fixed a 404 error when opening a query overview page in a workspace different from the currently
selected workspace.
- Viewing another workspace members compute detail page no longer shows a blank screen.
---
polars/docs/source/polars-cloud/run/compute-context.md
---
# Defining a compute context
The compute context defines the hardware configuration used to execute your queries. This can be
either a single node or, for distributed execution, multiple nodes. This section explains how to set
up and manage your compute context.
{{code_block('polars-cloud/compute-context','compute',['ComputeContext'])}}
## Setting the context
You can define your compute context in three ways:
1. Use your workspace default
2. Specify CPUs and RAM requirements
3. Select a specific instance type
### Workspace default
In the Polars Cloud dashboard, you can set default requirements from your cloud service provider to
be used for all queries. You can also manually define storage and the default cluster size.
Polars Cloud will use these defaults if no other parameters are passed to the `ComputeContext`.
{{code_block('polars-cloud/compute-context','default-compute',['ComputeContext'])}}
Find out more about how to [set workspace defaults](../workspace/settings.md) in the workspace
settings section.
### Define hardware specifications
You can directly specify the `cpus` and `memory` requirements in your `ComputeContext`. When set,
Polars Cloud will select the most suitable instance type from your cloud service provider that meets
the specifications. The requirements are lower bounds, meaning the machine will have at least that
number of CPUs and memory.
{{code_block('polars-cloud/compute-context','defined-compute',['ComputeContext'])}}
### Set instance type
For more control, you can specify the exact instance type for Polars to use. This is useful when you
have specific hardware requirements in a production environment.
{{code_block('polars-cloud/compute-context','set-compute',['ComputeContext'])}}
## Applying the compute context
Once defined, you can apply your compute context to queries in three ways:
1. By directly passing the context to the remote query:
```python
query.remote(context=ctx).sink_parquet(...)
```
2. By globally setting the compute context. This way you set it once and don't need to provide it to
every `remote` call:
```python
pc.set_compute_context(ctx)
query.remote().sink_parquet(...)
```
3. When a default compute context is set via the Polars Cloud dashboard. It is no longer required to
define a compoute context.
```python
query.remote().sink_parquet(...)
```
---
polars/docs/source/polars-cloud/run/distributed-engine.md
---
# Distributed query execution
With the introduction of Polars Cloud, we also introduced the distributed engine. This engine
enables users to horizontally scale workloads across multiple machines.
Polars has always been optimized for fast and efficient performance on a single machine. However,
when querying large datasets from cloud storage, performance is often constrained by the I/O
limitations of a single node. By scaling horizontally, these download limitations can be
significantly reduced, allowing users to process data at scale.
!!! info "Distributed engine is early stage"
The distributed engine is still in the very early stages of development. Major performance improvements are planned for the near future. When an operation is not yet available in a distributed manner, Polars Cloud will execute it on a single node.
Find out which operations are [currently supported in the distributed engine](https://github.com/pola-rs/polars/issues/21487).
## Using distributed engine
To execute queries using the distributed engine, you can call the `distributed()` method.
```python
lf: LazyFrame
result = (
lf.remote()
.distributed()
.collect()
)
```
### Example
```python
import polars as pl
import polars_cloud as pc
from datetime import date
query = (
pl.scan_parquet("s3://dataset/")
.filter(pl.col("l_shipdate") <= date(1998, 9, 2))
.group_by("l_returnflag", "l_linestatus")
.agg(
avg_price=pl.mean("l_extendedprice"),
avg_disc=pl.mean("l_discount"),
count_order=pl.len(),
)
)
result = (
query.remote(pc.ComputeContext(cpus=16, memory=64, cluster_size=32))
.distributed()
.sink_parquet("s3://output/result.parquet")
)
```
## Working with large datasets in the distributed engine
The distributed engine can only read sources partitioned with direct scan_ methods such as
`scan_parquet` and `scan_csv`. Open table formats like `scan_iceberg` are not yet supported in a
distributed fashion and will run on a single node when utilized.
---
polars/docs/source/polars-cloud/run/interactive-batch.md
---
# Interactive or batch mode
In Polars Cloud, a user can define two types of compute modes: batch & interactive. Batch mode is
designed for batch job style queries. These kinds of queries are typically scheduled and run once in
a certain period. Interactive mode allows for exploratory workflows where a user interacts with the
dataset and requires more compute resources than are locally available.
The rest of this page will give examples on how to set up one or the other. More information on the
architectural differences and implications can be found on
[the infrastructure page](../providers/aws/infra.md).
Below we create a simple dataframe to use as an example to demonstrate the difference between both
modes.
{{code_block('polars-cloud/interactive-batch','example',[])}}
## Batch
Batch workflows are systematic data processing pipelines, written for scheduled execution. They
typically process large volumes of data in scheduled intervals (e.g. hourly, daily, etc.). A key
characteristic is that the executed job has a defined lifetime. A predefined compute instance should
spin up at a certain time and shut down when the job is executed.
Polars Cloud makes it easy to run your query at scale whenever your use case requires it. You can
develop your query on your local machine and define a compute context and destination to execute it
in your cloud environment.
{{code_block('polars-cloud/interactive-batch','batch',['ComputeContext'])}}
The query you execute in batch mode runs in your cloud environment. The data and results of the
query are not sent to Polars Cloud, ensuring that your data and output remain secure.
```python
lf.remote(context=ctx).sink_parquet("s3://bucket/output.parquet")
```
## Interactive
Polars Cloud also supports interactive workflows. Different from batch mode, results are being
interactively updated. Polars Cloud will not automatically close the cluster when a result has been
produced, but the cluster stays active and intermediate state can still be accessed. In interactive
mode you directly communicate with the compute nodes.
Because this mode is used for exploratory use cases and short feedback cycles, the queries are not
logged to Polars Cloud and will not be available for later inspection.
{{code_block('polars-cloud/interactive-batch','interactive',['ComputeContext'])}}
The initial query remains the same. In the compute context the parameter `interactive` should be set
to `True`.
When calling `.collect()` on your remote query execution, the output is written to a temporary
location. These intermediate result files are automatically deleted after several hours. The output
of the remote query is a LazyFrame.
```python
print(type(res1))
```
```
```
If you want to inspect the results you can call collect again.
```python
print(res1.collect())
```
```text
shape: (4, 3)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ name â”† birth_year â”† bmi â”‚
â”‚ --- â”† --- â”† --- â”‚
â”‚ str â”† i32 â”† f64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ Chloe Cooper â”† 1983 â”† 19.687787 â”‚
â”‚ Ben Brown â”† 1985 â”† 23.141498 â”‚
â”‚ Alice Archer â”† 1997 â”† 23.791913 â”‚
â”‚ Daniel Donovan â”† 1981 â”† 27.134694 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
To continue your exploration you can use the returned LazyFrame to build another query.
{{code_block('polars-cloud/interactive-batch','interactive-next',[])}}
```python
print(res2.collect())
```
```text
shape: (2, 3)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ name â”† birth_year â”† bmi â”‚
â”‚ --- â”† --- â”† --- â”‚
â”‚ str â”† i32 â”† f64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ Chloe Cooper â”† 1983 â”† 19.687787 â”‚
â”‚ Ben Brown â”† 1985 â”† 23.141498 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
---
polars/docs/source/polars-cloud/run/service-accounts.md
---
# Using service accounts
Service accounts function as programmatic identities that enable secure machine-to-machine
communication for remote data processing workflows. These accounts facilitate automated processes
(typically in production environments) without requiring interactive login sessions.
## Create a service account
In the workspace settings page, navigate to the Service Accounts section. Here, you can create and
manage service accounts associated with your workspace.
To create a new Service Account:
1. Click the Create New Service Account button
2. Provide a name and description for the account
3. Copy and securely store the Client ID and Client Secret
!!! info "Client ID and Secret visible once"
If you lose the Client ID or Client Secret, you will need to generate a new service account.
## Set up your environment to use a Service Account
To authenticate using a Service Account, you must set environment variables. The following variables
should be defined using the credentials provided when the service account was created:
```bash
export POLARS_CLOUD_CLIENT_ID="CLIENT_ID_HERE"
export POLARS_CLOUD_CLIENT_SECRET="CLIENT_SECRET_HERE"
```
## Execute query with Service Account
Once the environment variables are set, you do not need to log in to Polars Cloud manually. Your
process will automatically connect to the workspace.
## Revoking access or deleting a Service Account
When a service account is no longer needed, it is recommended to revoke its access. Keep in mind
that deleting a service account is irreversible, and any processes or applications relying on this
account will lose access immediately.
1. Navigate to the Workspace Settings page.
2. Go to the Service Accounts section.
3. Locate the service account to delete.
4. Click the three-dot menu next to the service account and select Delete.
5. Confirm the deletion of the Service Account.
---
polars/docs/source/polars-cloud/run/workflow.md
---
# From local to cloud query execution
Data processing and analytics often begins small but can quickly grow beyond the capabilities of
your local machine. A typical workflow starts with exploring a sample dataset locally, developing
the analytical approach, and then scaling up to process the full dataset in the cloud.
This pattern allows you to iterate quickly during development while still handling larger datasets
in production. With Polars Cloud, you can maintain this natural workflow without rewriting your code
when moving from local to cloud execution, without requiring any migrations between local and
production tooling.
## Local exploration
For this workflow, we define the following simple mocked dataset that will act as a sample to
demonstrate the workflow. Here we will create the LazyFrame ourselves, but it could also be read as
(remote) file.
```python
import polars as pl
lf = pl.LazyFrame(
{
"region": [
"Australia",
"California",
"Benelux",
"Siberia",
"Mediterranean",
"Congo",
"Borneo",
],
"temperature": [32.1, 28.5, 30.2, 22.7, 29.3, 31.8, 33.2],
"humidity": [40, 35, 75, 30, 45, 80, 70],
"burn_area": [120, 85, 30, 65, 95, 25, 40],
"vegetation_density": [0.6, 0.7, 0.9, 0.4, 0.5, 0.9, 0.8],
}
)
```
A simple transformation will done to create a new column.
```python
(
lf.with_columns(
(
(pl.col("temperature") / 10)
* (1 - pl.col("humidity") / 100)
* pl.col("vegetation_density")
).alias("fire_risk"),
).filter(pl.col("humidity") < 70)
.sort(by="fire_risk", descending=True)
.collect()
)
```
```text
shape: (4, 6)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ region â”† temperature â”† humidity â”† burn_area â”† vegetation_density â”† fire_risk â”‚
â”‚ --- â”† --- â”† --- â”† --- â”† --- â”† --- â”‚
â”‚ str â”† f64 â”† i64 â”† i64 â”† f64 â”† f64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ California â”† 28.5 â”† 35 â”† 85 â”† 0.7 â”† 1.29675 â”‚
â”‚ Australia â”† 32.1 â”† 40 â”† 120 â”† 0.6 â”† 1.1556 â”‚
â”‚ Mediterranean â”† 29.3 â”† 45 â”† 95 â”† 0.5 â”† 0.80575 â”‚
â”‚ Siberia â”† 22.7 â”† 30 â”† 65 â”† 0.4 â”† 0.6356 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## Run at scale in the cloud
Imagine that there is a larger dataset stored in a cloud providerâ€™s storage solution. The dataset is
so large that it doesnâ€™t fit on our local machine. However, through local analysis, we have verified
that the defined query correctly calculates the column we are looking for.
With Polars Cloud, we can easily run the same query at scale. First, we make small changes to our
query to point to our resources in the cloud.
```python
lf = pl.scan_parquet("s3://climate-data/global/*.parquet")
query = (
lf.with_columns(
[
(
(pl.col("temperature") / 10)
* (1 - pl.col("humidity") / 100)
* pl.col("vegetation_density")
).alias("fire_risk"),
]
)
.filter(pl.col("humidity") < 70)
.sort(by="fire_risk", descending=True)
)
```
Next, we set our compute context and call `.remote(context=ctx)` on our query.
```python
import polars_cloud as pc
ctx = pc.ComputeContext(
workspace="environmental-analysis",
memory=32,
cpus=8
)
query.remote(context=ctx).sink_parquet("s3://bucket/result.parquet")
```
### Continue analysis in interactive mode
Running `.sink_parquet()` will write the results to the defined bucket on S3. Alternatively, we can
take a more interactive approach by adding the parameter `interactive=True` to our compute context.
```python
ctx = pc.ComputeContext(
workspace="environmental-analysis",
memory=32,
cpus=8,
interactive=True, # set interactive to True
)
result = query.remote(context=ctx).collect()
print(result.collect())
```
```text
shape: (4, 6)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ region â”† temperature â”† humidity â”† burn_area â”† vegetation_density â”† fire_risk â”‚
â”‚ --- â”† --- â”† --- â”† --- â”† --- â”† --- â”‚
â”‚ str â”† f64 â”† i64 â”† i64 â”† f64 â”† f64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ California â”† 28.5 â”† 35 â”† 85 â”† 0.7 â”† 1.29675 â”‚
â”‚ Australia â”† 32.1 â”† 40 â”† 120 â”† 0.6 â”† 1.1556 â”‚
â”‚ Mediterranean â”† 29.3 â”† 45 â”† 95 â”† 0.5 â”† 0.80575 â”‚
â”‚ Siberia â”† 22.7 â”† 30 â”† 65 â”† 0.4 â”† 0.6356 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
We can call `.collect()` instead of `.sink_parquet()`. This will store your results to a temporary
location which can be used to further iterate upon. A LazyFrame is returned that can be used in the
next steps of the workflow.
```python
res2 = (
result
.filter(pl.col("fire_risk") > 1)
.sink_parquet("s3://bucket/output-interactive.parquet")
)
```
The result of your interactive workflow can be written to S3.
---
polars/docs/source/polars-cloud/providers/aws/infra.md
---
# Infrastructure
Polars Cloud manages the hardware for you by spinning up and down raw EC2 instances. In order to do
this it needs permissions in your own cloud environment. None of the resources below have costs
associated with them. While no compute clusters are running Polars Cloud will not create any AWS
costs. The recommended way of doing this is running `pc workspace setup`.
## Recommended setup
When you deploy Polars Cloud the following infrastructure is setup.
![AWS infrastructure](https://raw.githubusercontent.com/pola-rs/polars-static/refs/heads/master/polars_cloud/aws-infra.png)
1. A `VPC` and `subnet` in which Polars EC2 workers can run.
1. Two `security groups`. One for batch mode, which does not have any public ports, and one for
interactive mode, which allows direct communication between your local environment and the
cluster.
1. `PolarsWorker` IAM role. Polars EC2 workers run under this IAM role.
1. `UserInitiated` & `Unattended` IAM role. The `UserInitiated` role has the permissions to start
Polars EC2 workers in your environment. The `Unattended` role can terminate unused compute
clusters that you might have forgot about.
## Security
By design Polars Cloud never has access to the data inside your cloud environment. The data never
leaves your environment.
### IAM permissions
The list below show an overview of the required permissions for each of the roles.
??? User Initiated
- ec2:CreateTags
- ec2:RunInstances
- ec2:DescribeInstances
- ec2:DescribeInstanceTypeOfferings
- ec2:DescribeInstanceTypes
- ec2:TerminateInstances
- ec2:CreateFleet
- ec2:CreateLaunchTemplate
- ec2:CreateLaunchTemplateVersion
- ec2:DescribeLaunchTemplates
??? Unattended
- ec2:DescribeInstances
- ec2:TerminateInstances
- ec2:DescribeFleets
- ec2:DeleteLaunchTemplate
- ec2:DeleteLaunchTemplateVersions
- ec2:DeleteFleets
- sts:GetCallerIdentity
- sts:TagSession
- cloudwatch:GetMetricData
- logs:GetLogEvents
- logs:FilterLogEvents
- logs:DescribeLogStreams
??? Worker
- logs:CreateLogGroup
- logs:PutRetentionPolicy
- cloudwatch:PutMetricData
## Custom setup
Depending on your enterprise needs or existing infrastructure, you may not require certain
components (e.g. VPC, subnet) of the default setup of Polars Cloud. Or you have additional security
requirements in place. Together with our team of engineers we can integrate Polars Cloud with your
existing infrastructure. Please contact us directly.
---
polars/docs/source/polars-cloud/providers/aws/permissions.md
---
# Permissions
The workspace is an isolation for all resources living within your cloud environment. Every
workspace has a single instance profile which defines the permissions for the compute. This profile
is attached to the compute within your environment. By default, the profile can read and write from
S3, but you can easily adjust depending on your own infrastructure stack.
## Adding or removing permissions
If you want Polars Cloud to be able to read from other data sources than `S3` within your cloud
environment you must provide the access control from directly within AWS. To do this go to `IAM`
within the aws console and locate the role called `polars--IAMWorkerRole-`.
Here you can adjust the permissions of the workspace for instance:
- [Narrow down the S3 access to certain buckets](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_deny-except-bucket.html)
- [Provide IAM access to rds database](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.IAMPolicy.html)
---
polars/docs/source/polars-cloud/workspace/settings.md
---
# Configuring workspace settings
The Workspace Settings page provides a centralized interface for managing your workspace
configuration.
## General
The General section contains basic information about your workspace:
- **Workspace Name**: Displays the current name of your workspace. You can modify this by clicking
the "Edit" button.
- **Description**: Provides space for a detailed description of your workspace's purpose. The
description can be modified by clicking the "Edit" button.
## Default Compute Configuration
This section allows you to define the default computational resources allocated to jobs and
processes within your workspace.
- **Set Cluster Defaults**: Clicking this button lets you configure either a Resource based default
or Instance based default.
- With Resource based custom amount of vCPUs, RAM, Storage and cluster size can be defined.
- Instance based allows to select a default instance type from the cloud service provider.
Setting default compute configurations eliminates the need to explicitly define a compute context.
More information on configuration can be found in the section on
[setting up compute context](../run/compute-context.md).
## Query and compute labels
Labels help organize and categorize queries and compute within your workspace. The labels can only
be set from the dashboard.
```python
ctx= plc.ComputeContext(
workspace="PolarsCloudDemo",
labels=["marketing", "cltv"],
)
```
- **Create New Label**: Create custom labels that can be applied to various resources for better
organization and insight in usage.
## Service Accounts
The Service Accounts section manages programmatic access to your workspace.
- **Token Description**: Displays the purpose of each service account token.
- **Created At**: Shows when a service account was created.
- **Action**: Provides options to delete service accounts.
- **Create New Service Account**: Allows you to generate new service accounts for API access and
automation.
You can find more information about using Service Accounts in your workflows in the "Use service
accounts" section.
## Disable workspace
Users have the ability to disable the entire workspace. This should be used with caution. Disabling
the workspace will stop processes, remove access and permanently delete all data and content within
the workspace.
---
polars/docs/source/polars-cloud/workspace/team.md
---
# Manage workspace members
The Team page serves as a central hub for managing who has access to the workspace. Administrators
are able to invite collaborators, monitor membership status, and manage user permissions.
## Adding your team to the workspace
### Email invitations
The primary method for adding team members is through email invitations:
1. Enter the email address of the person you wish to add in the provided input field
2. Click "Send email" to deliver an invitation directly to their inbox
Invitees will receive in invitation email that they can use to register and get added to the
workspace.
### Invitation links
With an invite link you can share the personal invitation link with an invitee.
1. Click "Generate invite link" to create a shareable URL
2. Copy and share the link to the invitee.
The invitee can use this link to join the workspace without requiring an email invitation.
!!! info "Inviting existing users"
Users that are already part of another workspace will get a prompt to join the workspace they are invited for.
---
polars/docs/source/polars-cloud/integrations/airflow.md
---
---
polars/docs/source/polars-cloud/explain/authentication.md
---
# Logging in
Polars cloud allows authentication through short-lived authentication tokens. There are two ways you
can obtain an access token:
- command line interface
- python client
After a successful `login` Polars Cloud stores the token in `{$HOME}/.polars`. You can alter this
path by setting the environment variable `POLARS_CLOUD_ACCESS_TOKEN_PATH`.
### Command Line Interface (CLI)
Authenticate with CLI using the following command
```bash
pc login
```
### Python client
Authenticate with the Polars Cloud using
{{code_block('polars-cloud/authentication','login',['login'])}}
Both methods redirect you to the browser where you can provide your login credentials and continue
the sign in process.
## Service accounts
Both flows described above are for interactive logins where a person is present in the process. For
non-interactive workflows such as orchestration tools there are service accounts. These allow you to
login programmatically.
To create a service account go to the Polars Cloud dashboard under Settings and service accounts.
Here you can create a new service account for your workspace. To authenticate set the
`POLARS_CLOUD_CLIENT_ID` and `POLARS_CLOUD_CLIENT_SECRET` environment variables. Polars Cloud will
automatically pick these up if there are no access tokens present in the path.
---
polars/docs/source/development/versioning.md
---
# Versioning
## Version changes
Polars adheres to the [semantic versioning](https://semver.org/) specification:
- Breaking changes lead to a **major** version increase (`1.0.0`, `2.0.0`, ...)
- New features and performance improvements lead to a **minor** version increase (`1.1.0`, `1.2.0`,
...)
- Other changes lead to a **patch** version increase (`1.0.1`, `1.0.2`, ...)
## Policy for breaking changes
Polars takes backwards compatibility seriously, but we are not afraid to change things if it leads
to a better product.
### Philosophy
We don't always get it right on the first try. We learn as we go along and get feedback from our
users. Sometimes, we're a little too eager to get out a new feature and didn't ponder all the
possible implications.
If this happens, we correct our mistakes and introduce a breaking change. Most of the time, this is
no big deal. Users get a deprecation warning, they do a quick search-and-replace in their code base,
and that's that.
At times, we run into an issue requires more effort on our user's part to fix. A change in the query
engine can seriously impact the assumptions in a data pipeline. We do not make such changes lightly,
but we will make them if we believe it makes Polars better.
Freeing ourselves of past indiscretions is important to keep Polars moving forward. We know it takes
time and energy for our users to keep up with new releases but, in the end, it benefits everyone for
Polars to be the best product possible.
### What qualifies as a breaking change
**A breaking change occurs when an existing component of the public API is changed or removed.**
A feature is part of the public API if it is documented in the
[API reference](https://docs.pola.rs/api/python/stable/reference/index.html).
Examples of breaking changes:
- A deprecated function or method is removed.
- The default value of a parameter is changed.
- The outcome of a query has changed due to changes to the query engine.
Examples of changes that are _not_ considered breaking:
- An undocumented function is removed.
- The module path of a public class is changed.
- An optional parameter is added to an existing method.
Bug fixes are not considered a breaking change, even though it may impact some users'
[workflows](https://xkcd.com/1172/).
### Unstable functionality
Some parts of the public API are marked as **unstable**. You can recognize this functionality from
the warning in the API reference, or from the warning issued when the configuration option
`warn_unstable` is active. There are a number of reasons functionality may be marked as unstable:
- We are unsure about the exact API. The name, function signature, or implementation are likely to
change in the future.
- The functionality is not tested extensively yet. Bugs may pop up when used in real-world
scenarios.
- The functionality does not yet integrate well with the full Polars API. You may find it works in
one context but not in another.
Releasing functionality as unstable allows us to gather important feedback from users that use
Polars in real-world scenarios. This helps us fine-tune things before giving it our final stamp of
approval. Users that are only interested in solid, well-tested functionality can avoid this part of
the API.
Functionality marked as unstable may change at any point without it being considered a breaking
change.
### Deprecation warnings
If we decide to introduce a breaking change, the existing behavior is deprecated _if possible_. For
example, if we choose to rename a function, the new function is added alongside the old function,
and using the old function will result in a deprecation warning.
Not all changes can be deprecated nicely. A change to the query engine may have effects across a
large part of the API. Such changes will not be warned for, but _will_ be included in the changelog
and the migration guide.
!!! warning Rust users only
Breaking changes to the Rust API are not deprecated first, but _will_ be listed in the changelog.
Supporting deprecated functionality would slow down development too much at this point in time.
### Deprecation period
As a rule, deprecated functionality is removed two breaking releases after the deprecation happens.
For example, a function deprecated in version `1.2.3` will be retained in version `2.0.0` and
removed in version `3.0.0`.
An exception to this rule are deprecations introduced with a breaking release. These will be
enforced on the next breaking release. For example, a function deprecated in version `2.0.0` will be
removed in version `3.0.0`.
This means that if your program does not raise any deprecation warnings, it should be mostly safe to
upgrade to the next major version. As breaking releases happen about once every six months, this
allows six to twelve months to adjust to any pending breaking changes.
**In some cases, we may decide to adjust the deprecation period.** If retaining the deprecated
functionality blocks other improvements to Polars, we may shorten the deprecation period to a single
breaking release. This will be mentioned in the warning message. If the deprecation affects many
users, we may extend the deprecation period.
## Release frequency
Polars does not have a set release schedule. We issue a new release whenever we feel like we have
something new and valuable to offer to our users. In practice, a new minor version is released about
once every one or two weeks.
### Breaking releases
Over time, issues pop up that require a breaking change to address. When enough issues have
accumulated, we issue a breaking release.
So far, breaking releases have happened about once every three to six months. The rate and severity
of breaking changes will continue to diminish as Polars grows more solid. From this point on, we
expect new major versions to be released about once every six months.
---
polars/docs/source/development/contributing/ci.md
---
# Continuous integration
Polars uses GitHub Actions as its continuous integration (CI) tool. The setup is reasonably complex,
as far as CI setups go. This page explains some of the design choices.
## Goal
Overall, the CI suite aims to achieve the following:
- Enforce code correctness by running automated tests.
- Enforce code quality by running automated linting checks.
- Enforce code performance by running benchmark tests.
- Enforce that code is properly documented.
- Allow maintainers to easily publish new releases.
We rely on a wide range of tools to achieve this for both the Rust and the Python code base, and
thus a lot of checks are triggered on each pull request.
It's entirely possible that you submit a relatively trivial fix that subsequently fails a bunch of
checks. Do not despair - check the logs to see what went wrong and try to fix it. You can run the
failing command locally to verify that everything works correctly. If you can't figure it out, ask a
maintainer for help!
## Design
The CI setup is designed with the following requirements in mind:
- Get feedback on each step individually. We want to avoid our test job being cancelled because a
linting check failed, only to find out later that we also have a failing test.
- Get feedback on each check as quickly as possible. We want to be able to iterate quickly if it
turns out our code does not pass some of the checks.
- Only run checks when they need to be run. A change to the Rust code does not warrant a linting
check of the Python code, for example.
This results in a modular setup with many separate workflows and jobs that rely heavily on caching.
### Modular setup
The repository consists of two main parts: the Rust code base and the Python code base. Both code
bases are interdependent: Rust code is tested through Python tests, and the Python code relies on
the Rust implementation for most functionality.
To make sure CI jobs are only run when they need to be run, each workflow is triggered only when
relevant files are modified.
### Caching
The main challenge is that the Rust code base for Polars is quite large, and consequently, compiling
the project from scratch is slow. This is addressed by caching the Rust build artifacts.
However, since GitHub Actions does not allow sharing caches between feature branches, we need to run
the workflows on the main branch as well - at least the part that builds the Rust cache. This leads
to many workflows that trigger both on pull request AND on push to the main branch, with individual
steps of jobs enabled or disabled based on the branch it runs on.
Care must also be taken not to exceed the maximum cache space of 10Gb allotted to open source GitHub
repositories. Hence we do not do any caching on feature branches - we always use the cache available
from the main branch. This also avoids any extra time that would be required to store the cache.
## Releases
The release jobs for Rust and Python are triggered manually. Refer to the
[contributing guide](./index.md#release-flow) for the full release process.
---
polars/docs/source/development/contributing/code-style.md
---
# Code style
This page contains some guidance on code style.
!!! info
Additional information will be added to this page later.
## Rust
### Naming conventions
Naming conventions for variables:
```rust
let s: Series = ...
let ca: ChunkedArray = ...
let arr: ArrayRef = ...
let arr: PrimitiveArray = ...
let dtype: DataType = ...
let dtype: ArrowDataType = ...
```
### Code example
```rust
use std::ops::Add;
use polars::export::arrow::array::*;
use polars::export::arrow::compute::arity::binary;
use polars::export::arrow::types::NativeType;
use polars::prelude::*;
use polars_core::utils::{align_chunks_binary, combine_validities_and};
use polars_core::with_match_physical_numeric_polars_type;
// Prefer to do the compute closest to the arrow arrays.
// this will tend to be faster as iterators can work directly on slices and don't have
// to go through boxed traits
fn compute_kernel(arr_1: &PrimitiveArray, arr_2: &PrimitiveArray) -> PrimitiveArray
where
T: Add + NativeType,
{
// process the null data separately
// this saves an expensive branch and bitoperation when iterating
let validity_1 = arr_1.validity();
let validity_2 = arr_2.validity();
let validity = combine_validities_and(validity_1, validity_2);
// process the numerical data as if there were no validities
let values_1: &[T] = arr_1.values().as_slice();
let values_2: &[T] = arr_2.values().as_slice();
let values = values_1
.iter()
.zip(values_2)
.map(|(a, b)| *a + *b)
.collect::>();
PrimitiveArray::from_data_default(values.into(), validity)
}
// Same kernel as above, but uses the `binary` abstraction. Prefer this,
#[allow(dead_code)]
fn compute_kernel2(arr_1: &PrimitiveArray, arr_2: &PrimitiveArray) -> PrimitiveArray
where
T: Add + NativeType,
{
binary(arr_1, arr_2, arr_1.dtype().clone(), |a, b| a + b)
}
fn compute_chunked_array_2_args(
ca_1: &ChunkedArray,
ca_2: &ChunkedArray,
) -> ChunkedArray {
// This ensures both ChunkedArrays have the same number of chunks with the
// same offset and the same length.
let (ca_1, ca_2) = align_chunks_binary(ca_1, ca_2);
let chunks = ca_1
.downcast_iter()
.zip(ca_2.downcast_iter())
.map(|(arr_1, arr_2)| compute_kernel(arr_1, arr_2));
ChunkedArray::from_chunk_iter(ca_1.name(), chunks)
}
pub fn compute_expr_2_args(arg_1: &Series, arg_2: &Series) -> Series {
// Dispatch the numerical series to `compute_chunked_array_2_args`.
with_match_physical_numeric_polars_type!(arg_1.dtype(), |$T| {
let ca_1: &ChunkedArray<$T> = arg_1.as_ref().as_ref().as_ref();
let ca_2: &ChunkedArray<$T> = arg_2.as_ref().as_ref().as_ref();
compute_chunked_array_2_args(ca_1, ca_2).into_series()
})
}
```
---
polars/docs/source/development/contributing/ide.md
---
# IDE configuration
Using an integrated development environments (IDE) and configuring it properly will help you work on
Polars more effectively. This page contains some recommendations for configuring popular IDEs.
## Visual Studio Code
Make sure to configure VSCode to use the virtual environment created by the Makefile.
### Extensions
The extensions below are recommended.
#### rust-analyzer
If you work on the Rust code at all, you will need the
[rust-analyzer](https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer)
extension. This extension provides code completion for the Rust code.
For it to work well for the Polars code base, add the following settings to your
`.vscode/settings.json`:
```json
{
"rust-analyzer.cargo.features": "all",
"rust-analyzer.cargo.targetDir": true
}
```
#### Ruff
The [Ruff](https://marketplace.visualstudio.com/items?itemName=charliermarsh.ruff) extension will
help you conform to the formatting requirements of the Python code. We use both the Ruff linter and
formatter. It is recommended to configure the extension to use the Ruff installed in your
environment. This will make it use the correct Ruff version and configuration.
```json
{
"ruff.importStrategy": "fromEnvironment"
}
```
#### CodeLLDB
The [CodeLLDB](https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb) extension is
useful for debugging Rust code. You can also debug Rust code called from Python (see section below).
### Debugging
Due to the way that Python and Rust interoperate, debugging the Rust side of development from Python
calls can be difficult. This guide shows how to set up a debugging environment that makes debugging
Rust code called from a Python script painless.
#### Preparation
Start by installing the CodeLLDB extension (see above). Then add the following two configurations to
your `launch.json` file. This file is usually found in the `.vscode` folder of your project root.
See the
[official VSCode documentation](https://code.visualstudio.com/docs/editor/debugging#_launch-configurations)
for more information about the `launch.json` file.
launch.json
```json
{
"configurations": [
{
"name": "Debug Rust/Python",
"type": "debugpy",
"request": "launch",
"program": "${workspaceFolder}/py-polars/debug/launch.py",
"args": [
"${file}"
],
"console": "internalConsole",
"justMyCode": true,
"serverReadyAction": {
"pattern": "pID = ([0-9]+)",
"action": "startDebugging",
"name": "Rust LLDB"
}
},
{
"name": "Rust LLDB",
"pid": "0",
"type": "lldb",
"request": "attach",
"program": "${workspaceFolder}/py-polars/.venv/bin/python",
"stopOnEntry": false,
"sourceLanguages": [
"rust"
],
"presentation": {
"hidden": true
}
}
]
}
```
!!! info
On some systems, the LLDB debugger will not attach unless [ptrace protection](https://linux-audit.com/protect-ptrace-processes-kernel-yama-ptrace_scope) is disabled.
To disable, run the following command:
```shell
echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope
```
#### Running the debugger
1. Create a Python script containing Polars code. Ensure that your virtual environment is activated.
2. Set breakpoints in any `.rs` or `.py` file.
3. In the `Run and Debug` panel on the left, select `Debug Rust/Python` from the drop-down menu on
top and click the `Start Debugging` button.
At this point, your debugger should stop on breakpoints in any `.rs` file located within the
codebase.
#### Details
The debugging feature runs via the specially-designed VSCode launch configuration shown above. The
initial Python debugger is launched using a special launch script located at
`py-polars/debug/launch.py` and passes the name of the script to be debugged (the target script) as
an input argument. The launch script determines the process ID, writes this value into the
`launch.json` configuration file, compiles the target script and runs it in the current environment.
At this point, a second (Rust) debugger is attached to the Python debugger. The result is two
simultaneous debuggers operating on the same running instance. Breakpoints in the Python code will
stop on the Python debugger and breakpoints in the Rust code will stop on the Rust debugger.
## JetBrains (PyCharm, RustRover, CLion)
!!! info
More information needed.
---
polars/docs/source/development/contributing/index.md
---
---
render_macros: false
---
# Overview
Thanks for taking the time to contribute! We appreciate all contributions, from reporting bugs to
implementing new features. If you're unclear on how to proceed after reading this guide, please
contact us on [Discord](https://discord.gg/4UfP5cfBE7).
## Reporting bugs
We use [GitHub issues](https://github.com/pola-rs/polars/issues) to track bugs and suggested
enhancements. You can report a bug by opening a
[new issue](https://github.com/pola-rs/polars/issues/new/choose). Use the appropriate issue type for
the language you are using
([Rust](https://github.com/pola-rs/polars/issues/new?labels=bug&template=bug_report_rust.yml) /
[Python](https://github.com/pola-rs/polars/issues/new?labels=bug&template=bug_report_python.yml)).
Before creating a bug report, please check that your bug has not already been reported, and that
your bug exists on the latest version of Polars. If you find a closed issue that seems to report the
same bug you're experiencing, open a new issue and include a link to the original issue in your
issue description.
Please include as many details as possible in your bug report. The information helps the maintainers
resolve the issue faster.
## Suggesting enhancements
We use [GitHub issues](https://github.com/pola-rs/polars/issues) to track bugs and suggested
enhancements. You can suggest an enhancement by opening a
[new feature request](https://github.com/pola-rs/polars/issues/new?labels=enhancement&template=feature_request.yml).
Before creating an enhancement suggestion, please check that a similar issue does not already exist.
Please describe the behavior you want and why, and provide examples of how Polars would be used if
your feature were added.
## Contributing to the codebase
### Picking an issue
Pick an issue by going through the [issue tracker](https://github.com/pola-rs/polars/issues) and
finding an issue you would like to work on. Feel free to pick any issue with an
[accepted](https://github.com/pola-rs/polars/issues?q=is%3Aopen+is%3Aissue+label%3Aaccepted) label
that is not already assigned. We use the
[help wanted](https://github.com/pola-rs/polars/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)
label to indicate issues that are high on our wishlist.
If you are a first time contributor, you might want to look for issues labeled
[good first issue](https://github.com/pola-rs/polars/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22).
The Polars code base is quite complex, so starting with a small issue will help you find your way
around!
If you would like to take on an issue, please comment on the issue to let others know. You may use
the issue to discuss possible solutions.
### Setting up your local environment
The Polars development flow relies on both Rust and Python, which means setting up your local
development environment is not trivial. If you run into problems, please contact us on
[Discord](https://discord.gg/4UfP5cfBE7).
!!! note
If you are a Windows user, the steps below might not work as expected.
Try developing using [WSL](https://learn.microsoft.com/en-us/windows/wsl/install).
Under native Windows, you may have to manually copy the contents of `toolchain.toml` to `py-polars/toolchain.toml`, as Git for Windows may not correctly handle symbolic links.
#### Configuring Git
For contributing to Polars you need a free [GitHub account](https://github.com) and have
[git](https://git-scm.com) installed on your machine. Start by
[forking](https://docs.github.com/en/get-started/quickstart/fork-a-repo) the Polars repository, then
clone your forked repository using `git`:
```bash
git clone https://github.com//polars.git
cd polars
```
Optionally set the `upstream` remote to be able to sync your fork with the Polars repository in the
future:
```bash
git remote add upstream https://github.com/pola-rs/polars.git
git fetch upstream
```
#### Installing dependencies
In order to work on Polars effectively, you will need [Rust](https://www.rust-lang.org/),
[Python](https://www.python.org/), and [dprint](https://dprint.dev/).
First, install Rust using [rustup](https://www.rust-lang.org/tools/install). After the initial
installation, you will also need to install the nightly toolchain:
```bash
rustup toolchain install nightly --component miri
```
Next, install Python, for example using [pyenv](https://github.com/pyenv/pyenv#installation). We
recommend using the latest Python version (`3.13`). Make sure you deactivate any active virtual
environments (command: `deactivate`) or conda environments (command: `conda deactivate`), as the
steps below will create a new [virtual environment](https://docs.python.org/3/tutorial/venv.html)
for Polars. You will need Python even if you intend to work on the Rust code only, as we rely on the
Python tests to verify all functionality.
Finally, install [dprint](https://dprint.dev/install/). This is not strictly required, but it is
recommended as we use it to autoformat certain file types.
You can now check that everything works correctly by going into the `py-polars` directory and
running the test suite (warning: this may be slow the first time you run it):
```bash
cd py-polars
make test
```
!!! note
You need to have [CMake](https://cmake.org/) installed for `make test` to work.
This will do a number of things:
- Use Python to create a virtual environment in the `.venv` folder.
- Use [pip](https://pip.pypa.io/) and [uv](https://github.com/astral-sh/uv) to install all Python
dependencies for development, linting, and building documentation.
- Use Rust to compile and install Polars in your virtual environment. _At least 8GB of RAM is
recommended for this step to run smoothly._
- Use [pytest](https://docs.pytest.org/) to run the Python unittests in your virtual environment
!!! note
There are a small number of specialized dependencies that are not installed by default.
If you are running specific tests and encounter an error message about a missing dependency,
try running `make requirements-all` to install _all_ known dependencies).
Check if linting also works correctly by running:
```bash
make pre-commit
```
Note that we do not actually use the [pre-commit](https://pre-commit.com/) tool. We use the Makefile
to conveniently run the following formatting and linting tools:
- [ruff](https://github.com/charliermarsh/ruff)
- [mypy](http://mypy-lang.org/)
- [rustfmt](https://github.com/rust-lang/rustfmt)
- [clippy](https://doc.rust-lang.org/nightly/clippy/index.html)
- [dprint](https://dprint.dev/)
If this all runs correctly, you're ready to start contributing to the Polars codebase!
#### Updating the development environment
Dependencies are updated regularly - at least once per month. If you do not keep your environment
up-to-date, you may notice tests or CI checks failing, or you may not be able to build Polars at
all.
To update your environment, first make sure your fork is in sync with the Polars repository:
```bash
git checkout main
git fetch upstream
git rebase upstream/main
git push origin main
```
Update all Python dependencies to their latest versions by running:
```bash
make requirements
```
If the Rust toolchain version has been updated, you should update your Rust toolchain. Follow it up
by running `cargo clean` to make sure your Cargo folder does not grow too large:
```bash
rustup update
cargo clean
```
### Working on your issue
Create a new git branch from the `main` branch in your local repository, and start coding!
The Rust code is located in the `crates` directory, while the Python codebase is located in the
`py-polars` directory. Both directories contain a `Makefile` with helpful commands. Most notably:
- `make test` to run the test suite (see the [test suite docs](./test.md) for more info)
- `make pre-commit` to run autoformatting and linting
Note that your work cannot be merged if these checks fail! Run `make help` to get a list of other
helpful commands.
Two other things to keep in mind:
- If you add code that should be tested, add tests.
- If you change the public API, [update the documentation](#api-reference).
### Pull requests
When you have resolved your issue,
[open a pull request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork)
in the Polars repository. Please adhere to the following guidelines:
- Title:
- Start your pull request title with a [conventional commit](https://www.conventionalcommits.org/) tag.
This helps us add your contribution to the right section of the changelog.
We use the [Angular convention](https://github.com/angular/angular/blob/22b96b9/CONTRIBUTING.md#type).
Scope can be `rust` and/or `python`, depending on your contribution: this tag determines which changelog(s) will include your change.
Omit the scope if your change affects both Rust and Python.
- Use a descriptive title starting with an uppercase letter.
This text will end up in the [changelog](https://github.com/pola-rs/polars/releases), so make sure the text is meaningful to the user.
Use single backticks to annotate code snippets.
Use active language and do not end your title with punctuation.
- Example: ``fix(python): Fix `DataFrame.top_k` not handling nulls correctly``
- Description:
- In the pull request description, [link](https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue) to the issue you were working on.
- Add any relevant information to the description that you think may help the maintainers review your code.
- Make sure your branch is [rebased](https://docs.github.com/en/get-started/using-git/about-git-rebase) against the latest version of the `main` branch.
- Make sure all [GitHub Actions checks](./ci.md) pass.
After you have opened your pull request, a maintainer will review it and possibly leave some
comments. Once all issues are resolved, the maintainer will merge your pull request, and your work
will be part of the next Polars release!
Keep in mind that your work does not have to be perfect right away! If you are stuck or unsure about
your solution, feel free to open a draft pull request and ask for help.
## Contributing to documentation
The most important components of Polars documentation are the
[user guide](https://docs.pola.rs/user-guide/), the
[API references](https://docs.pola.rs/api/python/stable/reference/index.html), and the database of
questions on [StackOverflow](https://stackoverflow.com/).
### User guide
The user guide is maintained in the `docs/source/user-guide` folder. Before creating a PR first
raise an issue to discuss what you feel is missing or could be improved.
#### Building and serving the user guide
The user guide is built using [MkDocs](https://www.mkdocs.org/). You install the dependencies for
building the user guide by running `make build` in the root of the repo. Additionally, you need to
make sure the [graphviz](https://graphviz.org/) `dot` binary is on your path.
Activate the virtual environment and run `mkdocs serve` to build and serve the user guide, so you
can view it locally and see updates as you make changes.
#### Creating a new user guide page
Each user guide page is based on a `.md` markdown file. This file must be listed in `mkdocs.yml`.
#### Adding a shell code block
To add a code block with code to be run in a shell with tabs for Python and Rust, use the following
format:
````
=== ":fontawesome-brands-python: Python"
```shell
$ pip install fsspec
```
=== ":fontawesome-brands-rust: Rust"
```shell
$ cargo add aws_sdk_s3
```
````
#### Adding a code block
The snippets for Python and Rust code blocks are in the `docs/source/src/python/` and
`docs/source/src/rust/` directories, respectively. To add a code snippet with Python or Rust code to
a `.md` page, use the following format:
```
{{code_block('user-guide/io/cloud-storage','read_parquet',['read_parquet','read_csv'])}}
```
- The first argument is a path to either or both files called
`docs/source/src/python/user-guide/io/cloud-storage.py` and
`docs/source/src/rust/user-guide/io/cloud-storage.rs`.
- The second argument is the name given at the start and end of each snippet in the `.py` or `.rs`
file
- The third argument is a list of links to functions in the API docs. For each element of the list
there must be a corresponding entry in `docs/source/_build/API_REFERENCE_LINKS.yml`
If the corresponding `.py` and `.rs` snippet files both exist then each snippet named in the second
argument to `code_block` above must exist or the build will fail. An empty snippet should be added
to the `.py` or `.rs` file if the snippet is not needed.
Each snippet is formatted as follows:
```python
# --8<-- [start:read_parquet]
import polars as pl
df = pl.read_parquet("file.parquet")
# --8<-- [end:read_parquet]
```
The snippet is delimited by `--8<-- [start:]` and `--8<-- [end:]`. The
snippet name must match the name given in the second argument to `code_block` above.
In some cases, you may need to add links to different functions for the Python and Rust APIs. When
that is the case, you can use the two extra optional arguments that `code_block` accepts, that can
be used to pass Python-only and Rust-only links:
```
{{code_block('path', 'snippet_name', ['common_api_links'], ['python_only_links'], ['rust_only_links'])}}
```
#### Linting
Before committing, install `dprint` (see above) and run `dprint fmt` from the `docs` directory to
lint the markdown files.
### API reference
Polars has separate API references for [Rust](https://docs.pola.rs/api/rust/dev/polars/) and
[Python](https://docs.pola.rs/api/python/dev/reference/index.html). These are generated directly
from the codebase, so in order to contribute, you will have to follow the steps outlined in
[this section](#contributing-to-the-codebase) above.
#### Rust
Rust Polars uses `cargo doc` to build its documentation. Contributions to improve or clarify the API
reference are welcome.
#### Python
For the Python API reference, we always welcome good docstring examples. There are still parts of
the API that do not have any code examples. This is a great way to start contributing to Polars!
Note that we follow the [numpydoc](https://numpydoc.readthedocs.io/en/latest/format.html)
convention. Docstring examples should also follow the [Black](https://black.readthedocs.io/)
codestyle. From the `py-polars` directory, run `make fmt` to make sure your additions pass the
linter, and run `make doctest` to make sure your docstring examples are valid.
Polars uses Sphinx to build the API reference. This means docstrings in general should follow the
[reST](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html) format. If you want
to build the API reference locally, go to the `py-polars/docs` directory and run `make html`. The
resulting HTML files will be in `py-polars/docs/build/html`.
New additions to the API should be added manually to the API reference by adding an entry to the
correct `.rst` file in the `py-polars/docs/source/reference` directory.
### StackOverflow
We use StackOverflow to create a database of high quality questions and answers that is searchable
and remains up-to-date. There is a separate tag for each language:
- [Python Polars](https://stackoverflow.com/questions/tagged/python-polars)
- [Rust Polars](https://stackoverflow.com/questions/tagged/rust-polars)
Contributions in the form of well-formulated questions or answers are always welcome! If you add a
new question, please notify us by adding a
[matching issue](https://github.com/pola-rs/polars/issues/new?&labels=question&template=question.yml)
to our GitHub issue tracker.
## Release flow
_This section is intended for Polars maintainers._
Polars releases Rust crates to [crates.io](https://crates.io/crates/polars) and Python packages to
[PyPI](https://pypi.org/project/polars/).
New releases are marked by an official [GitHub release](https://github.com/pola-rs/polars/releases)
and an associated git tag. We utilize
[Release Drafter](https://github.com/release-drafter/release-drafter) to automatically draft GitHub
releases with release notes.
### Steps
The steps for releasing a new Rust or Python version are similar. The release process is mostly
automated through GitHub Actions, but some manual steps are required. Follow the steps below to
release a new version.
Start by bumping the version number in the source code:
1. Check the [releases page](https://github.com/pola-rs/polars/releases) on GitHub and find the
appropriate draft release. Note the version number associated with this release.
2. Make sure your fork is up-to-date with the latest version of the main Polars repository, and
create a new branch.
3. Bump the version number.
- _Rust:_ Update the version number in all `Cargo.toml` files in the `polars` directory and
subdirectories. You'll probably want to use some search/replace strategy, as there are quite a few
crates that need to be updated.
- _Python:_ Update the version number in
[`py-polars/Cargo.toml`](https://github.com/pola-rs/polars/blob/main/py-polars/Cargo.toml#L3) to
match the version of the draft release.
4. From the `py-polars` directory, run `make build` to generate a new `Cargo.lock` file.
5. Create a new commit with all files added. The name of the commit should follow the format
`release():  Polars `. For example:
`release(python): Python Polars 0.16.1`
6. Push your branch and open a new pull request to the `main` branch of the main Polars repository.
7. Wait for the GitHub Actions checks to pass, then squash and merge your pull request.
Directly after merging your pull request, release the new version:
8. Go to the release workflow
([Python](https://github.com/pola-rs/polars/actions/workflows/release-python.yml)/[Rust](https://github.com/pola-rs/polars/actions/workflows/release-rust.yml)),
click _Run workflow_ in the top right, and click the green button. This will trigger the
workflow, which will build all release artifacts and publish them.
9. Wait for the workflow to finish, then check
[crates.io](https://crates.io/crates/polars)/[PyPI](https://pypi.org/project/polars/)/[GitHub](https://github.com/pola-rs/polars/releases)
to verify that the new Polars release is now available.
### Troubleshooting
It may happen that one or multiple release jobs fail. If so, you should first try to simply re-run
the failed jobs from the GitHub Actions UI.
If that doesn't help, you will have to figure out what's wrong and commit a fix. Once your fix has
made it to the `main` branch, simply re-trigger the release workflow.
## License
Any contributions you make to this project will fall under the
[MIT License](https://github.com/pola-rs/polars/blob/main/LICENSE) that covers the Polars project.
---
polars/docs/source/development/contributing/test.md
---
# Test suite
!!! info
Additional information on the Rust test suite will be added to this page later.
The `py-polars/tests` folder contains the main Polars test suite. This page contains some
information on the various components of the test suite, as well as guidelines for writing new
tests.
The test suite contains four main components, each confined to their own folder: unit tests,
parametric tests, benchmark tests, and doctests.
Note that this test suite is indirectly responsible for testing Rust Polars as well. The Rust test
suite is kept small to reduce compilation times. A lot of the Rust functionality is tested here
instead.
## Unit tests
The `unit` folder contains all regular unit tests. These tests are intended to make sure all Polars
functionality works as intended.
### Running unit tests
Run unit tests by running `make test` from the `py-polars` folder. This will compile the Rust
bindings and then run the unit tests.
If you're working in the Python code only, you can avoid recompiling every time by simply running
`pytest` instead from your virtual environment.
By default, "slow" tests and "ci-only" tests are skipped for local test runs. Such tests are marked
using a [custom pytest marker](https://docs.pytest.org/en/latest/example/markers.html). To run these
tests specifically, you can run `pytest -m slow`, `pytest -m ci_only`, `pytest -m slow ci_only` or
run `pytest -m ""` to run _all_ tests, regardless of marker.
Note that the "ci-only" tests may require you to run `make requirements-all` to get additional
dependencies (such as `torch`) that are otherwise not installed as part of the default Polars
development environment.
Tests can be run in parallel by running `pytest -n auto`. The parallelization is handled by
[`pytest-xdist`](https://pytest-xdist.readthedocs.io/en/latest/).
### Writing unit tests
Whenever you add new functionality, you should also add matching unit tests. Add your tests to
appropriate test module in the `unit` folder. Some guidelines to keep in mind:
- Try to fully cover all possible inputs and edge cases you can think of.
- Utilize pytest tools like [`fixture`](https://docs.pytest.org/en/latest/explanation/fixtures.html)
and [`parametrize`](https://docs.pytest.org/en/latest/how-to/parametrize.html) where appropriate.
- Since many tests will require some data to be defined first, it can be efficient to run multiple
checks in a single test. This can also be addressed using pytest fixtures.
- Unit tests should not depend on external factors, otherwise test parallelization will break.
## Parametric tests
The `parametric` folder contains parametric tests written using the
[Hypothesis](https://hypothesis.readthedocs.io/) framework. These tests are intended to find and
test edge cases by generating many random datapoints.
### Running parametric tests
Run parametric tests by running `pytest -m hypothesis`.
Note that parametric tests are excluded by default when running `pytest`. You must explicitly
specify `-m hypothesis` to run them.
These tests _will_ be included when calculating test coverage, and will also be run as part of the
`make test-all` make command.
## Doctests
The `docs` folder contains a script for running
[`doctest`](https://docs.python.org/3/library/doctest.html). This folder does not contain any actual
tests - rather, the script checks all docstrings in the Polars package for `Examples` sections, runs
the code examples, and verifies the output.
The aim of running `doctest` is to make sure the `Examples` sections in our docstrings are valid and
remain up-to-date with code changes.
### Running `doctest`
To run the `doctest` module, run `make doctest` from the `py-polars` folder. You can also run the
script directly from your virtual environment.
Note that doctests are _not_ run using pytest. While pytest does have the capability to run doc
examples, configuration options are too limited for our purposes.
Doctests will _not_ count towards test coverage. They are not a substitute for unit tests, but
rather intended to convey the intended use of the Polars API to the user.
### Writing doc examples
Almost all classes/methods/functions that are part of Polars' public API should include code
examples in their docstring. These examples help users understand basic usage and allow us to
illustrate more advanced concepts as well. Some guidelines for writing a good docstring `Examples`
section:
- Start with a minimal example that showcases the default functionality.
- Showcase the effect of its parameters.
- Showcase any special interactions when combined with other code.
- Keep it succinct and avoid multiple examples showcasing the same thing.
There are many great docstring examples already, just check other code if you need inspiration!
In addition to the [regular options](https://docs.python.org/3/library/doctest.html#option-flags)
available when writing doctests, the script configuration allows for a new `IGNORE_RESULT`
directive. Use this directive if you want to ensure the code runs, but the output may be random by
design or not interesting to check.
```python
>>> df.sample(n=2) # doctest: +IGNORE_RESULT
```
## Benchmark tests
The `benchmark` folder contains code for running various benchmark tests. The aim of this part of
the test suite is to spot performance regressions in the code, and to verify that Polars
functionality works as expected when run on a release build or at a larger scale.
Polars uses [CodSpeed](https://codspeed.io/pola-rs/polars) for tracking the performance of the
benchmark tests.
### Generating data
For most tests, a relatively large dataset must be generated first. This is done as part of the
`pytest` setup process.
The data generation logic was taken from the
[H2O.ai database benchmark](https://github.com/h2oai/db-benchmark), which is the foundation for many
of the benchmark tests.
### Running the benchmark tests
The benchmark tests can be run using pytest. Run `pytest -m benchmark --durations 0 -v` to run these
tests and report run duration.
Note that benchmark tests are excluded by default when running `pytest`. You must explicitly specify
`-m benchmark` to run them. They will also be excluded when calculating test coverage.
These tests _will_ be run as part of the `make test-all` make command.
---
polars/docs/source/api/reference.md
---
# Reference guide
The API reference contains detailed descriptions of all public functions and objects. It's the best
place to look if you need information on a specific function.
## Python
The Python API reference is built using Sphinx. It's available in
[our docs](https://docs.pola.rs/api/python/stable/reference/index.html).
## Rust
The Rust API reference is built using Cargo. It's available on
[docs.rs](https://docs.rs/polars/latest/polars/).
---
polars/docs/source/releases/changelog.md
---
# Changelog
Polars uses GitHub to manage both Python and Rust releases.
Refer to our [GitHub releases page](https://github.com/pola-rs/polars/releases) for the changelog
associated with each new release.
---
polars/docs/source/releases/upgrade/0.19.md
---
# Version 0.19
## Breaking changes
### Aggregation functions no longer support horizontal computation
This impacts aggregation functions like `sum`, `min`, and `max`. These functions were overloaded to
support both vertical and horizontal computation. Recently, new dedicated functionality for
horizontal computation was released, and horizontal computation was deprecated.
Restore the old behavior by using the horizontal variant, e.g. `sum_horizontal`.
**Example**
Before:
```shell
>>> df = pl.DataFrame({'a': [1, 2], 'b': [11, 12]})
>>> df.select(pl.sum('a', 'b')) # horizontal computation
shape: (2, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ sum â”‚
â”‚ --- â”‚
â”‚ i64 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 12 â”‚
â”‚ 14 â”‚
â””â”€â”€â”€â”€â”€â”˜
```
After:
```shell
>>> df = pl.DataFrame({'a': [1, 2], 'b': [11, 12]})
>>> df.select(pl.sum('a', 'b')) # vertical computation
shape: (1, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ a â”† b â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ 3 â”† 23 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
### Update to `all` / `any`
`all` will now ignore null values by default, rather than treat them as `False`.
For both `any` and `all`, the `drop_nulls` parameter has been renamed to `ignore_nulls` and is now
keyword-only. Also fixed an issue when setting this parameter to `False` would erroneously result in
`None` output in some cases.
To restore the old behavior, set `ignore_nulls` to `False` and check for `None` output.
**Example**
Before:
```shell
>>> pl.Series([True, None]).all()
False
```
After:
```shell
>>> pl.Series([True, None]).all()
True
```
### Improved error types for many methods
Improving our error messages is an ongoing effort. We did a sweep of our Python code base and made
many improvements to error messages and error types. Most notably, many `ValueError`s were changed
to `TypeError`s.
If your code relies on handling Polars exceptions, you may have to make some adjustments.
**Example**
Before:
```shell
>>> pl.Series(values=15)
...
ValueError: Series constructor called with unsupported type; got 'int'
```
After:
```shell
>>> pl.Series(values=15)
...
TypeError: Series constructor called with unsupported type 'int' for the `values` parameter
```
### Updates to expression input parsing
Methods like `select` and `with_columns` accept one or more expressions. But they also accept
strings, integers, lists, and other inputs that we try to interpret as expressions. We updated our
internal logic to parse inputs more consistently.
**Example**
Before:
```shell
>>> pl.DataFrame({'a': [1, 2]}).with_columns(None)
shape: (2, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ i64 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 1 â”‚
â”‚ 2 â”‚
â””â”€â”€â”€â”€â”€â”˜
```
After:
```shell
>>> pl.DataFrame({'a': [1, 2]}).with_columns(None)
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ a â”† literal â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† null â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† null â”‚
â”‚ 2 â”† null â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
### `shuffle` / `sample` now use an internal Polars seed
If you used the built-in Python `random.seed` function to control the randomness of Polars
expressions, this will no longer work. Instead, use the new `set_random_seed` function.
**Example**
Before:
```python
import random
random.seed(1)
```
After:
```python
import polars as pl
pl.set_random_seed(1)
```
## Deprecations
Creating a consistent and intuitive API is hard; finding the right name for each function, method,
and parameter might be the hardest part. The new version comes with several naming changes, and you
will most likely run into deprecation warnings when upgrading to `0.19`.
If you want to upgrade without worrying about deprecation warnings right now, you can add the
following snippet to your code:
```python
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
```
### `groupby` renamed to `group_by`
This is not a change we make lightly, as it will impact almost all our users. But "group by" is
really two different words, and our naming strategy dictates that these should be separated by an
underscore.
Most likely, a simple search and replace will be enough to take care of this update:
- Search: `.groupby(`
- Replace: `.group_by(`
### `apply` renamed to `map_*`
`apply` is probably the most misused part of our API. Many Polars users come from pandas, where
`apply` has a completely different meaning.
We now consolidate all our functionality for user-defined functions under the name `map`. This
results in the following renaming:
| Before | After |
| --------------------------- | -------------- |
| `Series/Expr.apply` | `map_elements` |
| `Series/Expr.rolling_apply` | `rolling_map` |
| `DataFrame.apply` | `map_rows` |
| `GroupBy.apply` | `map_groups` |
| `apply` | `map_groups` |
| `map` | `map_batches` |
---
polars/docs/source/releases/upgrade/0.20.md
---
# Version 0.20
## Breaking changes
### Change default `join` behavior with regard to null values
Previously, null values in the join key were considered a value like any other value. This meant
that null values in the left frame would be joined with null values in the right frame. This is
expensive and does not match default behavior in SQL.
Default behavior has now been changed to ignore null values in the join key. The previous behavior
can be retained by setting `join_nulls=True`.
**Example**
Before:
```pycon
>>> df1 = pl.DataFrame({"a": [1, 2, None], "b": [4, 4, 4]})
>>> df2 = pl.DataFrame({"a": [None, 2, 3], "c": [5, 5, 5]})
>>> df1.join(df2, on="a", how="inner")
shape: (2, 3)
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ a â”† b â”† c â”‚
â”‚ --- â”† --- â”† --- â”‚
â”‚ i64 â”† i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ null â”† 4 â”† 5 â”‚
â”‚ 2 â”† 4 â”† 5 â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> df1.join(df2, on="a", how="inner")
shape: (1, 3)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ a â”† b â”† c â”‚
â”‚ --- â”† --- â”† --- â”‚
â”‚ i64 â”† i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ 2 â”† 4 â”† 5 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
>>> df1.join(df2, on="a", how="inner", nulls_equal=True) # Keeps previous behavior
shape: (2, 3)
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ a â”† b â”† c â”‚
â”‚ --- â”† --- â”† --- â”‚
â”‚ i64 â”† i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ null â”† 4 â”† 5 â”‚
â”‚ 2 â”† 4 â”† 5 â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
### Preserve left and right join keys in outer joins
Previously, the result of an outer join did not contain the join keys of the left and right frames.
Rather, it contained a coalesced version of the left key and right key. This loses information and
does not conform to default SQL behavior.
The behavior has been changed to include the original join keys. Name clashes are solved by
appending a suffix (`_right` by default) to the right join key name. The previous behavior can be
retained by setting `how="outer_coalesce"`.
**Example**
Before:
```pycon
>>> df1 = pl.DataFrame({"L1": ["a", "b", "c"], "L2": [1, 2, 3]})
>>> df2 = pl.DataFrame({"L1": ["a", "c", "d"], "R2": [7, 8, 9]})
>>> df1.join(df2, on="L1", how="outer")
shape: (4, 3)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”�
â”‚ L1 â”† L2 â”† R2 â”‚
â”‚ --- â”† --- â”† --- â”‚
â”‚ str â”† i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•¡
â”‚ a â”† 1 â”† 7 â”‚
â”‚ c â”† 3 â”† 8 â”‚
â”‚ d â”† null â”† 9 â”‚
â”‚ b â”† 2 â”† null â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> df1.join(df2, on="L1", how="outer")
shape: (4, 4)
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”�
â”‚ L1 â”† L2 â”† L1_right â”† R2 â”‚
â”‚ --- â”† --- â”† --- â”† --- â”‚
â”‚ str â”† i64 â”† str â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•¡
â”‚ a â”† 1 â”† a â”† 7 â”‚
â”‚ b â”† 2 â”† null â”† null â”‚
â”‚ c â”† 3 â”† c â”† 8 â”‚
â”‚ null â”† null â”† d â”† 9 â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
>>> df1.join(df2, on="a", how="outer_coalesce") # Keeps previous behavior
shape: (4, 3)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”�
â”‚ L1 â”† L2 â”† R2 â”‚
â”‚ --- â”† --- â”† --- â”‚
â”‚ str â”† i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•¡
â”‚ a â”† 1 â”† 7 â”‚
â”‚ c â”† 3 â”† 8 â”‚
â”‚ d â”† null â”† 9 â”‚
â”‚ b â”† 2 â”† null â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```
### `count` now ignores null values
The `count` method for `Expr` and `Series` now ignores null values. Use `len` to get the count with
null values included.
Note that `pl.count()` and `group_by(...).count()` are unchanged. These count the number of rows in
the context, so nulls are not applicable in the same way.
This brings behavior more in line with the SQL standard, where `COUNT(col)` ignores null values but
`COUNT(*)` counts rows regardless of null values.
**Example**
Before:
```pycon
>>> df = pl.DataFrame({"a": [1, 2, None]})
>>> df.select(pl.col("a").count())
shape: (1, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ u32 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 3 â”‚
â””â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> df.select(pl.col("a").count())
shape: (1, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ u32 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 2 â”‚
â””â”€â”€â”€â”€â”€â”˜
>>> df.select(pl.col("a").len()) # Mirrors previous behavior
shape: (1, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ u32 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 3 â”‚
â””â”€â”€â”€â”€â”€â”˜
```
### `NaN` values are now considered equal
Floating point `NaN` values were treated as unequal across Polars operations. This has been
corrected to better match user expectation and existing standards.
While this is considered a bug fix, it is included in this guide in order to draw attention to
possible impact on user workflows that may contain `NaN` values.
**Example**
Before:
```pycon
>>> s = pl.Series([1.0, float("nan"), float("inf")])
>>> s == s
shape: (3,)
Series: '' [bool]
[
true
false
true
]
```
After:
```pycon
>>> s == s
shape: (3,)
Series: '' [bool]
[
true
true
true
]
```
### Assertion utils updates to exact checking and `NaN` equality
The assertion utility functions `assert_frame_equal` and `assert_series_equal` would use the
tolerance parameters `atol` and `rtol` to do approximate checking, unless `check_exact` was set to
`True`. This could lead to some surprising behavior, as integers are generally thought of as exact
values. Integer values are now always checked exactly. To do inexact checking, convert to float
first.
Additionally, the `nans_compare_equal` parameter has been removed and `NaN` values are now always
considered equal, which was the previous default behavior. This parameter had previously been
deprecated but has been removed before the end of the standard deprecation period to facilitate the
change to `NaN` equality.
**Example**
Before:
```pycon
>>> from polars.testing import assert_frame_equal
>>> df1 = pl.DataFrame({"id": [123456]})
>>> df2 = pl.DataFrame({"id": [123457]})
>>> assert_frame_equal(df1, df2) # Passes
```
After:
```pycon
>>> assert_frame_equal(df1, df2)
...
AssertionError: DataFrames are different (value mismatch for column 'id')
[left]: [123456]
[right]: [123457]
```
### Allow all `DataType` objects to be instantiated
Polars data types are subclasses of the `DataType` class. We had a 'hack' in place that
automatically converted data types instantiated without any arguments to the `class`, rather than
actually instantiating it. The idea was to allow specifying data types as `Int64` rather than
`Int64()`, which is more succinct. However, this caused some unexpected behavior when working
directly with data type objects, especially as there was a discrepancy with data types like
`Datetime` which _will_ be instantiated in many cases.
Going forward, instantiating a data type will always return an instance of that class. Both classes
an instances are handled by Polars, so the previous short syntax is still available. Methods that
return data types like `Series.dtype` and `DataFrame.schema` now always return instantiated data
types objects.
You may have to update some of your data type checks if you were not already using the equality
operator (`==`), as well as update some type hints.
**Example**
Before:
```pycon
>>> s = pl.Series([1, 2, 3], dtype=pl.Int8)
>>> s.dtype == pl.Int8
True
>>> s.dtype is pl.Int8
True
>>> isinstance(s.dtype, pl.Int8)
False
```
After:
```pycon
>>> s.dtype == pl.Int8
True
>>> s.dtype is pl.Int8
False
>>> isinstance(s.dtype, pl.Int8)
True
```
### Update constructors for `Decimal` and `Array` data types
The data types `Decimal` and `Array` have had their parameters switched around. The new constructors
should more closely match user expectations.
**Example**
Before:
```pycon
>>> pl.Array(2, pl.Int16)
Array(Int16, 2)
>>> pl.Decimal(5, 10)
Decimal(precision=10, scale=5)
```
After:
```pycon
>>> pl.Array(pl.Int16, 2)
Array(Int16, width=2)
>>> pl.Decimal(10, 5)
Decimal(precision=10, scale=5)
```
### `DataType.is_nested` changed from a property to a class method
This is a minor change, but a very important one to properly update. Failure to update accordingly
may result in faulty logic, as Python will evaluate the _method_ to `True`. For example,
`if dtype.is_nested` will now evaluate to `True` regardless of the data type, because it returns the
method, which Python considers truthy.
**Example**
Before:
```
>>> pl.List(pl.Int8).is_nested
True
```
After:
```
>>> pl.List(pl.Int8).is_nested()
True
```
### Smaller integer data types for datetime components `dt.month`, `dt.week`
Most datetime components such as `month` and `week` would previously return a `UInt32` type. This
has been updated to the smallest appropriate signed integer type. This should reduce memory
consumption.
| Method | Dtype old | Dtype new |
| ----------- | --------- | --------- |
| year | i32 | i32 |
| iso_year | i32 | i32 |
| quarter | u32 | i8 |
| month | u32 | i8 |
| week | u32 | i8 |
| day | u32 | i8 |
| weekday | u32 | i8 |
| ordinal_day | u32 | i16 |
| hour | u32 | i8 |
| minute | u32 | i8 |
| second | u32 | i8 |
| millisecond | u32 | i32* |
| microsecond | u32 | i32 |
| nanosecond | u32 | i32 |
_*Technically, `millisecond` can be an `i16`. This may be updated in the future._
**Example**
Before:
```pycon
>>> from datetime import date
>>> s = pl.Series([date(2023, 12, 31), date(2024, 1, 1)])
>>> s.dt.month()
shape: (2,)
Series: '' [u32]
[
12
1
]
```
After:
```pycon
>>> s.dt.month()
shape: (2,)
Series: '' [u8]
[
12
1
]
```
### Series now defaults to `Null` data type when no data is present
This replaces the previous behavior of initializing as a `Float32` type.
**Example**
Before:
```pycon
>>> pl.Series("a", [None])
shape: (1,)
Series: 'a' [f32]
[
null
]
```
After:
```pycon
>>> pl.Series("a", [None])
shape: (1,)
Series: 'a' [null]
[
null
]
```
### `replace` reimplemented with slightly different behavior
The new implementation is mostly backwards compatible. Please do note the following:
1. The logic for determining the return data type has changed. You may want to specify
`return_dtype` to override the inferred data type, or take advantage of the new function
signature (separate `old` and `new` parameters) to influence the return type.
2. The previous workaround for referencing other columns as default by using a struct column no
longer works. It now simply works as expected, no workaround needed.
**Example**
Before:
```pycon
>>> df = pl.DataFrame({"a": [1, 2, 2, 3], "b": [1.5, 2.5, 5.0, 1.0]}, schema={"a": pl.Int8, "b": pl.Float64})
>>> df.select(pl.col("a").replace({2: 100}))
shape: (4, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ i8 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 1 â”‚
â”‚ 100 â”‚
â”‚ 100 â”‚
â”‚ 3 â”‚
â””â”€â”€â”€â”€â”€â”˜
>>> df.select(pl.struct("a", "b").replace({2: 100}, default=pl.col("b")))
shape: (4, 1)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ f64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 1.5 â”‚
â”‚ 100.0 â”‚
â”‚ 100.0 â”‚
â”‚ 1.0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> df.select(pl.col("a").replace({2: 100}))
shape: (4, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ i64 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 1 â”‚
â”‚ 100 â”‚
â”‚ 100 â”‚
â”‚ 3 â”‚
â””â”€â”€â”€â”€â”€â”˜
>>> df.select(pl.col("a").replace({2: 100}, default=pl.col("b"))) # No struct needed
shape: (4, 1)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ f64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 1.5 â”‚
â”‚ 100.0 â”‚
â”‚ 100.0 â”‚
â”‚ 1.0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜
```
### `value_counts` resulting column renamed from `counts` to `count`
The resulting struct field for the `value_counts` method has been renamed from `counts` to `count`.
**Example**
Before:
```pycon
>>> s = pl.Series("a", ["x", "x", "y"])
>>> s.value_counts()
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ a â”† counts â”‚
â”‚ --- â”† --- â”‚
â”‚ str â”† u32 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ x â”† 2 â”‚
â”‚ y â”† 1 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> s.value_counts()
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ a â”† count â”‚
â”‚ --- â”† --- â”‚
â”‚ str â”† u32 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ x â”† 2 â”‚
â”‚ y â”† 1 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
```
### Update `read_parquet` to use Object Store rather than fsspec
If you were using `read_parquet`, installing `fsspec` as an optional dependency is no longer
required. The new Object Store implementation was already in use for `scan_parquet`. It may have
slightly different behavior in certain cases, such as how credentials are detected and how downloads
are performed.
The resulting `DataFrame` should be identical between versions.
## Deprecations
### Cumulative functions renamed from `cum*` to `cum_*`
Technically, this deprecation was introduced in version `0.19.14`, but many users will first
encounter it when upgrading to `0.20`. It's a relatively impactful change, which is why we mention
it here.
| Old name | New name |
| ----------- | ------------ |
| `cumfold` | `cum_fold` |
| `cumreduce` | `cum_reduce` |
| `cumsum` | `cum_sum` |
| `cumprod` | `cum_prod` |
| `cummin` | `cum_min` |
| `cummax` | `cum_max` |
| `cumcount` | `cum_count` |
---
polars/docs/source/releases/upgrade/1.md
---
# Version 1
## Breaking changes
### Properly apply `strict` parameter in Series constructor
The behavior of the Series constructor has been updated. Generally, it will be more strict, unless
the user passes `strict=False`.
Strict construction is more efficient than non-strict construction, so make sure to pass values of
the same data type to the constructor for the best performance.
**Example**
Before:
```pycon
>>> s = pl.Series([1, 2, 3.5])
shape: (3,)
Series: '' [f64]
[
1.0
2.0
3.5
]
>>> s = pl.Series([1, 2, 3.5], strict=False)
shape: (3,)
Series: '' [i64]
[
1
2
null
]
>>> s = pl.Series([1, 2, 3.5], strict=False, dtype=pl.Int8)
Series: '' [i8]
[
1
2
null
]
```
After:
```pycon
>>> s = pl.Series([1, 2, 3.5])
Traceback (most recent call last):
...
TypeError: unexpected value while building Series of type Int64; found value of type Float64: 3.5
Hint: Try setting `strict=False` to allow passing data with mixed types.
>>> s = pl.Series([1, 2, 3.5], strict=False)
shape: (3,)
Series: '' [f64]
[
1.0
2.0
3.5
]
>>> s = pl.Series([1, 2, 3.5], strict=False, dtype=pl.Int8)
Series: '' [i8]
[
1
2
3
]
```
### Change data orientation inference logic for DataFrame construction
Polars no longer inspects data types to infer the orientation of the data passed to the DataFrame
constructor. Data orientation is inferred based on the data and schema dimensions.
Additionally, a warning is raised whenever row orientation is inferred. Because of some confusing
edge cases, users should pass `orient="row"` to make explicit that their input is row-based.
**Example**
Before:
```pycon
>>> data = [[1, "a"], [2, "b"]]
>>> pl.DataFrame(data)
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ column_0 â”† column_1 â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† str â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† a â”‚
â”‚ 2 â”† b â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> pl.DataFrame(data)
Traceback (most recent call last):
...
TypeError: unexpected value while building Series of type Int64; found value of type String: "a"
Hint: Try setting `strict=False` to allow passing data with mixed types.
```
Use instead:
```pycon
>>> pl.DataFrame(data, orient="row")
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ column_0 â”† column_1 â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† str â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† a â”‚
â”‚ 2 â”† b â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
### Consistently convert to given time zone in Series constructor
!!! danger
This change may silently impact the results of your pipelines.
If you work with time zones, please make sure to account for this change.
Handling of time zone information in the Series and DataFrame constructors was inconsistent.
Row-wise construction would convert to the given time zone, while column-wise construction would
_replace_ the time zone. The inconsistency has been fixed by always converting to the time zone
specified in the data type.
**Example**
Before:
```pycon
>>> from datetime import datetime
>>> pl.Series([datetime(2020, 1, 1)], dtype=pl.Datetime('us', 'Europe/Amsterdam'))
shape: (1,)
Series: '' [datetime[Î¼s, Europe/Amsterdam]]
[
2020-01-01 00:00:00 CET
]
```
After:
```pycon
>>> from datetime import datetime
>>> pl.Series([datetime(2020, 1, 1)], dtype=pl.Datetime('us', 'Europe/Amsterdam'))
shape: (1,)
Series: '' [datetime[Î¼s, Europe/Amsterdam]]
[
2020-01-01 01:00:00 CET
]
```
### Update some error types to more appropriate variants
We have updated a lot of error types to more accurately represent the problem. Most commonly,
`ComputeError` types were changed to `InvalidOperationError` or `SchemaError`.
**Example**
Before:
```pycon
>>> s = pl.Series("a", [100, 200, 300])
>>> s.cast(pl.UInt8)
Traceback (most recent call last):
...
polars.exceptions.ComputeError: conversion from `i64` to `u8` failed in column 'a' for 1 out of 3 values: [300]
```
After:
```pycon
>>> s.cast(pl.UInt8)
Traceback (most recent call last):
...
polars.exceptions.InvalidOperationError: conversion from `i64` to `u8` failed in column 'a' for 1 out of 3 values: [300]
```
### Update `read/scan_parquet` to disable Hive partitioning by default for file inputs
Parquet reading functions now also support directory inputs. Hive partitioning is enabled by default
for directories, but is now _disabled_ by default for file inputs. File inputs include single files,
globs, and lists of files. Explicitly pass `hive_partitioning=True` to restore previous behavior.
**Example**
Before:
```pycon
>>> pl.read_parquet("dataset/a=1/foo.parquet")
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ a â”† x â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† f64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† 1.0 â”‚
â”‚ 1 â”† 2.0 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> pl.read_parquet("dataset/a=1/foo.parquet")
shape: (2, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ x â”‚
â”‚ --- â”‚
â”‚ f64 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 1.0 â”‚
â”‚ 2.0 â”‚
â””â”€â”€â”€â”€â”€â”˜
>>> pl.read_parquet("dataset/a=1/foo.parquet", hive_partitioning=True)
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ a â”† x â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† f64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† 1.0 â”‚
â”‚ 1 â”† 2.0 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
### Update `reshape` to return Array types instead of List types
`reshape` now returns an Array type instead of a List type.
Users can restore the old functionality by calling `.arr.to_list()` on the output. Note that this is
not more expensive than it would be to create a List type directly, because reshaping into an array
is basically free.
**Example**
Before:
```pycon
>>> s = pl.Series([1, 2, 3, 4, 5, 6])
>>> s.reshape((2, 3))
shape: (2,)
Series: '' [list[i64]]
[
[1, 2, 3]
[4, 5, 6]
]
```
After:
```pycon
>>> s.reshape((2, 3))
shape: (2,)
Series: '' [array[i64, 3]]
[
[1, 2, 3]
[4, 5, 6]
]
```
### Read 2D NumPy arrays as `Array` type instead of `List`
The Series constructor now parses 2D NumPy arrays as an `Array` type rather than a `List` type.
**Example**
Before:
```pycon
>>> import numpy as np
>>> arr = np.array([[1, 2], [3, 4]])
>>> pl.Series(arr)
shape: (2,)
Series: '' [list[i64]]
[
[1, 2]
[3, 4]
]
```
After:
```pycon
>>> import numpy as np
>>> arr = np.array([[1, 2], [3, 4]])
>>> pl.Series(arr)
shape: (2,)
Series: '' [array[i64, 2]]
[
[1, 2]
[3, 4]
]
```
### Split `replace` functionality into two separate methods
The API for `replace` has proven to be confusing to many users, particularly with regards to the
`default` argument and the resulting data type.
It has been split up into two methods: `replace` and `replace_strict`. `replace` now always keeps
the existing data type _(breaking, see example below)_ and is meant for replacing some values in
your existing column. Its parameters `default` and `return_dtype` have been deprecated.
The new method `replace_strict` is meant for creating a new column, mapping some or all of the
values of the original column, and optionally specifying a default value. If no default is provided,
it raises an error if any non-null values are not mapped.
**Example**
Before:
```pycon
>>> s = pl.Series([1, 2, 3])
>>> s.replace(1, "a")
shape: (3,)
Series: '' [str]
[
"a"
"2"
"3"
]
```
After:
```pycon
>>> s.replace(1, "a")
Traceback (most recent call last):
...
polars.exceptions.InvalidOperationError: conversion from `str` to `i64` failed in column 'literal' for 1 out of 1 values: ["a"]
>>> s.replace_strict(1, "a", default=s)
shape: (3,)
Series: '' [str]
[
"a"
"2"
"3"
]
```
### Preserve nulls in `ewm_mean`, `ewm_std`, and `ewm_var`
Polars will no longer forward-fill null values in `ewm` methods. The user can call `.forward_fill()`
on the output to achieve the same result.
**Example**
Before:
```pycon
>>> s = pl.Series([1, 4, None, 3])
>>> s.ewm_mean(alpha=.9, ignore_nulls=False)
shape: (4,)
Series: '' [f64]
[
1.0
3.727273
3.727273
3.007913
]
```
After:
```pycon
>>> s.ewm_mean(alpha=.9, ignore_nulls=False)
shape: (4,)
Series: '' [f64]
[
1.0
3.727273
null
3.007913
]
```
### Update `clip` to no longer propagate nulls in the given bounds
Null values in the bounds no longer set the value to null - instead, the original value is retained.
**Before**
```pycon
>>> df = pl.DataFrame({"a": [0, 1, 2], "min": [1, None, 1]})
>>> df.select(pl.col("a").clip("min"))
shape: (3, 1)
â”Œâ”€â”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ i64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•¡
â”‚ 1 â”‚
â”‚ null â”‚
â”‚ 2 â”‚
â””â”€â”€â”€â”€â”€â”€â”˜
```
**After**
```pycon
>>> df.select(pl.col("a").clip("min"))
shape: (3, 1)
â”Œâ”€â”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ i64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•¡
â”‚ 1 â”‚
â”‚ 1 â”‚
â”‚ 2 â”‚
â””â”€â”€â”€â”€â”€â”€â”˜
```
### Change `str.to_datetime` to default to microsecond precision for format specifiers `"%f"` and `"%.f"`
In `.str.to_datetime`, when specifying `%.f` as the format, the default was to set the resulting
datatype to nanosecond precision. This has been changed to microsecond precision.
#### Example
**Before**
```pycon
>>> s = pl.Series(["2022-08-31 00:00:00.123456789"])
>>> s.str.to_datetime(format="%Y-%m-%d %H:%M:%S%.f")
shape: (1,)
Series: '' [datetime[ns]]
[
2022-08-31 00:00:00.123456789
]
```
**After**
```pycon
>>> s.str.to_datetime(format="%Y-%m-%d %H:%M:%S%.f")
shape: (1,)
Series: '' [datetime[us]]
[
2022-08-31 00:00:00.123456
]
```
### Update resulting column names in `pivot` when pivoting by multiple values
In `DataFrame.pivot`, when specifying multiple `values` columns, the result would redundantly
include the `column` column in the column names. This has been addressed.
**Example**
Before:
```python
>>> df = pl.DataFrame(
... {
... "name": ["Cady", "Cady", "Karen", "Karen"],
... "subject": ["maths", "physics", "maths", "physics"],
... "test_1": [98, 99, 61, 58],
... "test_2": [100, 100, 60, 60],
... }
... )
>>> df.pivot(index='name', columns='subject', values=['test_1', 'test_2'])
shape: (2, 5)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ name â”† test_1_subject_maths â”† test_1_subject_physics â”† test_2_subject_maths â”† test_2_subject_physics â”‚
â”‚ --- â”† --- â”† --- â”† --- â”† --- â”‚
â”‚ str â”† i64 â”† i64 â”† i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ Cady â”† 98 â”† 99 â”† 100 â”† 100 â”‚
â”‚ Karen â”† 61 â”† 58 â”† 60 â”† 60 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
After:
```python
>>> df = pl.DataFrame(
... {
... "name": ["Cady", "Cady", "Karen", "Karen"],
... "subject": ["maths", "physics", "maths", "physics"],
... "test_1": [98, 99, 61, 58],
... "test_2": [100, 100, 60, 60],
... }
... )
>>> df.pivot('subject', index='name')
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ name â”† test_1_maths â”† test_1_physics â”† test_2_maths â”† test_2_physics â”‚
â”‚ --- â”† --- â”† --- â”† --- â”† --- â”‚
â”‚ str â”† i64 â”† i64 â”† i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ Cady â”† 98 â”† 99 â”† 100 â”† 100 â”‚
â”‚ Karen â”† 61 â”† 58 â”† 60 â”† 60 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
Note that the function signature has also changed:
- `columns` has been renamed to `on`, and is now the first positional argument.
- `index` and `values` are both optional. If `index` is not specified, then it will use all columns
not specified in `on` and `values`. If `values` is not specified, it will use all columns not
specified in `on` and `index`.
### Support Decimal types by default when converting from Arrow
Update conversion from Arrow to always convert Decimals into Polars Decimals, rather than cast to
Float64. `Config.activate_decimals` has been removed.
**Example**
Before:
```pycon
>>> from decimal import Decimal as D
>>> import pyarrow as pa
>>> arr = pa.array([D("1.01"), D("2.25")])
>>> pl.from_arrow(arr)
shape: (2,)
Series: '' [f64]
[
1.01
2.25
]
```
After:
```pycon
>>> pl.from_arrow(arr)
shape: (2,)
Series: '' [decimal[3,2]]
[
1.01
2.25
]
```
### Remove serde functionality from `pl.read_json` and `DataFrame.write_json`
`pl.read_json` no longer supports reading JSON files produced by `DataFrame.serialize`. Users should
use `pl.DataFrame.deserialize` instead.
`DataFrame.write_json` now only writes row-oriented JSON. The parameters `row_oriented` and `pretty`
have been removed. Users should use `DataFrame.serialize` to serialize a DataFrame.
**Example - `write_json`**
Before:
```pycon
>>> df = pl.DataFrame({"a": [1, 2], "b": [3.0, 4.0]})
>>> df.write_json()
'{"columns":[{"name":"a","datatype":"Int64","bit_settings":"","values":[1,2]},{"name":"b","datatype":"Float64","bit_settings":"","values":[3.0,4.0]}]}'
```
After:
```pycon
>>> df.write_json() # Same behavior as previously `df.write_json(row_oriented=True)`
'[{"a":1,"b":3.0},{"a":2,"b":4.0}]'
```
**Example - `read_json`**
Before:
```pycon
>>> import io
>>> df_ser = '{"columns":[{"name":"a","datatype":"Int64","bit_settings":"","values":[1,2]},{"name":"b","datatype":"Float64","bit_settings":"","values":[3.0,4.0]}]}'
>>> pl.read_json(io.StringIO(df_ser))
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ a â”† b â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† f64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† 3.0 â”‚
â”‚ 2 â”† 4.0 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> pl.read_json(io.StringIO(df_ser)) # Format no longer supported: data is treated as a single row
shape: (1, 1)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ columns â”‚
â”‚ --- â”‚
â”‚ list[struct[4]] â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ [{"a","Int64","",[1.0, 2.0]}, â€¦ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
Use instead:
```pycon
>>> pl.DataFrame.deserialize(io.StringIO(df_ser))
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ a â”† b â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† f64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† 3.0 â”‚
â”‚ 2 â”† 4.0 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
### `Series.equals` no longer checks names by default
Previously, `Series.equals` would return `False` if the Series names didn't match. The method now no
longer checks the names by default. The previous behavior can be retained by setting
`check_names=True`.
**Example**
Before:
```pycon
>>> s1 = pl.Series("foo", [1, 2, 3])
>>> s2 = pl.Series("bar", [1, 2, 3])
>>> s1.equals(s2)
False
```
After:
```pycon
>>> s1.equals(s2)
True
>>> s1.equals(s2, check_names=True)
False
```
### Remove `columns` parameter from `nth` expression function
The `columns` parameter was removed in favor of treating positional inputs as additional indices.
Use `Expr.get` instead to get the same functionality.
**Example**
Before:
```pycon
>>> df = pl.DataFrame({"a": [1, 2], "b": [3, 4], "c": [5, 6]})
>>> df.select(pl.nth(1, "a"))
shape: (1, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ i64 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 2 â”‚
â””â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> df.select(pl.nth(1, "a"))
...
TypeError: argument 'indices': 'str' object cannot be interpreted as an integer
```
Use instead:
```pycon
>>> df.select(pl.col("a").get(1))
shape: (1, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ i64 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 2 â”‚
â””â”€â”€â”€â”€â”€â”˜
```
### Rename struct fields of `rle` output
The struct fields of the `rle` method have been renamed from `lengths/values` to `len/value`. The
data type of the `len` field has also been updated to match the index type (was previously `Int32`,
now `UInt32`).
**Before**
```pycon
>>> s = pl.Series(["a", "a", "b", "c", "c", "c"])
>>> s.rle().struct.unnest()
shape: (3, 2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ lengths â”† values â”‚
â”‚ --- â”† --- â”‚
â”‚ i32 â”† str â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 2 â”† a â”‚
â”‚ 1 â”† b â”‚
â”‚ 3 â”† c â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**After**
```pycon
>>> s.rle().struct.unnest()
shape: (3, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ len â”† value â”‚
â”‚ --- â”† --- â”‚
â”‚ u32 â”† str â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 2 â”† a â”‚
â”‚ 1 â”† b â”‚
â”‚ 3 â”† c â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
```
### Update `set_sorted` to only accept a single column
Calling `set_sorted` indicates that a column is sorted _individually_. Passing multiple columns
indicates that each of those columns are also sorted individually. However, many users assumed this
meant that the columns were sorted as a group, which led to incorrect results.
To help users avoid this pitfall, we removed the possibility to specify multiple columns in
`set_sorted`. To set multiple columns as sorted, simply call `set_sorted` multiple times.
**Example**
Before:
```pycon
>>> df = pl.DataFrame({"a": [1, 2, 3], "b": [4.0, 5.0, 6.0], "c": [9, 7, 8]})
>>> df.set_sorted("a", "b")
```
After:
```pycon
>>> df.set_sorted("a", "b")
Traceback (most recent call last):
...
TypeError: DataFrame.set_sorted() takes 2 positional arguments but 3 were given
```
Use instead:
```pycon
>>> df.set_sorted("a").set_sorted("b")
```
### Default to raising on out-of-bounds indices in all `get`/`gather` operations
The default behavior was inconsistent between `get` and `gather` operations in various places. Now
all such operations will raise by default. Pass `null_on_oob=True` to restore previous behavior.
**Example**
Before:
```pycon
>>> s = pl.Series([[0, 1, 2], [0]])
>>> s.list.get(1)
shape: (2,)
Series: '' [i64]
[
1
null
]
```
After:
```pycon
>>> s.list.get(1)
Traceback (most recent call last):
...
polars.exceptions.ComputeError: get index is out of bounds
```
Use instead:
```pycon
>>> s.list.get(1, null_on_oob=True)
shape: (2,)
Series: '' [i64]
[
1
null
]
```
### Change default engine for `read_excel` to `"calamine"`
The `calamine` engine (available through the `fastexcel` package) has been added to Polars
relatively recently. It's much faster than the other engines, and was already the default for `xlsb`
and `xls` files. We now made it the default for all Excel files.
There may be subtle differences between this engine and the previous default (`xlsx2csv`). One clear
difference is that the `calamine` engine does not support the `engine_options` parameter. If you
cannot get your desired behavior with the `calamine` engine, specify `engine="xlsx2csv"` to restore
previous behavior.
### Example
Before:
```pycon
>>> pl.read_excel("data.xlsx", engine_options={"skip_empty_lines": True})
```
After:
```pycon
>>> pl.read_excel("data.xlsx", engine_options={"skip_empty_lines": True})
Traceback (most recent call last):
...
TypeError: read_excel() got an unexpected keyword argument 'skip_empty_lines'
```
Instead, explicitly specify the `xlsx2csv` engine or omit the `engine_options`:
```pycon
>>> pl.read_excel("data.xlsx", engine="xlsx2csv", engine_options={"skip_empty_lines": True})
```
### Remove class variables from some DataTypes
Some DataType classes had class variables. The `Datetime` class, for example, had `time_unit` and
`time_zone` as class variables. This was unintended: these should have been instance variables. This
has now been corrected.
**Example**
Before:
```pycon
>>> dtype = pl.Datetime
>>> dtype.time_unit is None
True
```
After:
```pycon
>>> dtype.time_unit is None
Traceback (most recent call last):
...
AttributeError: type object 'Datetime' has no attribute 'time_unit'
```
Use instead:
```pycon
>>> getattr(dtype, "time_unit", None) is None
True
```
### Change default `offset` in `group_by_dynamic` from 'negative `every`' to 'zero'
This affects the start of the first window in `group_by_dynamic`. The new behavior should align more
with user expectations.
**Example**
Before:
```pycon
>>> from datetime import date
>>> df = pl.DataFrame({
... "ts": [date(2020, 1, 1), date(2020, 1, 2), date(2020, 1, 3)],
... "value": [1, 2, 3],
... })
>>> df.group_by_dynamic("ts", every="1d", period="2d").agg("value")
shape: (4, 2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ ts â”† value â”‚
â”‚ --- â”† --- â”‚
â”‚ date â”† list[i64] â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 2019-12-31 â”† [1] â”‚
â”‚ 2020-01-01 â”† [1, 2] â”‚
â”‚ 2020-01-02 â”† [2, 3] â”‚
â”‚ 2020-01-03 â”† [3] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> df.group_by_dynamic("ts", every="1d", period="2d").agg("value")
shape: (3, 2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ ts â”† value â”‚
â”‚ --- â”† --- â”‚
â”‚ date â”† list[i64] â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 2020-01-01 â”† [1, 2] â”‚
â”‚ 2020-01-02 â”† [2, 3] â”‚
â”‚ 2020-01-03 â”† [3] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
### Change default serialization format of `LazyFrame/DataFrame/Expr`
The only serialization format available for the `serialize/deserialize` methods on Polars objects
was JSON. We added a more optimized binary format and made this the default. JSON serialization is
still available by passing `format="json"`.
**Example**
Before:
```pycon
>>> lf = pl.LazyFrame({"a": [1, 2, 3]}).sum()
>>> serialized = lf.serialize()
>>> serialized
'{"MapFunction":{"input":{"DataFrameScan":{"df":{"columns":[{"name":...'
>>> from io import StringIO
>>> pl.LazyFrame.deserialize(StringIO(serialized)).collect()
shape: (1, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ i64 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 6 â”‚
â””â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> lf = pl.LazyFrame({"a": [1, 2, 3]}).sum()
>>> serialized = lf.serialize()
>>> serialized
b'\xa1kMapFunction\xa2einput\xa1mDataFrameScan\xa4bdf...'
>>> from io import BytesIO # Note: using BytesIO instead of StringIO
>>> pl.LazyFrame.deserialize(BytesIO(serialized)).collect()
shape: (1, 1)
â”Œâ”€â”€â”€â”€â”€â”�
â”‚ a â”‚
â”‚ --- â”‚
â”‚ i64 â”‚
â•žâ•�â•�â•�â•�â•�â•¡
â”‚ 6 â”‚
â””â”€â”€â”€â”€â”€â”˜
```
### Constrain access to globals from `DataFrame.sql` in favor of `pl.sql`
The `sql` methods on `DataFrame` and `LazyFrame` can no longer access global variables. These
methods should be used for operating on the frame itself. For global access, there is now the
top-level `sql` function.
**Example**
Before:
```pycon
>>> df1 = pl.DataFrame({"id1": [1, 2]})
>>> df2 = pl.DataFrame({"id2": [3, 4]})
>>> df1.sql("SELECT * FROM df1 CROSS JOIN df2")
shape: (4, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ id1 â”† id2 â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† 3 â”‚
â”‚ 1 â”† 4 â”‚
â”‚ 2 â”† 3 â”‚
â”‚ 2 â”† 4 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
After:
```pycon
>>> df1.sql("SELECT * FROM df1 CROSS JOIN df2")
Traceback (most recent call last):
...
polars.exceptions.SQLInterfaceError: relation 'df1' was not found
```
Use instead:
```pycon
>>> pl.sql("SELECT * FROM df1 CROSS JOIN df2", eager=True)
shape: (4, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ id1 â”† id2 â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† 3 â”‚
â”‚ 1 â”† 4 â”‚
â”‚ 2 â”† 3 â”‚
â”‚ 2 â”† 4 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
### Remove re-export of type aliases
We have a lot of type aliases defined in the `polars.type_aliases` module. Some of these were
re-exported at the top-level and in the `polars.datatypes` module. These re-exports have been
removed.
We plan on adding a public `polars.typing` module in the future with a number of curated type
aliases. Until then, please define your own type aliases, or import from our `polars.type_aliases`
module. Note that the `type_aliases` module is not technically public, so use at your own risk.
**Example**
Before:
```python
def foo(dtype: pl.PolarsDataType) -> None: ...
```
After:
```python
PolarsDataType = pl.DataType | type[pl.DataType]
def foo(dtype: PolarsDataType) -> None: ...
```
### Streamline optional dependency definitions in `pyproject.toml`
We revisited to optional dependency definitions and made some minor changes. If you were using the
extras `fastexcel`, `gevent`, `matplotlib`, or `async`, this is a breaking change. Please update
your Polars installation to use the new extras.
**Example**
Before:
```bash
pip install 'polars[fastexcel,gevent,matplotlib]'
```
After:
```bash
pip install 'polars[calamine,async,graph]'
```
## Deprecations
### Issue `PerformanceWarning` when LazyFrame properties `schema/dtypes/columns/width` are used
Recent improvements to the correctness of the schema resolving in the lazy engine have had
significant performance impact on the cost of resolving the schema. It is no longer 'free' - in
fact, in complex pipelines with lazy file reading, resolving the schema can be relatively expensive.
Because of this, the schema-related properties on LazyFrame were no longer good API design.
Properties represent information that is already available, and just needs to be retrieved. However,
for the LazyFrame properties, accessing these may have significant performance cost.
To solve this, we added the `LazyFrame.collect_schema` method, which retrieves the schema and
returns a `Schema` object. The properties raise a `PerformanceWarning` and tell the user to use
`collect_schema` instead. We chose not to deprecate the properties for now to facilitate writing
code that is generic for both DataFrames and LazyFrames.
---
polars/docs/source/releases/upgrade/index.md
---
# About
Polars releases an upgrade guide alongside each breaking release. This guide is intended to help you
upgrade from an older Polars version to the new version.
Each guide contains all breaking changes that were not previously deprecated, as well as any
significant new deprecations.
A full list of all changes is available in the [changelog](../changelog.md).
!!! tip
It can be useful to upgrade to the latest non-breaking version before upgrading to a new breaking version.
This way, you can run your code and address any deprecation warnings.
The upgrade to the new breaking version should then go much more smoothly!
!!! tip
One of our maintainers has created a tool for automatically upgrading your Polars code to a later version.
It's based on the well-known pyupgrade tool.
Try out [polars-upgrade](https://github.com/MarcoGorelli/polars-upgrade) and let us know what you think!
!!! rust "Note"
There are no upgrade guides yet for Rust releases.
These will be added once the rate of breaking changes to the Rust API slows down and a [deprecation policy](../../development/versioning.md#deprecation-period) is added.
---
polars/docs/source/user-guide/ecosystem.md
---
# Ecosystem
## Introduction
On this page you can find a non-exhaustive list of libraries and tools that support Polars. As the
data ecosystem is evolving fast, more libraries will likely support Polars in the future. One of the
main drivers is that Polars makes adheres its memory layout to the `Apache Arrow` spec.
### Table of contents:
- [Apache Arrow](#apache-arrow)
- [Data visualisation](#data-visualisation)
- [IO](#io)
- [Machine learning](#machine-learning)
- [Other](#other)
---
### Apache Arrow
[Apache Arrow](https://arrow.apache.org/) enables zero-copy reads of data within the same process,
meaning that data can be directly accessed in its in-memory format without the need for copying or
serialisation. This enhances performance when integrating with different tools using Apache Arrow.
Polars is compatible with a wide range of libraries that also make use of Apache Arrow, like Pandas
and DuckDB.
### Data visualisation
See the [dedicated visualization section](misc/visualization.md).
### IO
#### Delta Lake
The [Delta Lake](https://github.com/delta-io/delta-rs) project aims to unlock the power of the
Deltalake for as many users and projects as possible by providing native low-level APIs aimed at
developers and integrators, as well as a high-level operations API that lets you query, inspect, and
operate your Delta Lake with ease. Delta Lake builds on the native Polars Parquet reader allowing
you to write standard Polars queries against a DeltaTable.
Read how to use Delta Lake with Polars
[at Delta Lake](https://delta-io.github.io/delta-rs/integrations/delta-lake-polars/#reading-a-delta-lake-table-with-polars).
### Machine Learning
#### Scikit Learn
The [Scikit Learn](https://scikit-learn.org/stable/) machine learning package accepts a Polars
`DataFrame` as input/output to all transformers and as input to models.
[skrub](https://skrub-data.org) helps encoding DataFrames for scikit-learn estimators (eg converting
dates or strings).
#### XGBoost & LightGBM
XGBoost and LightGBM are gradient boosting packages for doing regression or classification on
tabular data.
[XGBoost accepts Polars `DataFrame` and `LazyFrame` as input](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)
while LightGBM accepts Polars `DataFrame` as input.
#### Time series forecasting
The
[Nixtla time series forecasting packages](https://nixtlaverse.nixtla.io/statsforecast/docs/getting-started/getting_started_complete_polars.html)
accept a Polars `DataFrame` as input.
#### Hugging Face
Hugging Face is a platform for working with machine learning datasets and models.
[Polars can be used to work with datasets downloaded from Hugging Face](io/hugging-face.md).
#### Deep learning frameworks
A `DataFrame` can be transformed
[into a PyTorch format using `to_torch`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.to_torch.html)
or
[into a JAX format using `to_jax`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.to_jax.html).
### Other
#### DuckDB
[DuckDB](https://duckdb.org) is a high-performance analytical database system. It is designed to be
fast, reliable, portable, and easy to use. DuckDB provides a rich SQL dialect, with support far
beyond basic SQL. DuckDB supports arbitrary and nested correlated subqueries, window functions,
collations, complex types (arrays, structs), and more. Read about integration with Polars
[on the DuckDB website](https://duckdb.org/docs/guides/python/polars).
#### Great Tables
With [Great Tables](https://posit-dev.github.io/great-tables/articles/intro.html) anyone can make
wonderful-looking tables in Python. Here is a
[blog post](https://posit-dev.github.io/great-tables/blog/polars-styling/) on how to use Great
Tables with Polars.
#### LanceDB
[LanceDB](https://lancedb.com/) is a developer-friendly, serverless vector database for AI
applications. They have added a direct integration with Polars. LanceDB can ingest Polars
dataframes, return results as polars dataframes, and export the entire table as a polars lazyframe.
You can find a quick tutorial in their blog
[LanceDB + Polars](https://blog.lancedb.com/lancedb-polars-2d5eb32a8aa3)
#### Mage
[Mage](https://www.mage.ai) is an open-source data pipeline tool for transforming and integrating
data. Learn about integration between Polars and Mage at
[docs.mage.ai](https://docs.mage.ai/integrations/polars).
#### marimo
[marimo](https://marimo.io) is a reactive notebook for Python and SQL that models notebooks as
dataflow graphs. It offers built-in support for Polars, allowing seamless integration of Polars
dataframes in an interactive, reactive environment - such as displaying rich Polars tables, no-code
transformations of Polars dataframes, or selecting points on a Polars-backed reactive chart.
---
polars/docs/source/user-guide/getting-started.md
---
# Getting started
This chapter is here to help you get started with Polars. It covers all the fundamental features and
functionalities of the library, making it easy for new users to familiarise themselves with the
basics from initial installation and setup to core functionalities. If you're already an advanced
user or familiar with dataframes, feel free to skip ahead to the
[next chapter about installation options](installation.md).
## Installing Polars
=== ":fontawesome-brands-python: Python"
``` bash
pip install polars
```
=== ":fontawesome-brands-rust: Rust"
``` shell
cargo add polars -F lazy
# Or Cargo.toml
[dependencies]
polars = { version = "x", features = ["lazy", ...]}
```
## Reading & writing
Polars supports reading and writing for common file formats (e.g., csv, json, parquet), cloud
storage (S3, Azure Blob, BigQuery) and databases (e.g., postgres, mysql). Below, we create a small
dataframe and show how to write it to disk and read it back.
{{code_block('user-guide/getting-started','df',['DataFrame'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:df"
```
In the example below we write the dataframe to a csv file called `output.csv`. After that, we read
it back using `read_csv` and then print the result for inspection.
{{code_block('user-guide/getting-started','csv',['read_csv','write_csv'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:csv"
```
For more examples on the CSV file format and other data formats, see the [IO section](io/index.md)
of the user guide.
## Expressions and contexts
_Expressions_ are one of the main strengths of Polars because they provide a modular and flexible
way of expressing data transformations.
Here is an example of a Polars expression:
```py
pl.col("weight") / (pl.col("height") ** 2)
```
As you might be able to guess, this expression takes the column named â€œweightâ€� and divides its
values by the square of the values in the column â€œheightâ€�, computing a person's BMI. Note that the
code above expresses an abstract computation: it's only inside a Polars _context_ that the
expression materalizes into a series with the results.
Below, we will show examples of Polars expressions inside different contexts:
- `select`
- `with_columns`
- `filter`
- `group_by`
For a more
[detailed exploration of expressions and contexts see the respective user guide section](concepts/expressions-and-contexts.md).
### `select`
The context `select` allows you to select and manipulate columns from a dataframe. In the simplest
case, each expression you provide will map to a column in the result dataframe:
{{code_block('user-guide/getting-started','select',['select','alias','Expr.dt'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:select"
```
Polars also supports a feature called â€œexpression expansionâ€�, in which one expression acts as
shorthand for multiple expressions. In the example below, we use expression expansion to manipulate
the columns â€œweightâ€�Â and â€œheightâ€� with a single expression. When using expression expansion you can
use `.name.suffix` to add a suffix to the names of the original columns:
{{code_block('user-guide/getting-started','expression-expansion',['select','alias','Expr.name'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:expression-expansion"
```
You can check other sections of the user guide to learn more about
[basic operations](expressions/basic-operations.md) or
[column selections in expression expansion](expressions/expression-expansion.md).
### `with_columns`
The context `with_columns` is very similar to the context `select` but `with_columns` adds columns
to the dataframe instead of selecting them. Notice how the resulting dataframe contains the four
columns of the original dataframe plus the two new columns introduced by the expressions inside
`with_columns`:
{{code_block('user-guide/getting-started','with_columns',['with_columns'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:with_columns"
```
In the example above we also decided to use named expressions instead of the method `alias` to
specify the names of the new columns. Other contexts like `select` and `group_by` also accept named
expressions.
### `filter`
The context `filter` allows us to create a second dataframe with a subset of the rows of the
original one:
{{code_block('user-guide/getting-started','filter',['filter','Expr.dt'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:filter"
```
You can also provide multiple predicate expressions as separate parameters, which is more convenient
than putting them all together with `&`:
{{code_block('user-guide/getting-started','filter-multiple',['filter','is_between'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:filter-multiple"
```
### `group_by`
The context `group_by` can be used to group together the rows of the dataframe that share the same
value across one or more expressions. The example below counts how many people were born in each
decade:
{{code_block('user-guide/getting-started','group_by',['group_by','alias','Expr.dt'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:group_by"
```
The keyword argument `maintain_order` forces Polars to present the resulting groups in the same
order as they appear in the original dataframe. This slows down the grouping operation but is used
here to ensure reproducibility of the examples.
After using the context `group_by` we can use `agg` to compute aggregations over the resulting
groups:
{{code_block('user-guide/getting-started','group_by-agg',['group_by','agg'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:group_by-agg"
```
### More complex queries
Contexts and the expressions within can be chained to create more complex queries according to your
needs. In the example below we combine some of the contexts we have seen so far to create a more
complex query:
{{code_block('user-guide/getting-started','complex',['group_by','agg','select','with_columns','Expr.str','Expr.list'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:complex"
```
## Combining dataframes
Polars provides a number of tools to combine two dataframes. In this section, we show an example of
a join and an example of a concatenation.
### Joining dataframes
Polars provides many different join algorithms. The example below shows how to use a left outer join
to combine two dataframes when a column can be used as a unique identifier to establish a
correspondence between rows across the dataframes:
{{code_block('user-guide/getting-started','join',['join'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:join"
```
Polars provides many different join algorithms that you can learn about in the
[joins section of the user guide](transformations/joins.md).
### Concatenating dataframes
Concatenating dataframes creates a taller or wider dataframe, depending on the method used. Assuming
we have a second dataframe with data from other people, we could use vertical concatenation to
create a taller dataframe:
{{code_block('user-guide/getting-started','concat',['concat'])}}
```python exec="on" result="text" session="getting-started"
--8<-- "python/user-guide/getting-started.py:concat"
```
Polars provides vertical and horizontal concatenation, as well as diagonal concatenation. You can
learn more about these in the
[concatenations section of the user guide](transformations/concatenation.md).
---
polars/docs/source/user-guide/gpu-support.md
---
# GPU Support [Open Beta]
Polars provides an in-memory, GPU-accelerated execution engine for Python users of the Lazy API on
NVIDIA GPUs using [RAPIDS cuDF](https://docs.rapids.ai/api/cudf/stable/). This functionality is
available in Open Beta and is undergoing rapid development.
### System Requirements
- NVIDIA Voltaâ„¢ or higher GPU with [compute capability](https://developer.nvidia.com/cuda-gpus) 7.0+
- CUDA 11 or CUDA 12
- Linux or Windows Subsystem for Linux 2 (WSL2)
See the [RAPIDS installation guide](https://docs.rapids.ai/install#system-req) for full details.
### Installation
You can install the GPU backend for Polars with a feature flag as part of a normal
[installation](installation.md).
=== ":fontawesome-brands-python: Python"
```bash
pip install polars[gpu]
```
!!! note Installation on a CUDA 11 system
If you have CUDA 11, the installation line also needs the NVIDIA package index to get the CUDA 11 package.
=== ":fontawesome-brands-python: Python"
```bash
pip install --extra-index-url=https://pypi.nvidia.com polars cudf-polars-cu11
```
### Usage
Having built a query using the lazy API [as normal](lazy/index.md), GPU-enabled execution is
requested by running `.collect(engine="gpu")` instead of `.collect()`.
{{ code_header("python", [], []) }}
```python
--8<-- "python/user-guide/lazy/gpu.py:setup"
result = q.collect(engine="gpu")
print(result)
```
```python exec="on" result="text" session="user-guide/lazy"
--8<-- "python/user-guide/lazy/gpu.py:setup"
--8<-- "python/user-guide/lazy/gpu.py:simple-result"
```
For more detailed control over the execution, for example to specify which GPU to use on a multi-GPU
node, we can provide a `GPUEngine` object. By default, the GPU engine will use a configuration
applicable to most use cases.
{{ code_header("python", [], []) }}
```python
--8<-- "python/user-guide/lazy/gpu.py:engine-setup"
result = q.collect(engine=pl.GPUEngine(device=1))
print(result)
```
```python exec="on" result="text" session="user-guide/lazy"
--8<-- "python/user-guide/lazy/gpu.py:engine-setup"
--8<-- "python/user-guide/lazy/gpu.py:engine-result"
```
### How It Works
When you use the GPU-accelerated engine, Polars creates and optimizes a query plan and dispatches to
a [RAPIDS](https://rapids.ai/) cuDF-based physical execution engine to compute the results on NVIDIA
GPUs. The final result is returned as a normal CPU-backed Polars dataframe.
### What's Supported on the GPU?
GPU support is currently in Open Beta and the engine is undergoing rapid development. The engine
currently supports many, but not all, of the core expressions and data types.
Since expressions are composable, it's not feasible to list a full matrix of expressions supported
on the GPU. Instead, we provide a list of the high-level categories of expressions and interfaces
that are currently supported and not supported.
#### Supported
- LazyFrame API
- SQL API
- I/O from CSV, Parquet, ndjson, and in-memory CPU DataFrames.
- Operations on numeric, logical, string, and datetime types
- String processing
- Aggregations and grouped aggregations
- Joins
- Filters
- Missing data
- Concatenation
#### Not Supported
- Eager DataFrame API
- Streaming API
- Operations on categorical, struct, and list data types
- Rolling aggregations
- Time series resampling
- Timezones
- Folds
- User-defined functions
- JSON, Excel, and Database file formats
#### Did my query use the GPU?
The release of the GPU engine in Open Beta implies that we expect things to work well, but there are
still some rough edges we're working on. In particular the full breadth of the Polars expression API
is not yet supported. With fallback to the CPU, your query _should_ complete, but you might not
observe any change in the time it takes to execute. There are two ways to get more information on
whether the query ran on the GPU.
When running in verbose mode, any queries that cannot execute on the GPU will issue a
`PerformanceWarning`:
{{ code_header("python", [], []) }}
```python
--8<-- "python/user-guide/lazy/gpu.py:fallback-setup"
with pl.Config() as cfg:
cfg.set_verbose(True)
result = q.collect(engine="gpu")
print(result)
```
```python exec="on" result="text" session="user-guide/lazy"
--8<-- "python/user-guide/lazy/gpu.py:fallback-setup"
print(
"PerformanceWarning: Query execution with GPU not supported, reason: \n"
": Grouped rolling window not implemented"
)
print("# some details elided")
print()
print(q.collect())
```
To disable fallback, and have the GPU engine raise an exception if a query is unsupported, we can
pass an appropriately configured `GPUEngine` object:
{{ code_header("python", [], []) }}
```python
q.collect(engine=pl.GPUEngine(raise_on_fail=True))
```
```pytb
Traceback (most recent call last):
File "", line 1, in 
File "/home/coder/third-party/polars/py-polars/polars/lazyframe/frame.py", line 2035, in collect
return wrap_df(ldf.collect(callback))
polars.exceptions.ComputeError: 'cuda' conversion failed: NotImplementedError: Grouped rolling window not implemented
```
Currently, only the proximal cause of failure to execute on the GPU is reported, we plan to extend
this functionality to report all unsupported operations for a query.
### Testing
The Polars and NVIDIA RAPIDS teams run comprehensive unit and integration tests to ensure that the
GPU-accelerated Polars backend works smoothly.
The **full** Polars test suite is run on every commit made to the GPU engine, ensuring consistency
of results.
The GPU engine currently passes 99.2% of the Polars unit tests with CPU fallback enabled. Without
CPU fallback, the GPU engine passes 88.8% of the Polars unit tests. With fallback, there are
approximately 100 failing tests: around 40 of these fail due to mismatching debug output; there are
some cases where the GPU engine produces the a correct result but uses a different data type; the
remainder are cases where we do not correctly determine that a query is unsupported and therefore
fail at runtime, instead of falling back.
### When Should I Use a GPU?
Based on our benchmarking, you're most likely to observe speedups using the GPU engine when your
workflow's profile is dominated by grouped aggregations and joins. In contrast I/O bound queries
typically show similar performance on GPU and CPU. GPUs typically have less RAM than CPU systems,
therefore very large datasets will fail due to out of memory errors. Based on our testing, raw
datasets of 50-100 GiB fit (depending on the workflow) well with a GPU with 80GiB of memory.
### CPU-GPU Interoperability
Both the CPU and GPU engine use the Apache Arrow columnar memory specification, making it possible
to quickly move data between the CPU and GPU. Additionally, files written by one engine can be read
by the other engine.
When using GPU mode, your workflow won't fail if something isn't supported. When you run
`collect(engine="gpu")`, the optimized query plan is inspected to see whether it can be executed on
the GPU. If it can't, it will transparently fall back to the standard Polars engine and run on the
CPU.
GPU execution is only available in the Lazy API, so materialized DataFrames will reside in CPU
memory when the query execution finishes.
### Providing feedback
Please report issues, and missing features, on the Polars
[issue tracker](https://github.com/pola-rs/polars/issues).
---
polars/docs/source/user-guide/installation.md
---
# Installation
Polars is a library and installation is as simple as invoking the package manager of the
corresponding programming language.
=== ":fontawesome-brands-python: Python"
``` bash
pip install polars
# Or for legacy CPUs without AVX2 support
pip install polars-lts-cpu
```
=== ":fontawesome-brands-rust: Rust"
``` shell
cargo add polars -F lazy
# Or Cargo.toml
[dependencies]
polars = { version = "x", features = ["lazy", ...]}
```
## Big Index
By default, Polars dataframes are limited to $2^{32}$ rows (~4.3 billion). Increase this limit to
$2^{64}$ (~18 quintillion) by enabling the big index extension:
=== ":fontawesome-brands-python: Python"
``` bash
pip install polars-u64-idx
```
=== ":fontawesome-brands-rust: Rust"
``` shell
cargo add polars -F bigidx
# Or Cargo.toml
[dependencies]
polars = { version = "x", features = ["bigidx", ...] }
```
## Legacy CPU
To install Polars for Python on an old CPU without
[AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) support, run:
=== ":fontawesome-brands-python: Python"
``` bash
pip install polars-lts-cpu
```
## Importing
To use the library, simply import it into your project:
=== ":fontawesome-brands-python: Python"
``` python
import polars as pl
```
=== ":fontawesome-brands-rust: Rust"
``` rust
use polars::prelude::*;
```
## Feature flags
By using the above command you install the core of Polars onto your system. However, depending on
your use case, you might want to install the optional dependencies as well. These are made optional
to minimize the footprint. The flags are different depending on the programming language. Throughout
the user guide we will mention when a functionality used requires an additional dependency.
### Python
```text
# For example
pip install 'polars[numpy,fsspec]'
```
#### All
| Tag | Description |
| --- | ---------------------------------- |
| all | Install all optional dependencies. |
#### GPU
| Tag | Description |
| --- | --------------------------- |
| gpu | Run queries on NVIDIA GPUs. |
!!! note
See [GPU support](gpu-support.md) for more detailed instructions and
prerequisites.
#### Interoperability
| Tag | Description |
| -------- | -------------------------------------------------- |
| pandas | Convert data to and from pandas dataframes/series. |
| numpy | Convert data to and from NumPy arrays. |
| pyarrow | Convert data to and from PyArrow tables/arrays. |
| pydantic | Convert data from Pydantic models to Polars. |
#### Excel
| Tag | Description |
| ---------- | ------------------------------------------------ |
| calamine | Read from Excel files with the calamine engine. |
| openpyxl | Read from Excel files with the openpyxl engine. |
| xlsx2csv | Read from Excel files with the xlsx2csv engine. |
| xlsxwriter | Write to Excel files with the XlsxWriter engine. |
| excel | Install all supported Excel engines. |
#### Database
| Tag | Description |
| ---------- | ------------------------------------------------------------------------------------ |
| adbc | Read from and write to databases with the Arrow Database Connectivity (ADBC) engine. |
| connectorx | Read from databases with the ConnectorX engine. |
| sqlalchemy | Write to databases with the SQLAlchemy engine. |
| database | Install all supported database engines. |
#### Cloud
| Tag | Description |
| ------ | ------------------------------------------- |
| fsspec | Read from and write to remote file systems. |
#### Other I/O
| Tag | Description |
| --------- | ------------------------------------ |
| deltalake | Read from and write to Delta tables. |
| iceberg | Read from Apache Iceberg tables. |
#### Other
| Tag | Description |
| ----------- | ----------------------------------------------- |
| async | Collect LazyFrames asynchronously. |
| cloudpickle | Serialize user-defined functions. |
| graph | Visualize LazyFrames as a graph. |
| plot | Plot dataframes through the `plot` namespace. |
| style | Style dataframes through the `style` namespace. |
| timezone | Timezone support[^note]. |
[^note]: Only needed if you are on Windows.
### Rust
```toml
# Cargo.toml
[dependencies]
polars = { version = "0.26.1", features = ["lazy", "temporal", "describe", "json", "parquet", "dtype-datetime"] }
```
The opt-in features are:
- Additional data types:
- `dtype-date`
- `dtype-datetime`
- `dtype-time`
- `dtype-duration`
- `dtype-i8`
- `dtype-i16`
- `dtype-u8`
- `dtype-u16`
- `dtype-categorical`
- `dtype-struct`
- `lazy` - Lazy API:
- `regex` - Use regexes in column selection.
- `dot_diagram` - Create dot diagrams from lazy logical plans.
- `sql` - Pass SQL queries to Polars.
- `streaming` - Be able to process datasets that are larger than RAM.
- `random` - Generate arrays with randomly sampled values
- `ndarray`- Convert from `DataFrame` to `ndarray`
- `temporal` - Conversions between [Chrono](https://docs.rs/chrono/) and Polars for temporal data types
- `timezones` - Activate timezone support.
- `strings` - Extra string utilities for `StringChunked`:
- `string_pad` - for `pad_start`, `pad_end`, `zfill`.
- `string_to_integer` - for `parse_int`.
- `object` - Support for generic ChunkedArrays called `ObjectChunked` (generic over `T`).
These are downcastable from Series through the [Any](https://doc.rust-lang.org/std/any/index.html) trait.
- Performance related:
- `nightly` - Several nightly only features such as SIMD and specialization.
- `performant` - more fast paths, slower compile times.
- `bigidx` - Activate this feature if you expect >> $2^{32}$ rows.
This allows polars to scale up way beyond that by using `u64` as an index.
Polars will be a bit slower with this feature activated as many data structures
are less cache efficient.
- `cse` - Activate common subplan elimination optimization.
- IO related:
- `serde` - Support for [serde](https://crates.io/crates/serde) serialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
- `serde-lazy` - Support for [serde](https://crates.io/crates/serde) serialization and deserialization.
Can be used for JSON and more serde supported serialization formats.
- `parquet` - Read Apache Parquet format.
- `json` - JSON serialization.
- `ipc` - Arrow's IPC format serialization.
- `decompress` - Automatically infer compression of csvs and decompress them.
Supported compressions:
- gzip
- zlib
- zstd
- Dataframe operations:
- `dynamic_group_by` - Group by based on a time window instead of predefined keys.
Also activates rolling window group by operations.
- `sort_multiple` - Allow sorting a dataframe on multiple columns.
- `rows` - Create dataframe from rows and extract rows from `dataframes`.
Also activates `pivot` and `transpose` operations.
- `join_asof` - Join ASOF, to join on nearest keys instead of exact equality match.
- `cross_join` - Create the Cartesian product of two dataframes.
- `semi_anti_join` - SEMI and ANTI joins.
- `row_hash` - Utility to hash dataframe rows to `UInt64Chunked`.
- `diagonal_concat` - Diagonal concatenation thereby combining different schemas.
- `dataframe_arithmetic` - Arithmetic between dataframes and other dataframes or series.
- `partition_by` - Split into multiple dataframes partitioned by groups.
- Series/expression operations:
- `is_in` - Check for membership in Series.
- `zip_with` - Zip two `Series` / `ChunkedArray`s.
- `round_series` - round underlying float types of series.
- `repeat_by` - Repeat element in an array a number of times specified by another array.
- `is_first_distinct` - Check if element is first unique value.
- `is_last_distinct` - Check if element is last unique value.
- `checked_arithmetic` - checked arithmetic returning `None` on invalid operations.
- `dot_product` - Dot/inner product on series and expressions.
- `concat_str` - Concatenate string data in linear time.
- `reinterpret` - Utility to reinterpret bits to signed/unsigned.
- `take_opt_iter` - Take from a series with `Iterator>`.
- `mode` - Return the most frequently occurring value(s).
- `cum_agg` - `cum_sum`, `cum_min`, and `cum_max`, aggregations.
- `rolling_window` - rolling window functions, like `rolling_mean`.
- `interpolate` - Interpolate intermediate `None` values.
- `extract_jsonpath` - [Run `jsonpath` queries on `StringChunked`](https://goessner.net/articles/JsonPath/).
- `list` - List utils:
- `list_gather` - take sublist by multiple indices.
- `rank` - Ranking algorithms.
- `moment` - Kurtosis and skew statistics.
- `ewma` - Exponential moving average windows.
- `abs` - Get absolute values of series.
- `arange` - Range operation on series.
- `product` - Compute the product of a series.
- `diff` - `diff` operation.
- `pct_change` - Compute change percentages.
- `unique_counts` - Count unique values in expressions.
- `log` - Logarithms for series.
- `list_to_struct` - Convert `List` to `Struct` data types.
- `list_count` - Count elements in lists.
- `list_eval` - Apply expressions over list elements.
- `cumulative_eval` - Apply expressions over cumulatively increasing windows.
- `arg_where` - Get indices where condition holds.
- `search_sorted` - Find indices where elements should be inserted to maintain order.
- `offset_by` - Add an offset to dates that take months and leap years into account.
- `trigonometry` - Trigonometric functions.
- `sign` - Compute the element-wise sign of a series.
- `propagate_nans` - `NaN`-propagating min/max aggregations.
- Dataframe pretty printing:
- `fmt` - Activate dataframe formatting.
---
polars/docs/source/user-guide/misc/arrow.md
---
# Arrow producer/consumer
## Using pyarrow
Polars can move data in and out of arrow zero copy. This can be done either via pyarrow or natively.
Let's first start by showing the pyarrow solution:
{{code_block('user-guide/misc/arrow','to_arrow',[])}}
```
pyarrow.Table
foo: int64
bar: large_string
----
foo: [[1,2,3]]
bar: [["ham","spam","jam"]]
```
Or if you want to ensure the output is zero-copy:
{{code_block('user-guide/misc/arrow','to_arrow_zero',[])}}
```
pyarrow.Table
foo: int64
bar: string_view
----
foo: [[1,2,3]]
bar: [["ham","spam","jam"]]
```
Importing from pyarrow can be achieved with `pl.from_arrow`.
## Using the Arrow PyCapsule Interface
As of Polars v1.3 and higher, Polars implements the
[Arrow PyCapsule Interface](https://arrow.apache.org/docs/format/CDataInterface/PyCapsuleInterface.html),
a protocol for sharing Arrow data across Python libraries.
### Exporting data from Polars to pyarrow
To convert a Polars `DataFrame` to a `pyarrow.Table`, use the `pyarrow.table` constructor:
!!! note
This requires pyarrow v15 or higher.
{{code_block('user-guide/misc/arrow_pycapsule','to_arrow',[])}}
```
pyarrow.Table
foo: int64
bar: string_view
----
foo: [[1,2,3]]
bar: [["ham","spam","jam"]]
```
To convert a Polars `Series` to a `pyarrow.ChunkedArray`, use the `pyarrow.chunked_array`
constructor.
{{code_block('user-guide/misc/arrow_pycapsule','to_arrow_series',[])}}
```
[
[
1,
2,
3
]
]
```
You can also pass a `Series` to the `pyarrow.array` constructor to create a contiguous array. Note
that this will not be zero-copy if the underlying `Series` had multiple chunks.
{{code_block('user-guide/misc/arrow_pycapsule','to_arrow_array_rechunk',[])}}
```
[
1,
2,
3
]
```
### Importing data from pyarrow to Polars
We can pass the pyarrow `Table` back to Polars by using the `polars.DataFrame` constructor:
{{code_block('user-guide/misc/arrow_pycapsule','to_polars',[])}}
```
shape: (3, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”�
â”‚ foo â”† bar â”‚
â”‚ --- â”† --- â”‚
â”‚ i64 â”† str â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† ham â”‚
â”‚ 2 â”† spam â”‚
â”‚ 3 â”† jam â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```
Similarly, we can pass the pyarrow `ChunkedArray` or `Array` back to Polars by using the
`polars.Series` constructor:
{{code_block('user-guide/misc/arrow_pycapsule','to_polars_series',[])}}
```
shape: (3,)
Series: '' [i64]
[
1
2
3
]
```
### Usage with other arrow libraries
There's a [growing list](https://github.com/apache/arrow/issues/39195#issuecomment-2245718008) of
libraries that support the PyCapsule Interface directly. Polars `Series` and `DataFrame` objects
work automatically with every such library.
### For library maintainers
If you're developing a library that you wish to integrate with Polars, it's suggested to implement
the
[Arrow PyCapsule Interface](https://arrow.apache.org/docs/format/CDataInterface/PyCapsuleInterface.html)
yourself. This comes with a number of benefits:
- Zero-copy exchange for both Polars Series and DataFrame
- No required dependency on pyarrow.
- No direct dependency on Polars.
- Harder to cause memory leaks than handling pointers as raw integers.
- Automatic zero-copy integration other PyCapsule Interface-supported libraries.
## Using Polars directly
Polars can also consume and export to and import from the
[Arrow C Data Interface](https://arrow.apache.org/docs/format/CDataInterface.html) directly. This is
recommended for libraries that don't support the Arrow PyCapsule Interface and want to interop with
Polars without requiring a pyarrow installation.
- To export `ArrowArray` C structs, Polars exposes: `Series._export_arrow_to_c`.
- To import an `ArrowArray` C struct, Polars exposes `Series._import_arrow_from_c`.
---
polars/docs/source/user-guide/misc/comparison.md
---
# Comparison with other tools
These are several libraries and tools that share similar functionalities with Polars. This often
leads to questions from data experts about what the differences are. Below is a short comparison
between some of the more popular data processing tools and Polars, to help data experts make a
deliberate decision on which tool to use.
You can find performance benchmarks (h2oai benchmark) of these tools here:
[Polars blog post](https://pola.rs/posts/benchmarks/) or a more recent benchmark
[done by DuckDB](https://duckdblabs.github.io/db-benchmark/)
### Pandas
Pandas stands as a widely-adopted and comprehensive tool in Python data analysis, renowned for its
rich feature set and strong community support. However, due to its single threaded nature, it can
struggle with performance and memory usage on medium and large datasets.
In contrast, Polars is optimised for high-performance multithreaded computing on single nodes,
providing significant improvements in speed and memory efficiency, particularly for medium to large
data operations. Its more composable and stricter API results in greater expressiveness and fewer
schema-related bugs.
### Dask
Dask extends Pandas' capabilities to large, distributed datasets. Dask mimics Pandas' API, offering
a familiar environment for Pandas users, but with the added benefit of parallel and distributed
computing.
While Dask excels at scaling Pandas workflows across clusters, it only supports a subset of the
Pandas API and therefore cannot be used for all use cases. Polars offers a more versatile API that
delivers strong performance within the constraints of a single node.
The choice between Dask and Polars often comes down to familiarity with the Pandas API and the need
for distributed processing for extremely large datasets versus the need for efficiency and speed in
a vertically scaled environment for a wide range of use cases.
### Modin
Similar to Dask. In 2023, Snowflake acquired Ponder, the organisation that maintains Modin.
### Spark
Spark (specifically PySpark) represents a different approach to large-scale data processing. While
Polars has an optimised performance for single-node environments, Spark is designed for distributed
data processing across clusters, making it suitable for extremely large datasets.
However, Spark's distributed nature can introduce complexity and overhead, especially for small
datasets and tasks that can run on a single machine. Another consideration is collaboration between
data scientists and engineers. As they typically work with different tools (Pandas and Pyspark),
refactoring is often required by engineers to deploy data scientists' data processing pipelines.
Polars offers a single syntax that, due to vertical scaling, works in local environments and on a
single machine in the cloud.
The choice between Polars and Spark often depends on the scale of data and the specific requirements
of the processing task. If you need to process TBs of data, Spark is a better choice.
### DuckDB
Polars and DuckDB have many similarities. However, DuckDB is focused on providing an in-process SQL
OLAP database management system, while Polars is focused on providing a scalable `DataFrame`
interface to many languages. The different front-ends lead to different optimisation strategies and
different algorithm prioritisation. The interoperability between both is zero-copy. DuckDB offers a
guide on [how to integrate with Polars](https://duckdb.org/docs/guides/python/polars.html).
---
polars/docs/source/user-guide/misc/multiprocessing.md
---
# Multiprocessing
TLDR: if you find that using Python's built-in `multiprocessing` module together with Polars results
in a Polars error about multiprocessing methods, you should make sure you are using `spawn`, not
`fork`, as the starting method:
{{code_block('user-guide/misc/multiprocess','recommendation',[])}}
## When not to use multiprocessing
Before we dive into the details, it is important to emphasize that Polars has been built from the
start to use all your CPU cores. It does this by executing computations which can be done in
parallel in separate threads. For example, requesting two expressions in a `select` statement can be
done in parallel, with the results only being combined at the end. Another example is aggregating a
value within groups using `group_by().agg()`, each group can be evaluated separately. It is
very unlikely that the `multiprocessing` module can improve your code performance in these cases. If
you're using the GPU Engine with Polars you should also avoid manual multiprocessing. When used
simultaneously, they can compete for system memory and processing power, leading to reduced
performance.
See [the optimizations section](../lazy/optimizations.md) for more optimizations.
## When to use multiprocessing
Although Polars is multithreaded, other libraries may be single-threaded. When the other library is
the bottleneck, and the problem at hand is parallelizable, it makes sense to use multiprocessing to
gain a speed up.
## The problem with the default multiprocessing config
### Summary
The [Python multiprocessing documentation](https://docs.python.org/3/library/multiprocessing.html)
lists the three methods to create a process pool:
1. spawn
1. fork
1. forkserver
The description of fork is (as of 2022-10-15):
> The parent process uses os.fork() to fork the Python interpreter. The child process, when it
> begins, is effectively identical to the parent process. All resources of the parent are inherited
> by the child process. Note that safely forking a multithreaded process is problematic.
> Available on Unix only. The default on Unix.
The short summary is: Polars is multithreaded as to provide strong performance out-of-the-box. Thus,
it cannot be combined with `fork`. If you are on Unix (Linux, BSD, etc), you are using `fork`,
unless you explicitly override it.
The reason you may not have encountered this before is that pure Python code, and most Python
libraries, are (mostly) single threaded. Alternatively, you are on Windows or MacOS, on which `fork`
is not even available as a method (for MacOS it was up to Python 3.7).
Thus one should use `spawn`, or `forkserver`, instead. `spawn` is available on all platforms and the
safest choice, and hence the recommended method.
### Example
The problem with `fork` is in the copying of the parent's process. Consider the example below, which
is a slightly modified example posted on the
[Polars issue tracker](https://github.com/pola-rs/polars/issues/3144):
{{code_block('user-guide/misc/multiprocess','example1',[])}}
Using `fork` as the method, instead of `spawn`, will cause a dead lock.
The fork method is equivalent to calling `os.fork()`, which is a system call as defined in
[the POSIX standard](https://pubs.opengroup.org/onlinepubs/9699919799/functions/fork.html):
> A process shall be created with a single thread. If a multi-threaded process calls fork(), the new
> process shall contain a replica of the calling thread and its entire address space, possibly
> including the states of mutexes and other resources. Consequently, to avoid errors, the child
> process may only execute async-signal-safe operations until such time as one of the exec functions
> is called.
In contrast, `spawn` will create a completely new fresh Python interpreter, and not inherit the
state of mutexes.
So what happens in the code example? For reading the file with `pl.read_parquet` the file has to be
locked. Then `os.fork()` is called, copying the state of the parent process, including mutexes. Thus
all child processes will copy the file lock in an acquired state, leaving them hanging indefinitely
waiting for the file lock to be released, which never happens.
What makes debugging these issues tricky is that `fork` can work. Change the example to not having
the call to `pl.read_parquet`:
{{code_block('user-guide/misc/multiprocess','example2',[])}}
This works fine. Therefore debugging these issues in larger code bases, i.e. not the small toy
examples here, can be a real pain, as a seemingly unrelated change can break your multiprocessing
code. In general, one should therefore never use the `fork` start method with multithreaded
libraries unless there are very specific requirements that cannot be met otherwise.
### Pro's and cons of fork
Based on the example, you may think, why is `fork` available in Python to start with?
First, probably because of historical reasons: `spawn` was added to Python in version 3.4, whilst
`fork` has been part of Python from the 2.x series.
Second, there are several limitations for `spawn` and `forkserver` that do not apply to `fork`, in
particular all arguments should be pickleable. See the
[Python multiprocessing docs](https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods)
for more information.
Third, because it is faster to create new processes compared to `spawn`, as `spawn` is effectively
`fork` + creating a brand new Python process without the locks by calling
[execv](https://pubs.opengroup.org/onlinepubs/9699919799/functions/exec.html). Hence the warning in
the Python docs that it is slower: there is more overhead to `spawn`. However, in almost all cases,
one would like to use multiple processes to speed up computations that take multiple minutes or even
hours, meaning the overhead is negligible in the grand scheme of things. And more importantly, it
actually works in combination with multithreaded libraries.
Fourth, `spawn` starts a new process, and therefore it requires code to be importable, in contrast
to `fork`. In particular, this means that when using `spawn` the relevant code should not be in the
global scope, such as in Jupyter notebooks or in plain scripts. Hence in the examples above, we
define functions where we spawn within, and run those functions from a `__main__` clause. This is
not an issue for typical projects, but during quick experimentation in notebooks it could fail.
## References
1. https://docs.python.org/3/library/multiprocessing.html
1. https://pythonspeed.com/articles/python-multiprocessing/
1. https://pubs.opengroup.org/onlinepubs/9699919799/functions/fork.html
1. https://bnikolic.co.uk/blog/python/parallelism/2019/11/13/python-forkserver-preload.html
---
polars/docs/source/user-guide/misc/polars_llms.md
---
# Generating Polars code with LLMs
Large Language Models (LLMs) can sometimes return Pandas code or invalid Polars code in their
output. This guide presents approaches that help LLMs generate valid Polars code more consistently.
These approaches have been developed by the Polars community through testing model responses to
various inputs. If you find additional effective approaches for generating Polars code from LLMs,
please raise a [pull request](https://github.com/pola-rs/polars/pulls).
## System prompt
Many LLMs allow you to provide a system prompt that is included with every individual prompt you
send to the model. In the system prompt, you can specify your preferred defaults, such as "Use
Polars as the default dataframe library". Including such a system prompt typically leads to models
consistently generating Polars code rather than Pandas code.
You can set this system prompt in the settings menu of both web-based LLMs like ChatGPT and
IDE-based LLMs like Cursor. Refer to each application's documentation for specific instructions.
## Enable web search
Some LLMs can search the web to access information beyond their pre-training data. Enabling web
search allows an LLM to reference up-to-date Polars documentation for the current API.
Some IDE-based LLMs can index the Polars API documentation and reference this when generating code.
For example, in Cursor you can add Polars as a custom docs source and instruct the agent to
reference the Polars documentation in a prompt.
However, web search does not yet guarantee that valid code will be produced. If a model is confident
in a result based on its pre-training data, it may not incorporate web search results in its output.
The Polars API pages also have AI-enabled search to help you find the information you need more
easily.
## Provide examples
You can guide LLMs to use correct syntax by including relevant examples in your prompt.
For instance, this basic query:
```python
df = pl.DataFrame({
"id": ["a", "b", "a", "b", "c"],
"score": [1, 2, 1, 3, 3],
"year": [2020, 2020, 2021, 2021, 2021],
})
# Compute average of score by id
```
Often results in outdated `groupby` syntax instead of the correct `group_by`.
However, including a simple example from the Polars `group_by` documentation (preferably with web
search enabled) like this:
```python
df = pl.DataFrame({
"id": ["a", "b", "a", "b", "c"],
"score": [1, 2, 1, 3, 3],
"year": [2020, 2020, 2021, 2021, 2021],
})
# Compute average of score by id
# Examples of Polars code:
# df.group_by("a").agg(pl.col("b").mean())
```
Produces valid outputs more often. This approach has been validated across several leading models.
The combination of web search and examples is more effective than either independently. Model
outputs indicate that when an example contradicts the model's pre-trained expectations, it seems
more likely to trigger a web search for verification.
Additionally, explicit instructions like "use `group_by` instead of `groupby`" can be effective in
guiding the model to use correct syntax.
Common examples such as `df.group_by("a").agg(pl.col("b").mean())` can also be added the system
prompt for more consistency.
---
polars/docs/source/user-guide/misc/styling.md
---
# Styling
Data in a Polars `DataFrame` can be styled for presentation use the `DataFrame.style` property. This
returns a `GT` object from
[Great Tables](https://posit-dev.github.io/great-tables/articles/intro.html), which enables
structuring, formatting, and styling for table display.
{{code_block('user-guide/misc/styling','dataframe',[])}}
```python exec="on" result="text" session="user-guide/misc/styling"
--8<-- "python/user-guide/misc/styling.py:dataframe"
```
## Structure: add header title
{{code_block('user-guide/misc/styling','structure-header',[])}}
```python exec="on" session="user-guide/misc/styling"
--8<-- "python/user-guide/misc/styling.py:structure-header-out"
```
## Structure: add row stub
{{code_block('user-guide/misc/styling','structure-stub',[])}}
```python exec="on" session="user-guide/misc/styling"
--8<-- "python/user-guide/misc/styling.py:structure-stub-out"
```
## Structure: add column spanner
{{code_block('user-guide/misc/styling','structure-spanner',[])}}
```python exec="on" session="user-guide/misc/styling"
--8<-- "python/user-guide/misc/styling.py:structure-spanner-out"
```
## Format: limit decimal places
{{code_block('user-guide/misc/styling','format-number',[])}}
```python exec="on" session="user-guide/misc/styling"
--8<-- "python/user-guide/misc/styling.py:format-number-out"
```
## Style: highlight max row
{{code_block('user-guide/misc/styling','style-simple',[])}}
```python exec="on" session="user-guide/misc/styling"
--8<-- "python/user-guide/misc/styling.py:style-simple-out"
```
## Style: bold species column
{{code_block('user-guide/misc/styling','style-bold-column',[])}}
```python exec="on" session="user-guide/misc/styling"
--8<-- "python/user-guide/misc/styling.py:style-bold-column-out"
```
## Full example
{{code_block('user-guide/misc/styling','full-example',[])}}
```python exec="on" session="user-guide/misc/styling"
--8<-- "python/user-guide/misc/styling.py:full-example-out"
```
---
polars/docs/source/user-guide/misc/visualization.md
---
# Visualization
Data in a Polars `DataFrame` can be visualized using common visualization libraries.
We illustrate plotting capabilities using the Iris dataset. We read a CSV and then plot one column
against another, colored by a yet another column.
{{code_block('user-guide/misc/visualization','dataframe',[])}}
```python exec="on" result="text" session="user-guide/misc/visualization"
--8<-- "python/user-guide/misc/visualization.py:dataframe"
```
## Built-in plotting with Altair
Polars has a `plot` method to create plots using [Altair](https://altair-viz.github.io/):
{{code_block('user-guide/misc/visualization','altair_show_plot',[])}}
```python exec="on" session="user-guide/misc/visualization"
--8<-- "python/user-guide/misc/visualization.py:altair_make_plot"
```
This is shorthand for:
```python
import altair as alt
(
alt.Chart(df).mark_point(tooltip=True).encode(
x="sepal_length",
y="sepal_width",
color="species",
)
.properties(width=500)
.configure_scale(zero=False)
)
```
and is only provided for convenience, and to signal that Altair is known to work well with Polars.
For configuration, we suggest reading
[Chart Configuration](https://altair-viz.github.io/altair-tutorial/notebooks/08-Configuration.html).
For example, you can:
- Change the width/height/title with `.properties(width=500, height=350, title="My amazing plot")`.
- Change the x-axis label rotation with `.configure_axisX(labelAngle=30)`.
- Change the opacity of the points in your scatter plot with `.configure_point(opacity=.5)`.
## hvPlot
If you import `hvplot.polars`, then it registers a `hvplot` method which you can use to create
interactive plots using [hvPlot](https://hvplot.holoviz.org/).
{{code_block('user-guide/misc/visualization','hvplot_show_plot',[])}}
```python exec="on" session="user-guide/misc/visualization"
--8<-- "python/user-guide/misc/visualization.py:hvplot_make_plot"
```
## Matplotlib
To create a scatter plot we can pass columns of a `DataFrame` directly to Matplotlib as a `Series`
for each column. Matplotlib does not have explicit support for Polars objects but can accept a
Polars `Series` by converting it to a NumPy array (which is zero-copy for numeric data without null
values).
Note that because the column `'species'` isn't numeric, we need to first convert it to numeric
values so that it can be passed as an argument to `c`.
{{code_block('user-guide/misc/visualization','matplotlib_show_plot',[])}}
```python exec="on" session="user-guide/misc/visualization"
--8<-- "python/user-guide/misc/visualization.py:matplotlib_make_plot"
```
## Plotnine
[Plotnine](https://plotnine.org/) is a reimplementation of ggplot2 in Python, bringing the Grammar
of Graphics to Python users with an interface similar to its R counterpart. It supports Polars
`DataFrame` by internally converting it to a pandas `DataFrame`.
{{code_block('user-guide/misc/visualization','plotnine_show_plot',[])}}
```python exec="on" session="user-guide/misc/visualization"
--8<-- "python/user-guide/misc/visualization.py:plotnine_make_plot"
```
## Seaborn and Plotly
[Seaborn](https://seaborn.pydata.org/) and [Plotly](https://plotly.com/) can accept a Polars
`DataFrame` by leveraging the
[dataframe interchange protocol](https://data-apis.org/dataframe-api/), which offers zero-copy
conversion where possible. Note that the protocol does not support all Polars data types (e.g.
`List`) so your mileage may vary here.
### Seaborn
{{code_block('user-guide/misc/visualization','seaborn_show_plot',[])}}
```python exec="on" session="user-guide/misc/visualization"
--8<-- "python/user-guide/misc/visualization.py:seaborn_make_plot"
```
### Plotly
{{code_block('user-guide/misc/visualization','plotly_show_plot',[])}}
```python exec="on" session="user-guide/misc/visualization"
--8<-- "python/user-guide/misc/visualization.py:plotly_make_plot"
```
---
polars/docs/source/user-guide/expressions/aggregation.md
---
# Aggregation
The Polars [context](../concepts/expressions-and-contexts.md#contexts) `group_by` lets you apply
expressions on subsets of columns, as defined by the unique values of the column over which the data
is grouped. This is a very powerful capability that we explore in this section of the user guide.
We start by reading in a
[US congress `dataset`](https://github.com/unitedstates/congress-legislators):
{{code_block('user-guide/expressions/aggregation','dataframe',['DataFrame','Categorical'])}}
```python exec="on" result="text" session="user-guide/expressions"
--8<-- "python/user-guide/expressions/aggregation.py:dataframe"
```
## Basic aggregations
You can easily apply multiple expressions to your aggregated values. Simply list all of the
expressions you want inside the function `agg`. There is no upper bound on the number of
aggregations you can do and you can make any combination you want. In the snippet below we will
group the data based on the column â€œfirst_nameâ€� and then we will apply the following aggregations:
- count the number of rows in the group (which means we count how many people in the data set have
each unique first name);
- combine the values of the column â€œgenderâ€� into a list by referring the column but omitting an
aggregate function; and
- get the first value of the column â€œlast_nameâ€� within the group.
After computing the aggregations, we immediately sort the result and limit it to the top five rows
so that we have a nice summary overview:
{{code_block('user-guide/expressions/aggregation','basic',['group_by'])}}
```python exec="on" result="text" session="user-guide/expressions"
--8<-- "python/user-guide/expressions/aggregation.py:basic"
```
It's that easy! Let's turn it up a notch.
## Conditionals
Let's say we want to know how many delegates of a state are â€œProâ€� or â€œAntiâ€� administration. We can
query that directly in the aggregation without the need for a `lambda` or grooming the dataframe:
{{code_block('user-guide/expressions/aggregation','conditional',['group_by'])}}
```python exec="on" result="text" session="user-guide/expressions"
--8<-- "python/user-guide/expressions/aggregation.py:conditional"
```
## Filtering
We can also filter the groups. Let's say we want to compute a mean per group, but we don't want to
include all values from that group, and we also don't want to actually filter the rows from the
dataframe because we need those rows for another aggregation.
In the example below we show how this can be done.
!!! note
Note that we can define Python functions for clarity.
These functions don't cost us anything because they return Polars expressions, we don't apply a custom function over a series during runtime of the query.
Of course, you can write functions that return expressions in Rust, too.
{{code_block('user-guide/expressions/aggregation','filter',['group_by'])}}
```python exec="on" result="text" session="user-guide/expressions"
--8<-- "python/user-guide/expressions/aggregation.py:filter"
```
Do the average age values look nonsensical? That's because we are working with historical data that
dates back to the 1800s and we are doing our computations assuming everyone represented in the
dataset is still alive and kicking.
## Nested grouping
The two previous queries could have been done with a nested `group_by`, but that wouldn't have let
us show off some of these features. ðŸ˜‰ To do a nested `group_by`, simply list the columns that will
be used for grouping.
First, we use a nested `group_by` to figure out how many delegates of a state are â€œProâ€� or â€œAntiâ€�
administration:
{{code_block('user-guide/expressions/aggregation','nested',['group_by'])}}
```python exec="on" result="text" session="user-guide/expressions"
--8<-- "python/user-guide/expressions/aggregation.py:nested"
```
Next, we use a nested `group_by` to compute the average age of delegates per state and per gender:
{{code_block('user-guide/expressions/aggregation','filter-nested',['group_by'])}}
```python exec="on" result="text" session="user-guide/expressions"
--8<-- "python/user-guide/expressions/aggregation.py:filter-nested"
```
Note that we get the same results but the format of the data is different. Depending on the
situation, one format may be more suitable than the other.
## Sorting
It is common to see a dataframe being sorted for the sole purpose of managing the ordering during a
grouping operation. Let's say that we want to get the names of the oldest and youngest politicians
per state. We could start by sorting and then grouping:
{{code_block('user-guide/expressions/aggregation','sort',['group_by'])}}
```python exec="on" result="text" session="user-guide/expressions"
--8<-- "python/user-guide/expressions/aggregation.py:sort"
```
However, if we also want to sort the names alphabetically, we need to perform an extra sort
operation. Luckily, we can sort in a `group_by` context without changing the sorting of the
underlying dataframe:
{{code_block('user-guide/expressions/aggregation','sort2',['group_by'])}}
```python exec="on" result="text" session="user-guide/expressions"
--8<-- "python/user-guide/expressions/aggregation.py:sort2"
```
We can even sort a column with the order induced by another column, and this also works inside the
context `group_by`. This modification to the previous query lets us check if the delegate with the
first name is male or female:
{{code_block('user-guide/expressions/aggregation','sort3',['group_by'])}}
```python exec="on" result="text" session="user-guide/expressions"
--8<-- "python/user-guide/expressions/aggregation.py:sort3"
```
## Do not kill parallelization
!!! warning "Python users only"
The following section is specific to Python, and doesn't apply to Rust.
Within Rust, blocks and closures (lambdas) can, and will, be executed concurrently.
Python is generally slower than Rust. Besides the overhead of running â€œslowâ€� bytecode, Python has to
remain within the constraints of the Global Interpreter Lock (GIL). This means that if you were to
use a `lambda` or a custom Python function to apply during a parallelized phase, Polars' speed is
capped running Python code, preventing any multiple threads from executing the function.
Polars will try to parallelize the computation of the aggregating functions over the groups, so it
is recommended that you avoid using `lambda`s and custom Python functions as much as possible.
Instead, try to stay within the realm of the Polars expression API. This is not always possible,
though, so if you want to learn more about using `lambda`s you can go
[the user guide section on using user-defined functions](user-defined-python-functions.md).
---
polars/docs/source/user-guide/expressions/basic-operations.md
---
# Basic operations
This section shows how to do basic operations on dataframe columns, like do basic arithmetic
calculations, perform comparisons, and other general-purpose operations. We will use the following
dataframe for the examples that follow:
{{code_block('user-guide/expressions/operations', 'dataframe', ['DataFrame'])}}
```python exec="on" result="text" session="expressions/operations"
--8<-- "python/user-guide/expressions/operations.py:dataframe"
```
## Basic arithmetic
Polars supports basic arithmetic between series of the same length, or between series and literals.
When literals are mixed with series, the literals are broadcast to match the length of the series
they are being used with.
{{code_block('user-guide/expressions/operations', 'arithmetic', ['operators'])}}
```python exec="on" result="text" session="expressions/operations"
--8<-- "python/user-guide/expressions/operations.py:arithmetic"
```
The example above shows that when an arithmetic operation takes `null` as one of its operands, the
result is `null`.
Polars uses operator overloading to allow you to use your language's native arithmetic operators
within your expressions. If you prefer, in Python you can use the corresponding named functions, as
the snippet below demonstrates:
```python
--8<-- "python/user-guide/expressions/operations.py:operator-overloading"
```
```python exec="on" result="text" session="expressions/operations"
--8<-- "python/user-guide/expressions/operations.py:operator-overloading"
```
## Comparisons
Like with arithmetic operations, Polars supports comparisons via the overloaded operators or named
functions:
{{code_block('user-guide/expressions/operations','comparison',['operators'])}}
```python exec="on" result="text" session="expressions/operations"
--8<-- "python/user-guide/expressions/operations.py:comparison"
```
## Boolean and bitwise operations
Depending on the language, you may use the operators `&`, `|`, and `~`, for the Boolean operations
â€œandâ€�, â€œorâ€�, and â€œnotâ€�, respectively, or the functions of the same name:
{{code_block('user-guide/expressions/operations', 'boolean', ['operators'])}}
```python exec="on" result="text" session="expressions/operations"
--8<-- "python/user-guide/expressions/operations.py:boolean"
```
??? info "Python trivia"
The Python functions are called `and_`, `or_`, and `not_`, because the words `and`, `or`, and `not` are reserved keywords in Python.
Similarly, we cannot use the keywords `and`, `or`, and `not`, as the Boolean operators because these Python keywords will interpret their operands in the context of Truthy and Falsy through the dunder method `__bool__`.
Thus, we overload the bitwise operators `&`, `|`, and `~`, as the Boolean operators because they are the second best choice.
These operators/functions can also be used for the respective bitwise operations, alongside the
bitwise operator `^` / function `xor`:
{{code_block('user-guide/expressions/operations', 'bitwise', [])}}
```python exec="on" result="text" session="expressions/operations"
--8<-- "python/user-guide/expressions/operations.py:bitwise"
```
## Counting (unique) values
Polars has two functions to count the number of unique values in a series. The function `n_unique`
can be used to count the exact number of unique values in a series. However, for very large data
sets, this operation can be quite slow. In those cases, if an approximation is good enough, you can
use the function `approx_n_unique` that uses the algorithm
[HyperLogLog++](https://en.wikipedia.org/wiki/HyperLogLog) to estimate the result.
The example below shows an example series where the `approx_n_unique` estimation is wrong by 0.9%:
{{code_block('user-guide/expressions/operations', 'count', ['n_unique', 'approx_n_unique'])}}
```python exec="on" result="text" session="expressions/operations"
--8<-- "python/user-guide/expressions/operations.py:count"
```
You can get more information about the unique values and their counts with the function
`value_counts`, that Polars also provides:
{{code_block('user-guide/expressions/operations', 'value_counts', ['value_counts'])}}
```python exec="on" result="text" session="expressions/operations"
--8<-- "python/user-guide/expressions/operations.py:value_counts"
```
The function `value_counts` returns the results in
[structs, a data type that we will explore in a later section](structs.md).
Alternatively, if you only need a series with the unique values or a series with the unique counts,
they are one function away:
{{code_block('user-guide/expressions/operations', 'unique_counts', ['unique', 'unique_counts'])}}
```python exec="on" result="text" session="expressions/operations"
--8<-- "python/user-guide/expressions/operations.py:unique_counts"
```
Note that we need to specify `maintain_order=True` in the function `unique` so that the order of the
results is consistent with the order of the results in `unique_counts`. See the API reference for
more information.
## Conditionals
Polars supports something akin to a ternary operator through the function `when`, which is followed
by one function `then` and an optional function `otherwise`.
The function `when` accepts a predicate expression. The values that evaluate to `True` are replaced
by the corresponding values of the expression inside the function `then`. The values that evaluate
to `False` are replaced by the corresponding values of the expression inside the function
`otherwise` or `null`, if `otherwise` is not provided.
The example below applies one step of the
[Collatz conjecture](https://en.wikipedia.org/wiki/Collatz_conjecture) to the numbers in the column
â€œnrsâ€�:
{{code_block('user-guide/expressions/operations', 'collatz', ['when'])}}
```python exec="on" result="text" session="expressions/operations"
--8<-- "python/user-guide/expressions/operations.py:collatz"
```
You can also emulate a chain of an arbitrary number of conditionals, akin to Python's `elif`
statement, by chaining an arbitrary number of consecutive blocks of `.when(...).then(...)`. In those
cases, and for each given value, Polars will only consider a replacement expression that is deeper
within the chain if the previous predicates all failed for that value.
---
polars/docs/source/user-guide/expressions/casting.md
---
# Casting
Casting converts the [underlying data type of a column](../concepts/data-types-and-structures.md) to
a new one. Casting is available through the function `cast`.
The function `cast` includes a parameter `strict` that determines how Polars behaves when it
encounters a value that cannot be converted from the source data type to the target data type. The
default behaviour is `strict=True`, which means that Polars will thrown an error to notify the user
of the failed conversion while also providing details on the values that couldn't be cast. On the
other hand, if `strict=False`, any values that cannot be converted to the target data type will be
quietly converted to `null`.
## Basic example
Let's take a look at the following dataframe which contains both integers and floating point
numbers:
{{code_block('user-guide/expressions/casting', 'dfnum', [])}}
```python exec="on" result="text" session="user-guide/casting"
--8<-- "python/user-guide/expressions/casting.py:dfnum"
```
To perform casting operations between floats and integers, or vice versa, we use the function
`cast`:
{{code_block('user-guide/expressions/casting','castnum',['cast'])}}
```python exec="on" result="text" session="user-guide/casting"
--8<-- "python/user-guide/expressions/casting.py:castnum"
```
Note that floating point numbers are truncated when casting to an integer data type.
## Downcasting numerical data types
You can reduce the memory footprint of a column by changing the precision associated with its
numeric data type. As an illustration, the code below demonstrates how casting from `Int64` to
`Int16` and from `Float64` to `Float32` can be used to lower memory usage:
{{code_block('user-guide/expressions/casting','downcast',['cast', 'estimated_size'])}}
```python exec="on" result="text" session="user-guide/casting"
--8<-- "python/user-guide/expressions/casting.py:downcast"
```
When performing downcasting it is crucial to ensure that the chosen number of bits (such as 64, 32,
or 16) is sufficient to accommodate the largest and smallest numbers in the column. For example, a
32-bit signed integer (`Int32`) represents integers between -2147483648 and 2147483647, inclusive,
while an 8-bit signed integer only represents integers between -128 and 127, inclusive. Attempting
to downcast to a data type with insufficient precision results in an error thrown by Polars:
{{code_block('user-guide/expressions/casting','overflow',['cast'])}}
```python exec="on" result="text" session="user-guide/casting"
--8<-- "python/user-guide/expressions/casting.py:overflow"
```
If you set the parameter `strict` to `False` the overflowing/underflowing values are converted to
`null`:
{{code_block('user-guide/expressions/casting','overflow2',['cast'])}}
```python exec="on" result="text" session="user-guide/casting"
--8<-- "python/user-guide/expressions/casting.py:overflow2"
```
## Converting strings to numeric data types
Strings that represent numbers can be converted to the appropriate data types via casting. The
opposite conversion is also possible:
{{code_block('user-guide/expressions/casting','strings',['cast'])}}
```python exec="on" result="text" session="user-guide/casting"
--8<-- "python/user-guide/expressions/casting.py:strings"
```
In case the column contains a non-numerical value, or a poorly formatted one, Polars will throw an
error with details on the conversion error. You can set `strict=False` to circumvent the error and
get a `null` value instead.
{{code_block('user-guide/expressions/casting','strings2',['cast'])}}
```python exec="on" result="text" session="user-guide/casting"
--8<-- "python/user-guide/expressions/casting.py:strings2"
```
## Booleans
Booleans can be expressed as either 1 (`True`) or 0 (`False`). It's possible to perform casting
operations between a numerical data type and a Boolean, and vice versa.
When converting numbers to Booleans, the number 0 is converted to `False` and all other numbers are
converted to `True`, in alignment with Python's Truthy and Falsy values for numbers:
{{code_block('user-guide/expressions/casting','bool',['cast'])}}
```python exec="on" result="text" session="user-guide/casting"
--8<-- "python/user-guide/expressions/casting.py:bool"
```
## Parsing / formatting temporal data types
All temporal data types are represented internally as the number of time units elapsed since a
reference moment, usually referred to as the epoch. For example, values of the data type `Date` are
stored as the number of days since the epoch. For the data type `Datetime` the time unit is the
microsecond (us) and for `Time` the time unit is the nanosecond (ns).
Casting between numerical types and temporal data types is allowed and exposes this relationship:
{{code_block('user-guide/expressions/casting','dates',['cast'])}}
```python exec="on" result="text" session="user-guide/casting"
--8<-- "python/user-guide/expressions/casting.py:dates"
```
To format temporal data types as strings we can use the function `dt.to_string` and to parse
temporal data types from strings we can use the function `str.to_datetime`. Both functions adopt the
[chrono format syntax](https://docs.rs/chrono/latest/chrono/format/strftime/index.html) for
formatting.
{{code_block('user-guide/expressions/casting','dates2',['dt.to_string','str.to_date'])}}
```python exec="on" result="text" session="user-guide/casting"
--8<-- "python/user-guide/expressions/casting.py:dates2"
```
It's worth noting that `str.to_datetime` features additional options that support timezone
functionality. Refer to the API documentation for further information.
---
polars/docs/source/user-guide/expressions/categorical-data-and-enums.md
---
# Categorical data and enums
A column that holds string values that can only take on one of a limited number of possible values
is a column that holds [categorical data](https://en.wikipedia.org/wiki/Categorical_variable).
Usually, the number of possible values is much smaller than the length of the column. Some typical
examples include your nationality, the operating system of your computer, or the license that your
favorite open source project uses.
When working with categorical data you can use Polars' dedicated types, `Categorical` and `Enum`, to
make your queries more performant. Now, we will see what are the differences between the two data
types `Categorical` and `Enum` and when you should use one data type or the other. We also include
some notes on
[why the data types `Categorical` and `Enum` are more efficient than using the plain string values](#performance-considerations-on-categorical-data-types)
in the end of this user guide section.
## `Enum` vs `Categorical`
In short, you should prefer `Enum` over `Categorical` whenever possible. When the categories are
fixed and known up front, use `Enum`. When you don't know the categories or they are not fixed then
you must use `Categorical`. In case your requirements change along the way you can always cast from
one to the other.
## Data type `Enum`
### Creating an `Enum`
The data type `Enum` is an ordered categorical data type. To use the data type `Enum` you have to
specify the categories in advance to create a new data type that is a variant of an `Enum`. Then,
when creating a new series, a new dataframe, or when casting a string column, you can use that
`Enum` variant.
{{code_block('user-guide/expressions/categoricals', 'enum-example', ['Enum'])}}
```python exec="on" result="text" session="expressions/categoricals"
--8<-- "python/user-guide/expressions/categoricals.py:enum-example"
```
### Invalid values
Polars will raise an error if you try to specify a data type `Enum` whose categories do not include
all the values present:
{{code_block('user-guide/expressions/categoricals', 'enum-wrong-value', ['Enum'])}}
```python exec="on" result="text" session="expressions/categoricals"
--8<-- "python/user-guide/expressions/categoricals.py:enum-wrong-value"
```
If you are in a position where you cannot know all of the possible values in advance and erroring on
unknown values is semantically wrong, you may need to
[use the data type `Categorical`](#data-type-categorical).
### Category ordering and comparison
The data type `Enum` is ordered and the order is induced by the order in which you specify the
categories. The example below uses log levels as an example of where an ordered `Enum` is useful:
{{code_block('user-guide/expressions/categoricals', 'log-levels', ['Enum'])}}
```python exec="on" result="text" session="expressions/categoricals"
--8<-- "python/user-guide/expressions/categoricals.py:log-levels"
```
This example shows that we can compare `Enum` values with a string, but this only works if the
string matches one of the `Enum` values. If we compared the column â€œlevelâ€� with any string other
than `"debug"`, `"info"`, `"warning"`, or `"error"`, Polars would raise an exception.
Columns with the data type `Enum` can also be compared with other columns that have the same data
type `Enum` or columns that hold strings, but only if all the strings are valid `Enum` values.
## Data type `Categorical`
The data type `Categorical` can be seen as a more flexible version of `Enum`.
### Creating a `Categorical` series
To use the data type `Categorical`, you can cast a column of strings or specify `Categorical` as the
data type of a series or dataframe column:
{{code_block('user-guide/expressions/categoricals', 'categorical-example', ['Categorical'])}}
```python exec="on" result="text" session="expressions/categoricals"
--8<-- "python/user-guide/expressions/categoricals.py:categorical-example"
```
Having Polars infer the categories for you may sound strictly better than listing the categories
beforehand, but this inference comes with a performance cost. That is why, whenever possible, you
should use `Enum`. You can learn more by
[reading the subsection about the data type `Categorical` and its encodings](#data-type-categorical-and-encodings).
### Lexical comparison with strings
When comparing a `Categorical` column with a string, Polars will perform a lexical comparison:
{{code_block('user-guide/expressions/categoricals', 'categorical-comparison-string',
['Categorical'])}}
```python exec="on" result="text" session="expressions/categoricals"
--8<-- "python/user-guide/expressions/categoricals.py:categorical-comparison-string"
```
You can also compare a column of strings with your `Categorical` column, and the comparison will
also be lexical:
{{code_block('user-guide/expressions/categoricals', 'categorical-comparison-string-column',
['Categorical'])}}
```python exec="on" result="text" session="expressions/categoricals"
--8<-- "python/user-guide/expressions/categoricals.py:categorical-comparison-string-column"
```
Although it is possible to compare a string column with a categorical column, it is typically more
efficient to compare two categorical columns. We will see how to do that next.
### Comparing `Categorical` columns and the string cache
You are told that comparing columns with the data type `Categorical` is more efficient than if one
of them is a string column. So, you change your code so that the second column is also a categorical
column and then you perform your comparison... But Polars raises an exception:
{{code_block('user-guide/expressions/categoricals', 'categorical-comparison-categorical-column',
['Categorical'])}}
```python exec="on" result="text" session="expressions/categoricals"
--8<-- "python/user-guide/expressions/categoricals.py:categorical-comparison-categorical-column"
```
By default, the values in columns with the data type `Categorical` are
[encoded in the order they are seen in the column](#encodings), and independently from other
columns, which means that Polars cannot compare efficiently two categorical columns that were
created independently.
Enabling the Polars string cache and creating the columns with the cache enabled fixes this issue:
{{code_block('user-guide/expressions/categoricals', 'stringcache-categorical-equality',
['StringCache', 'Categorical'])}}
```python exec="on" result="text" session="expressions/categoricals"
--8<-- "python/user-guide/expressions/categoricals.py:stringcache-categorical-equality"
```
Note that using [the string cache comes at a performance cost](#using-the-global-string-cache).
### Combining `Categorical` columns
The string cache is also useful in any operation that combines or mixes two columns with the data
type `Categorical` in any way. An example of this is when
[concatenating two dataframes vertically](../getting-started.md#concatenating-dataframes):
{{code_block('user-guide/expressions/categoricals', 'concatenating-categoricals', ['StringCache',
'Categorical'])}}
```python exec="on" result="text" session="expressions/categoricals"
--8<-- "python/user-guide/expressions/categoricals.py:concatenating-categoricals"
```
In this case, Polars issues a warning complaining about an expensive reenconding that implies taking
a performance hit. Polars then suggests using the data type `Enum` if possible, or using the string
cache. To understand the issue with this operation and why Polars raises an error, please read the
final section about
[the performance considerations of using categorical data types](#performance-considerations-on-categorical-data-types).
### Comparison between `Categorical` columns is not lexical
When comparing two columns with data type `Categorical`, Polars does not perform lexical comparison
between the values by default. If you want lexical ordering, you need to specify so when creating
the column:
{{code_block('user-guide/expressions/categoricals', 'stringcache-categorical-comparison-lexical',
['StringCache', 'Categorical'])}}
```python exec="on" result="text" session="expressions/categoricals"
--8<-- "python/user-guide/expressions/categoricals.py:stringcache-categorical-comparison-lexical"
```
Otherwise, the order is inferred together with the values:
{{code_block('user-guide/expressions/categoricals', 'stringcache-categorical-comparison-physical',
['StringCache', 'Categorical'])}}
```python exec="on" result="text" session="expressions/categoricals"
--8<-- "python/user-guide/expressions/categoricals.py:stringcache-categorical-comparison-physical"
```
## Performance considerations on categorical data types
This part of the user guide explains
- why categorical data types are more performant than the string literals; and
- why Polars needs a string cache when doing some operations with the data type `Categorical`.
### Encodings
Categorical data represents string data where the values in the column have a finite set of values
(usually way smaller than the length of the column). Storing these values as plain strings is a
waste of memory and performance as we will be repeating the same string over and over again.
Additionally, in operations like joins we have to perform expensive string comparisons.
Categorical data types like `Enum` and `Categorical` let you encode the string values in a cheaper
way, establishing a relationship between a cheap encoding value and the original string literal.
As an example of a sensible encoding, Polars could choose to represent the finite set of categories
as positive integers. With that in mind, the diagram below shows a regular string column and a
possible representation of a Polars column with the categorical data type:
 String Column Categorical Column     Series     Polar   Panda   Brown   Panda   Brown   Brown   Polar        
   Physical     0   1   2   1   2   2   0   
     Categories     Polar   Panda   Brown         
The physical `0` in this case encodes (or maps) to the value 'Polar', the value `1` encodes to
'Panda', and the value `2` to 'Brown'. This encoding has the benefit of only storing the string
values once. Additionally, when we perform operations (e.g. sorting, counting) we can work directly
on the physical representation which is much faster than the working with string data.
### Encodings for the data type `Enum` are global
When working with the data type `Enum` we specify the categories in advance. This way, Polars can
ensure different columns and even different datasets have the same encoding and there is no need for
expensive re-encoding or cache lookups.
### Data type `Categorical` and encodings
The fact that the categories for the data type `Categorical` are inferred come at a cost. The main
cost here is that we have no control over our encodings.
Consider the following scenario where we append the following two categorical series:
{{code_block('user-guide/concepts/data-types/categoricals','append',[])}}
Polars encodes the string values in the order they appear. So, the series would look like this:
 cat_series cat2_series        Physical     0   1   2   2   0   
     Categories     Polar   Panda   Brown   
           Physical     0   1   1   2   2   
   Categories     Panda   Brown   Polar   
Combining the series becomes a non-trivial task which is expensive as the physical value of `0`
represents something different in both series. Polars does support these types of operations for
convenience, however these should be avoided due to its slower performance as it requires making
both encodings compatible first before doing any merge operations.
### Using the global string cache
One way to handle this reencoding problem is to enable the string cache. Under the string cache, the
diagram would instead look like this:
 SeriesString cache   cat_seriescat2_series      Physical     0   1   2   2   0         Physical     1   2   2   0   0            Categories     Polar   Panda   Brown      
When you enable the string cache, strings are no longer encoded in the order they appear on a
per-column basis. Instead, the encoding is shared across columns. The value 'Polar' will always be
encoded by the same value for all categorical columns created under the string cache. Merge
operations (e.g. appends, joins) become cheap again as there is no need to make the encodings
compatible first, solving the problem we had above.
However, the string cache does come at a small performance hit during construction of the series as
we need to look up or insert the string values in the cache. Therefore, it is preferred to use the
data type `Enum` if you know your categories in advance.
---
polars/docs/source/user-guide/expressions/expression-expansion.md
---
# Expression expansion
As you've seen in
[the section about expressions and contexts](../concepts/expressions-and-contexts.md), expression
expansion is a feature that enables you to write a single expression that can expand to multiple
different expressions, possibly depending on the schema of the context in which the expression is
used.
This feature isn't just decorative or syntactic sugar. It allows for a very powerful application of
[DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) principles in your code: a single
expression that specifies multiple columns expands into a list of expressions, which means you can
write one single expression and reuse the computation that it represents.
In this section we will show several forms of expression expansion and we will be using the
dataframe that you can see below for that effect:
{{code_block('user-guide/expressions/expression-expansion', 'df', [])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:df"
```
## Function `col`
The function `col` is the most common way of making use of expression expansion features in Polars.
Typically used to refer to one column of a dataframe, in this section we explore other ways in which
you can use `col` (or its variants, when in Rust).
### Explicit expansion by column name
The simplest form of expression expansion happens when you provide multiple column names to the
function `col`.
The example below uses a single function `col` with multiple column names to convert the values in
USD to EUR:
{{code_block('user-guide/expressions/expression-expansion', 'col-with-names', ['col'])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:col-with-names"
```
When you list the column names you want the expression to expand to, you can predict what the
expression will expand to. In this case, the expression that does the currency conversion is
expanded to a list of five expressions:
{{code_block('user-guide/expressions/expression-expansion', 'expression-list', ['col'])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:expression-list"
```
### Expansion by data type
We had to type five column names in the previous example but the function `col` can also
conveniently accept one or more data types. If you provide data types instead of column names, the
expression is expanded to all columns that match one of the data types provided.
The example below performs the exact same computation as before:
{{code_block('user-guide/expressions/expression-expansion', 'col-with-dtype', [], ['col'],
['dtype_col'])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:col-with-dtype"
```
When we use a data type with expression expansion we cannot know, beforehand, how many columns a
single expression will expand to. We need the schema of the input dataframe if we want to determine
what is the final list of expressions that is to be applied.
If we weren't sure about whether the price columns where of the type `Float64` or `Float32`, we
could specify both data types:
{{code_block('user-guide/expressions/expression-expansion', 'col-with-dtypes', [], ['col'],
['dtype_cols'])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:col-with-dtypes"
```
### Expansion by pattern matching
You can also use regular expressions to specify patterns that are used to match the column names. To
distinguish between a regular column name and expansion by pattern matching, regular expressions
start and end with `^` and `$`, respectively. This also means that the pattern must match against
the whole column name string.
Regular expressions can be mixed with regular column names:
{{code_block('user-guide/expressions/expression-expansion', 'col-with-regex', ['col'])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:col-with-regex"
```
### Arguments cannot be of mixed types
In Python, the function `col` accepts an arbitrary number of strings (as
[column names](#explicit-expansion-by-column-name) or as
[regular expressions](#expansion-by-pattern-matching)) or an arbitrary number of data types, but you
cannot mix both in the same function call:
```python
--8<-- "python/user-guide/expressions/expression-expansion.py:col-error"
```
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:col-error"
```
## Selecting all columns
Polars provides the function `all` as shorthand notation to refer to all columns of a dataframe:
{{code_block('user-guide/expressions/expression-expansion', 'all', ['all'])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:all"
```
!!! note
The function `all` is syntactic sugar for `col("*")`, but since the argument `"*"` is a special case and `all` reads more like English, the usage of `all` is preferred.
## Excluding columns
Polars also provides a mechanism to exclude certain columns from expression expansion. For that, you
use the function `exclude`, which accepts exactly the same types of arguments as `col`:
{{code_block('user-guide/expressions/expression-expansion', 'all-exclude', ['exclude'])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:all-exclude"
```
Naturally, the function `exclude` can also be used after the function `col`:
{{code_block('user-guide/expressions/expression-expansion', 'col-exclude', ['exclude'])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:col-exclude"
```
## Column renaming
By default, when you apply an expression to a column, the result keeps the same name as the original
column.
Preserving the column name can be semantically wrong and in certain cases Polars may even raise an
error if duplicate names occur:
{{code_block('user-guide/expressions/expression-expansion', 'duplicate-error', [])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:duplicate-error"
```
To prevent errors like this, and to allow users to rename their columns when appropriate, Polars
provides a series of functions that let you change the name of a column or a group of columns.
### Renaming a single column with `alias`
The function `alias` has been used thoroughly in the documentation already and it lets you rename a
single column:
{{code_block('user-guide/expressions/expression-expansion', 'alias', ['alias'])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:alias"
```
### Prefixing and suffixing column names
When using expression expansion you cannot use the function `alias` because the function `alias` is
designed specifically to rename a single column.
When it suffices to add a static prefix or a static suffix to the existing names, we can use the
functions `prefix` and `suffix` from the namespace `name`:
{{code_block('user-guide/expressions/expression-expansion', 'prefix-suffix', ['Expr.name', 'prefix',
'suffix'])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:prefix-suffix"
```
### Dynamic name replacement
If a static prefix/suffix is not enough, the namespace `name` also provides the function `map` that
accepts a callable that accepts the old column names and produces the new ones:
{{code_block('user-guide/expressions/expression-expansion', 'name-map', ['Expr.name', 'map'])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:name-map"
```
See the API reference for the full contents of the namespace `name`.
## Programmatically generating expressions
Expression expansion is a very useful feature but it does not solve all of your problems. For
example, if we want to compute the day and year amplitude of the prices of the stocks in our
dataframe, expression expansion won't help us.
At first, you may think about using a `for` loop:
{{code_block('user-guide/expressions/expression-expansion', 'for-with_columns', [])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:for-with_columns"
```
Do not do this. Instead, generate all of the expressions you want to compute programmatically and
use them only once in a context. Loosely speaking, you want to swap the `for` loop with the context
`with_columns`. In practice, you could do something like the following:
{{code_block('user-guide/expressions/expression-expansion', 'yield-expressions', [])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:yield-expressions"
```
This produces the same final result and by specifying all of the expressions in one go we give
Polars the opportunity to:
1. do a better job at optimising the query; and
2. parallelise the execution of the actual computations.
## More flexible column selections
Polars comes with the submodule `selectors` that provides a number of functions that allow you to
write more flexible column selections for expression expansion.
!!! warning
This functionality is not available in Rust yet. Refer to [Polars issue #10594](https://github.com/pola-rs/polars/issues/10594).
As a first example, here is how we can use the functions `string` and `ends_with`, and the set
operations that the functions from `selectors` support, to select all string columns and the columns
whose names end with `"_high"`:
{{code_block('user-guide/expressions/expression-expansion', 'selectors', [], ['selectors'], [])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:selectors"
```
The submodule `selectors` provides
[a number of selectors that match based on the data type of the columns](#selectors-for-data-types),
of which the most useful are the functions that match a whole category of types, like `cs.numeric`
for all numeric data types or `cs.temporal` for all temporal data types.
The submodule `selectors` also provides
[a number of selectors that match based on patterns in the column names](#selectors-for-column-name-patterns)
which make it more convenient to specify common patterns you may want to check for, like the
function `cs.ends_with` that was shown above.
### Combining selectors with set operations
We can combine multiple selectors using set operations and the usual Python operators:
| Operator | Operation |
| ----------------------- | -------------------- |
| `A | B` | Union |
| `A & B` | Intersection |
| `A - B` | Difference |
| `A ^ B` | Symmetric difference |
| `~A` | Complement |
The next example matches all non-string columns that contain an underscore in the name:
{{code_block('user-guide/expressions/expression-expansion', 'selectors-set-operations', [],
['selectors'], [])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:selectors-set-operations"
```
### Resolving operator ambiguity
Expression functions can be chained on top of selectors:
{{code_block('user-guide/expressions/expression-expansion', 'selectors-expressions', [],
['selectors'], [])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:selectors-expressions"
```
However, some operators have been overloaded to operate both on Polars selectors and on expressions.
For example, the operator `~` on a selector represents
[the set operation â€œcomplementâ€�](#combining-selectors-with-set-operations) and on an expression
represents the Boolean operation of negation.
When you use a selector and then want to use, in the context of an expression, one of the
[operators that act as set operators for selectors](#combining-selectors-with-set-operations), you
can use the function `as_expr`.
Below, we want to negate the Boolean values in the columns â€œhas_partnerâ€�, â€œhas_kidsâ€�, and
â€œhas_tattoosâ€�. If we are not careful, the combination of the operator `~` and the selector
`cs.starts_with("has_")` will actually select the columns that we do not care about:
{{code_block('user-guide/expressions/expression-expansion', 'selector-ambiguity', [], [], [])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:selector-ambiguity"
```
The correct solution uses `as_expr`:
{{code_block('user-guide/expressions/expression-expansion', 'as_expr', [])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:as_expr"
```
### Debugging selectors
When you are not sure whether you have a Polars selector at hand or not, you can use the function
`cs.is_selector` to check:
{{code_block('user-guide/expressions/expression-expansion', 'is_selector', [], ['is_selector'],
[])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:is_selector"
```
This should help you avoid any ambiguous situations where you think you are operating with
expressions but are in fact operating with selectors.
Another helpful debugging utility is the function `expand_selector`. Given a target frame or schema,
you can check what columns a given selector will expand to:
{{code_block('user-guide/expressions/expression-expansion', 'expand_selector', [],
['expand_selector'], [])}}
```python exec="on" result="text" session="expressions/expression-expansion"
--8<-- "python/user-guide/expressions/expression-expansion.py:expand_selector"
```
### Complete reference
The tables below group the functions available in the submodule `selectors` by their type of
behaviour.
#### Selectors for data types
Selectors that match based on the data type of the column:
| Selector function | Data type(s) matched |
| ------------------ | ------------------------------------------------------------------ |
| `binary` | `Binary` |
| `boolean` | `Boolean` |
| `by_dtype` | Data types specified as arguments |
| `categorical` | `Categorical` |
| `date` | `Date` |
| `datetime` | `Datetime`, optionally filtering by time unit/zone |
| `decimal` | `Decimal` |
| `duration` | `Duration`, optionally filtering by time unit |
| `float` | All float types, regardless of precision |
| `integer` | All integer types, signed and unsigned, regardless of precision |
| `numeric` | All numeric types, namely integers, floats, and `Decimal` |
| `signed_integer` | All signed integer types, regardless of precision |
| `string` | `String` |
| `temporal` | All temporal data types, namely `Date`, `Datetime`, and `Duration` |
| `time` | `Time` |
| `unsigned_integer` | All unsigned integer types, regardless of precision |
#### Selectors for column name patterns
Selectors that match based on column name patterns:
| Selector function | Columns selected |
| ----------------- | ------------------------------------------------------------ |
| `alpha` | Columns with alphabetical names |
| `alphanumeric` | Columns with alphanumeric names (letters and the digits 0-9) |
| `by_name` | Columns with the names specified as arguments |
| `contains` | Columns whose names contain the substring specified |
| `digit` | Columns with numeric names (only the digits 0-9) |
| `ends_with` | Columns whose names end with the given substring |
| `matches` | Columns whose names match the given regex pattern |
| `starts_with` | Columns whose names start with the given substring |
#### Positional selectors
Selectors that match based on the position of the columns:
| Selector function | Columns selected |
| ----------------- | ------------------------------------ |
| `all` | All columns |
| `by_index` | The columns at the specified indices |
| `first` | The first column in the context |
| `last` | The last column in the context |
#### Miscellaneous functions
The submodule `selectors` also provides the following functions:
| Function | Behaviour |
| ----------------- | ------------------------------------------------------------------------------------- |
| `as_expr`* | Convert a selector to an expression |
| `exclude` | Selects all columns except those matching the given names, data types, or selectors |
| `expand_selector` | Expand selector to matching columns with respect to a specific frame or target schema |
| `is_selector` | Check whether the given object/expression is a selector |
*`as_expr` isn't a function defined on the submodule `selectors`, but rather a method defined on
selectors.
---
polars/docs/source/user-guide/expressions/folds.md
---
# Folds
Polars provides many expressions to perform computations across columns, like `sum_horizontal`,
`mean_horizontal`, and `min_horizontal`. However, these are just special cases of a general
algorithm called a fold, and Polars provides a general mechanism for you to compute custom folds for
when the specialised versions of Polars are not enough.
Folds computed with the function `fold` operate on the full columns for maximum speed. They utilize
the data layout very efficiently and often have vectorized execution.
## Basic example
As a first example, we will reimplement `sum_horizontal` with the function `fold`:
{{code_block('user-guide/expressions/folds','mansum',['fold'])}}
```python exec="on" result="text" session="user-guide/folds"
--8<-- "python/user-guide/expressions/folds.py:mansum"
```
The function `fold` expects a function `f` as the parameter `function` and `f` should accept two
arguments. The first argument is the accumulated result, which we initialise as zero, and the second
argument takes the successive values of the expressions listed in the parameter `exprs`. In our
case, they're the two columns â€œaâ€� and â€œbâ€�.
The snippet below includes a third explicit expression that represents what the function `fold` is
doing above:
{{code_block('user-guide/expressions/folds','mansum-explicit',['fold'])}}
```python exec="on" result="text" session="user-guide/folds"
--8<-- "python/user-guide/expressions/folds.py:mansum-explicit"
```
??? tip "`fold` in Python"
Most programming languages include a higher-order function that implements the algorithm that the function `fold` in Polars implements.
The Polars `fold` is very similar to Python's `functools.reduce`.
You can [learn more about the power of `functools.reduce` in this article](http://mathspp.com/blog/pydonts/the-power-of-reduce).
## The initial value `acc`
The initial value chosen for the accumulator `acc` is typically, but not always, the
[identity element](https://en.wikipedia.org/wiki/Identity_element) of the operation you want to
apply. For example, if we wanted to multiply across the columns, we would not get the correct result
if our accumulator was set to zero:
{{code_block('user-guide/expressions/folds','manprod',['fold'])}}
```python exec="on" result="text" session="user-guide/folds"
--8<-- "python/user-guide/expressions/folds.py:manprod"
```
To fix this, the accumulator `acc` should be set to `1`:
{{code_block('user-guide/expressions/folds','manprod-fixed',['fold'])}}
```python exec="on" result="text" session="user-guide/folds"
--8<-- "python/user-guide/expressions/folds.py:manprod-fixed"
```
## Conditional
In the case where you'd want to apply a condition/predicate across all columns in a dataframe, a
fold can be a very concise way to express this.
{{code_block('user-guide/expressions/folds','conditional',['fold'])}}
```python exec="on" result="text" session="user-guide/folds"
--8<-- "python/user-guide/expressions/folds.py:conditional"
```
The snippet above filters all rows where all columns are greater than 1.
## Folds and string data
Folds could be used to concatenate string data. However, due to the materialization of intermediate
columns, this operation will have squared complexity.
Therefore, we recommend using the function `concat_str` for this:
{{code_block('user-guide/expressions/folds','string',['concat_str'])}}
```python exec="on" result="text" session="user-guide/folds"
--8<-- "python/user-guide/expressions/folds.py:string"
```
---
polars/docs/source/user-guide/expressions/index.md
---
# Expressions
We
[introduced the concept of â€œexpressionsâ€� in a previous section](../concepts/expressions-and-contexts.md#expressions).
In this section we will focus on exploring the types of expressions that Polars offers. Each section
gives an overview of what they do and provides additional examples.
- Essentials:
- [Basic operations](basic-operations.md) â€“ how to do basic operations on dataframe columns, like arithmetic calculations, comparisons, and other common, general-purpose operations
- [Expression expansion](expression-expansion.md) â€“ what is expression expansion and how to use it
- [Casting](casting.md) â€“ how to convert / cast values to different data types
- How to work with specific types of data or data type namespaces:
- [Strings](strings.md) â€“ how to work with strings and the namespace `str`
- [Lists and arrays](lists-and-arrays.md) â€“ the differences between the data types `List` and `Array`, when to use them, and how to use them
- [Categorical data and enums](categorical-data-and-enums.md) â€“ the differences between the data types `Categorical` and `Enum`, when to use them, and how to use them
- [Structs](structs.md) â€“ when to use the data type `Struct` and how to use it
- [Missing data](missing-data.md) â€“ how to work with missing data and how to fill missing data
- Types of operations:
- [Aggregation](aggregation.md) â€“ how to work with aggregating contexts like `group_by`
- [Window functions](window-functions.md) â€“ how to apply window functions over columns in a dataframe
- [Folds](folds.md) â€“ how to perform arbitrary computations horizontally across columns
- [User-defined Python functions](user-defined-python-functions.md) â€“ how to apply user-defined Python functions to dataframe columns or to column values
- [Numpy functions](numpy-functions.md) â€“ how to use NumPy native functions on Polars dataframes and series
---
polars/docs/source/user-guide/expressions/lists-and-arrays.md
---
# Lists and arrays
Polars has first-class support for two homogeneous container data types: `List` and `Array`. Polars
supports many operations with the two data types and their APIs overlap, so this section of the user
guide has the objective of clarifying when one data type should be chosen in favour of the other.
## Lists vs arrays
### The data type `List`
The data type list is suitable for columns whose values are homogeneous 1D containers of varying
lengths.
The dataframe below contains three examples of columns with the data type `List`:
{{code_block('user-guide/expressions/lists', 'list-example', ['List'])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:list-example"
```
Note that the data type `List` is different from Python's type `list`, where elements can be of any
type. If you want to store true Python lists in a column, you can do so with the data type `Object`
and your column will not have the list manipulation features that we're about to discuss.
### The data type `Array`
The data type `Array` is suitable for columns whose values are homogeneous containers of an
arbitrary dimension with a known and fixed shape.
The dataframe below contains two examples of columns with the data type `Array`.
{{code_block('user-guide/expressions/lists', 'array-example', ['Array'])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:array-example"
```
The example above shows how to specify that the columns â€œbit_flagsâ€� and â€œtic_tac_toeâ€� have the data
type `Array`, parametrised by the data type of the elements contained within and by the shape of
each array.
In general, Polars does not infer that a column has the data type `Array` for performance reasons,
and defaults to the appropriate variant of the data type `List`. In Python, an exception to this
rule is when you provide a NumPy array to build a column. In that case, Polars has the guarantee
from NumPy that all subarrays have the same shape, so an array of $n + 1$ dimensions will generate a
column of $n$ dimensional arrays:
{{code_block('user-guide/expressions/lists', 'numpy-array-inference', ['Array'])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:numpy-array-inference"
```
### When to use each
In short, prefer the data type `Array` over `List` because it is more memory efficient and more
performant. If you cannot use `Array`, then use `List`:
- when the values within a column do not have a fixed shape; or
- when you need functions that are only available in the list API.
## Working with lists
### The namespace `list`
Polars provides many functions to work with values of the data type `List` and these are grouped
inside the namespace `list`. We will explore this namespace a bit now.
!!! warning "`arr` then, `list` now"
In previous versions of Polars, the namespace for list operations used to be `arr`.
`arr` is now the namespace for the data type `Array`.
If you find references to the namespace `arr` on StackOverflow or other sources, note that those sources _may_ be outdated.
The dataframe `weather` defined below contains data from different weather stations across a region.
When the weather station is unable to get a result, an error code is recorded instead of the actual
temperature at that time.
{{code_block('user-guide/expressions/lists', 'weather', [])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:weather"
```
### Programmatically creating lists
Given the dataframe `weather` defined previously, it is very likely we need to run some analysis on
the temperatures that are captured by each station. To make this happen, we need to first be able to
get individual temperature measurements. We
[can use the namespace `str`](strings.md#the-string-namespace) for this:
{{code_block('user-guide/expressions/lists', 'split', ['str.split'])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:split"
```
A natural follow-up would be to explode the list of temperatures so that each measurement is in its
own row:
{{code_block('user-guide/expressions/lists', 'explode', ['explode'])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:explode"
```
However, in Polars we often do not need to do this to operate on the list elements.
### Operating on lists
Polars provides several standard operations on columns with the `List` data type.
[Similar to what you can do with strings](strings.md#slicing), lists can be sliced with the
functions `head`, `tail`, and `slice`:
{{code_block('user-guide/expressions/lists', 'list-slicing', ['Expr.list'])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:list-slicing"
```
### Element-wise computation within lists
If we need to identify the stations that are giving the most number of errors we need to
1. try to convert the measurements into numbers;
2. count the number of non-numeric values (i.e., `null` values) in the list, by row; and
3. rename this output column as â€œerrorsâ€� so that we can easily identify the stations.
To perform these steps, we need to perform a casting operation on each measurement within the list
values. The function `eval` is used as the entry point to perform operations on the elements of the
list. Within it, you can use the context `element` to refer to each single element of the list
individually, and then you can use any Polars expression on the element:
{{code_block('user-guide/expressions/lists', 'element-wise-casting', ['element'])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:element-wise-casting"
```
Another alternative would be to use a regular expression to check if a measurement starts with a
letter:
{{code_block('user-guide/expressions/lists', 'element-wise-regex', ['element'])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:element-wise-regex"
```
If you are unfamiliar with the namespace `str` or the notation `(?i)` in the regex, now is a good
time to
[look at how to work with strings and regular expressions in Polars](strings.md#check-for-the-existence-of-a-pattern).
### Row-wise computations
The function `eval` gives us access to the list elements and `pl.element` refers to each individual
element, but we can also use `pl.all()` to refer to all of the elements of the list.
To show this in action, we will start by creating another dataframe with some more weather data:
{{code_block('user-guide/expressions/lists', 'weather_by_day', [])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:weather_by_day"
```
Now, we will calculate the percentage rank of the temperatures by day, measured across stations.
Polars does not provide a function to do this directly, but because expressions are so versatile we
can create our own percentage rank expression for highest temperature. Let's try that:
{{code_block('user-guide/expressions/lists', 'rank_pct', ['element', 'rank'])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:rank_pct"
```
## Working with arrays
### Creating an array column
As [we have seen above](#the-data-type-array), Polars usually does not infer the data type `Array`
automatically. You have to specify the data type `Array` when creating a series/dataframe or
[cast a column](casting.md) explicitly unless you create the column out of a NumPy array.
### The namespace `arr`
The data type `Array` was recently introduced and is still pretty nascent in features that it
offers. Even so, the namespace `arr` aggregates several functions that you can use to work with
arrays.
!!! warning "`arr` then, `list` now"
In previous versions of Polars, the namespace for list operations used to be `arr`.
`arr` is now the namespace for the data type `Array`.
If you find references to the namespace `arr` on StackOverflow or other sources, note that those sources _may_ be outdated.
The API documentation should give you a good overview of the functions in the namespace `arr`, of
which we present a couple:
{{code_block('user-guide/expressions/lists', 'array-overview', ['Expr.arr'])}}
```python exec="on" result="text" session="expressions/lists"
--8<-- "python/user-guide/expressions/lists.py:array-overview"
```
---
polars/docs/source/user-guide/expressions/missing-data.md
---
# Missing data
This section of the user guide teaches how to work with missing data in Polars.
## `null` and `NaN` values
In Polars, missing data is represented by the value `null`. This missing value `null` is used for
all data types, including numerical types.
Polars also supports the value `NaN` (â€œNot a Numberâ€�) for columns with floating point numbers. The
value `NaN` is considered to be a valid floating point value, which is different from missing data.
[We discuss the value `NaN` separately below](#not-a-number-or-nan-values).
When creating a series or a dataframe, you can set a value to `null` by using the appropriate
construct for your language:
{{code_block('user-guide/expressions/missing-data','dataframe',['DataFrame'])}}
```python exec="on" result="text" session="user-guide/missing-data"
--8<-- "python/user-guide/expressions/missing-data.py:dataframe"
```
!!! info "Difference from pandas"
In pandas, the value used to represent missing data depends on the data type of the column.
In Polars, missing data is always represented by the value `null`.
## Missing data metadata
Polars keeps track of some metadata regarding the missing data of each series. This metadata allows
Polars to answer some basic queries about missing values in a very efficient way, namely how many
values are missing and which ones are missing.
To determine how many values are missing from a column you can use the function `null_count`:
{{code_block('user-guide/expressions/missing-data','count',['null_count'])}}
```python exec="on" result="text" session="user-guide/missing-data"
--8<-- "python/user-guide/expressions/missing-data.py:count"
```
The function `null_count` can be called on a dataframe, a column from a dataframe, or on a series
directly. The function `null_count` is a cheap operation because the result is already known.
Polars uses something called a â€œvalidity bitmapâ€� to know which values are missing in a series. The
validity bitmap is memory efficient as it is bit encoded. If a series has length $n$, then its
validity bitmap will cost $n / 8$ bytes. The function `is_null` uses the validity bitmap to
efficiently report which values are `null` and which are not:
{{code_block('user-guide/expressions/missing-data','isnull',['is_null'])}}
```python exec="on" result="text" session="user-guide/missing-data"
--8<-- "python/user-guide/expressions/missing-data.py:isnull"
```
The function `is_null` can be used on a column of a dataframe or on a series directly. Again, this
is a cheap operation because the result is already known by Polars.
??? info "Why does Polars waste memory on a validity bitmap?"
It all comes down to a tradeoff.
By using a bit more memory per column, Polars can be much more efficient when performing most operations on your columns.
If the validity bitmap wasn't known, every time you wanted to compute something you would have to check each position of the series to see if a legal value was present or not.
With the validity bitmap, Polars knows automatically the positions where your operations can be applied.
## Filling missing data
Missing data in a series can be filled with the function `fill_null`. You can specify how missing
data is effectively filled in a couple of different ways:
- a literal of the correct data type;
- a Polars expression, such as replacing with values computed from another column;
- a strategy based on neighbouring values, such as filling forwards or backwards; and
- interpolation.
To illustrate how each of these methods work we start by defining a simple dataframe with two
missing values in the second column:
{{code_block('user-guide/expressions/missing-data','dataframe2',['DataFrame'])}}
```python exec="on" result="text" session="user-guide/missing-data"
--8<-- "python/user-guide/expressions/missing-data.py:dataframe2"
```
### Fill with a specified literal value
You can fill the missing data with a specified literal value. This literal value will replace all of
the occurrences of the value `null`:
{{code_block('user-guide/expressions/missing-data','fill',['fill_null'])}}
```python exec="on" result="text" session="user-guide/missing-data"
--8<-- "python/user-guide/expressions/missing-data.py:fill"
```
However, this is actually just a special case of the general case where
[the function `fill_null` replaces missing values with the corresponding values from the result of a Polars expression](#fill-with-a-strategy-based-on-neighbouring-values),
as seen next.
### Fill with an expression
In the general case, the missing data can be filled by extracting the corresponding values from the
result of a general Polars expression. For example, we can fill the second column with values taken
from the double of the first column:
{{code_block('user-guide/expressions/missing-data','fillexpr',['fill_null'])}}
```python exec="on" result="text" session="user-guide/missing-data"
--8<-- "python/user-guide/expressions/missing-data.py:fillexpr"
```
### Fill with a strategy based on neighbouring values
You can also fill the missing data by following a fill strategy based on the neighbouring values.
The two simpler strategies look for the first non-`null` value that comes immediately before or
immediately after the value `null` that is being filled:
{{code_block('user-guide/expressions/missing-data','fillstrategy',['fill_null'])}}
```python exec="on" result="text" session="user-guide/missing-data"
--8<-- "python/user-guide/expressions/missing-data.py:fillstrategy"
```
You can find other fill strategies in the API docs.
### Fill with interpolation
Additionally, you can fill intermediate missing data with interpolation by using the function
`interpolate` instead of the function `fill_null`:
{{code_block('user-guide/expressions/missing-data','fillinterpolate',['interpolate'])}}
```python exec="on" result="text" session="user-guide/missing-data"
--8<-- "python/user-guide/expressions/missing-data.py:fillinterpolate"
```
Note: With interpolate, nulls at the beginning and end of the series remain null.
## Not a Number, or `NaN` values
Missing data in a series is only ever represented by the value `null`, regardless of the data type
of the series. Columns with a floating point data type can sometimes have the value `NaN`, which
might be confused with `null`.
The special value `NaN` can be created directly:
{{code_block('user-guide/expressions/missing-data','nan',['DataFrame'])}}
```python exec="on" result="text" session="user-guide/missing-data"
--8<-- "python/user-guide/expressions/missing-data.py:nan"
```
And it might also arise as the result of a computation:
{{code_block('user-guide/expressions/missing-data','nan-computed',[])}}
```python exec="on" result="text" session="user-guide/missing-data"
--8<-- "python/user-guide/expressions/missing-data.py:nan-computed"
```
!!! info
By default, a `NaN` value in an integer column causes the column to be cast to a float data type in pandas.
This does not happen in Polars; instead, an exception is raised.
`NaN` values are considered to be a type of floating point data and are **not considered to be
missing data** in Polars. This means:
- `NaN` values are **not** counted with the function `null_count`; and
- `NaN` values are filled when you use the specialised function `fill_nan` method but are **not**
filled with the function `fill_null`.
Polars has the functions `is_nan` and `fill_nan`, which work in a similar way to the functions
`is_null` and `fill_null`. Unlike with missing data, Polars does not hold any metadata regarding the
`NaN` values, so the function `is_nan` entails actual computation.
One further difference between the values `null` and `NaN` is that numerical aggregating functions,
like `mean` and `sum`, skip the missing values when computing the result, whereas the value `NaN` is
considered for the computation and typically propagates into the result. If desirable, this behavior
can be avoided by replacing the occurrences of the value `NaN` with the value `null`:
{{code_block('user-guide/expressions/missing-data','nanfill',['fill_nan'])}}
```python exec="on" result="text" session="user-guide/missing-data"
--8<-- "python/user-guide/expressions/missing-data.py:nanfill"
```
You can learn more about the value `NaN` in
[the section about floating point number data types](../concepts/data-types-and-structures.md#floating-point-numbers).
---
polars/docs/source/user-guide/expressions/numpy-functions.md
---
# Numpy functions
Polars expressions support NumPy [ufuncs](https://numpy.org/doc/stable/reference/ufuncs.html). See
[the NumPy documentation for a list of all supported NumPy functions](https://numpy.org/doc/stable/reference/ufuncs.html#available-ufuncs).
This means that if a function is not provided by Polars, we can use NumPy and we still have fast
columnar operations through the NumPy API.
## Example
{{code_block('user-guide/expressions/numpy-example',api_functions=['DataFrame','np.log'])}}
```python exec="on" result="text" session="user-guide/numpy"
--8<-- "python/user-guide/expressions/numpy-example.py"
```
## Interoperability
Polars' series have support for NumPy universal functions (ufuncs) and generalized ufuncs.
Element-wise functions such as `np.exp`, `np.cos`, `np.div`, etc, all work with almost zero
overhead.
However, bear in mind that
[Polars keeps track of missing values with a separate bitmask](missing-data.md) and NumPy does not
receive this information. This can lead to a window function or a `np.convolve` giving flawed or
incomplete results, so an error will be raised if you pass a series with missing data to a
generalized ufunc. Convert a Polars series to a NumPy array with the function `to_numpy`. Missing
values will be replaced by `np.nan` during the conversion.
---
polars/docs/source/user-guide/expressions/strings.md
---
# Strings
The following section discusses operations performed on string data, which is a frequently used data
type when working with dataframes. String processing functions are available in the namespace `str`.
Working with strings in other dataframe libraries can be highly inefficient due to the fact that
strings have unpredictable lengths. Polars mitigates these inefficiencies by
[following the Arrow Columnar Format specification](../concepts/data-types-and-structures.md#data-types-internals),
so you can write performant data queries on string data too.
## The string namespace
When working with string data you will likely need to access the namespace `str`, which aggregates
40+ functions that let you work with strings. As an example of how to access functions from within
that namespace, the snippet below shows how to compute the length of the strings in a column in
terms of the number of bytes and the number of characters:
{{code_block('user-guide/expressions/strings','df',['str.len_bytes','str.len_chars'])}}
```python exec="on" result="text" session="expressions/strings"
--8<-- "python/user-guide/expressions/strings.py:df"
```
!!! note
If you are working exclusively with ASCII text, then the results of the two computations will be the same and using `len_bytes` is recommended since it is faster.
## Parsing strings
Polars offers multiple methods for checking and parsing elements of a string column, namely checking
for the existence of given substrings or patterns, and counting, extracting, or replacing, them. We
will demonstrate some of these operations in the upcoming examples.
### Check for the existence of a pattern
We can use the function `contains` to check for the presence of a pattern within a string. By
default, the argument to the function `contains` is interpreted as a regular expression. If you want
to specify a literal substring, set the parameter `literal` to `True`.
For the special cases where you want to check if the strings start or end with a fixed substring,
you can use the functions `starts_with` or `ends_with`, respectively.
{{code_block('user-guide/expressions/strings','existence',['str.contains',
'str.starts_with','str.ends_with'])}}
```python exec="on" result="text" session="expressions/strings"
--8<-- "python/user-guide/expressions/strings.py:existence"
```
### Regex specification
Polars relies on the Rust crate `regex` to work with regular expressions, so you may need to
[refer to the syntax documentation](https://docs.rs/regex/latest/regex/#syntax) to see what features
and flags are supported. In particular, note that the flavor of regex supported by Polars is
different from Python's module `re`.
### Extract a pattern
The function `extract` allows us to extract patterns from the string values in a column. The
function `extract` accepts a regex pattern with one or more capture groups and extracts the capture
group specified as the second argument.
{{code_block('user-guide/expressions/strings','extract',['str.extract'])}}
```python exec="on" result="text" session="expressions/strings"
--8<-- "python/user-guide/expressions/strings.py:extract"
```
To extract all occurrences of a pattern within a string, we can use the function `extract_all`. In
the example below, we extract all numbers from a string using the regex pattern `(\d+)`, which
matches one or more digits. The resulting output of the function `extract_all` is a list containing
all instances of the matched pattern within the string.
{{code_block('user-guide/expressions/strings','extract_all',['str.extract_all'])}}
```python exec="on" result="text" session="expressions/strings"
--8<-- "python/user-guide/expressions/strings.py:extract_all"
```
### Replace a pattern
Akin to the functions `extract` and `extract_all`, Polars provides the functions `replace` and
`replace_all`. These accept a regex pattern or a literal substring (if the parameter `literal` is
set to `True`) and perform the replacements specified. The function `replace` will make at most one
replacement whereas the function `replace_all` will make all the non-overlapping replacements it
finds.
{{code_block('user-guide/expressions/strings','replace',['str.replace', 'str.replace_all'])}}
```python exec="on" result="text" session="expressions/strings"
--8<-- "python/user-guide/expressions/strings.py:replace"
```
## Modifying strings
### Case conversion
Converting the casing of a string is a common operation and Polars supports it out of the box with
the functions `to_lowercase`, `to_titlecase`, and `to_uppercase`:
{{code_block('user-guide/expressions/strings','casing', ['str.to_lowercase', 'str.to_titlecase',
'str.to_uppercase'])}}
```python exec="on" result="text" session="expressions/strings"
--8<-- "python/user-guide/expressions/strings.py:casing"
```
### Stripping characters from the ends
Polars provides five functions in the namespace `str` that let you strip characters from the ends of
the string:
| Function | Behaviour |
| ------------------- | --------------------------------------------------------------------- |
| `strip_chars` | Removes leading and trailing occurrences of the characters specified. |
| `strip_chars_end` | Removes trailing occurrences of the characters specified. |
| `strip_chars_start` | Removes leading occurrences of the characters specified. |
| `strip_prefix` | Removes an exact substring prefix if present. |
| `strip_suffix` | Removes an exact substring suffix if present. |
??? info "Similarity to Python string methods"
`strip_chars` is similar to Python's string method `strip` and `strip_prefix`/`strip_suffix`
are similar to Python's string methods `removeprefix` and `removesuffix`, respectively.
It is important to understand that the first three functions interpret their string argument as a
set of characters whereas the functions `strip_prefix` and `strip_suffix` do interpret their string
argument as a literal string.
{{code_block('user-guide/expressions/strings', 'strip', ['str.strip_chars', 'str.strip_chars_end',
'str.strip_chars_start', 'str.strip_prefix', 'str.strip_suffix'])}}
```python exec="on" result="text" session="expressions/strings"
--8<-- "python/user-guide/expressions/strings.py:strip"
```
If no argument is provided, the three functions `strip_chars`, `strip_chars_end`, and
`strip_chars_start`, remove whitespace by default.
### Slicing
Besides [extracting substrings as specified by patterns](#extract-a-pattern), you can also slice
strings at specified offsets to produce substrings. The general-purpose function for slicing is
`slice` and it takes the starting offset and the optional _length_ of the slice. If the length of
the slice is not specified or if it's past the end of the string, Polars slices the string all the
way to the end.
The functions `head` and `tail` are specialised versions used for slicing the beginning and end of a
string, respectively.
{{code_block('user-guide/expressions/strings', 'slice', [], ['str.slice', 'str.head', 'str.tail'],
['str.str_slice', 'str.str_head', 'str.str_tail'])}}
```python exec="on" result="text" session="expressions/strings"
--8<-- "python/user-guide/expressions/strings.py:slice"
```
## API documentation
In addition to the examples covered above, Polars offers various other string manipulation
functions. To explore these additional methods, you can go to the API documentation of your chosen
programming language for Polars.
---
polars/docs/source/user-guide/expressions/structs.md
---
# Structs
The data type `Struct` is a composite data type that can store multiple fields in a single column.
!!! tip "Python analogy"
For Python users, the data type `Struct` is kind of like a Python
dictionary. Even better, if you are familiar with Python typing, you can think of the data type
`Struct` as `typing.TypedDict`.
In this page of the user guide we will see situations in which the data type `Struct` arises, we
will understand why it does arise, and we will see how to work with `Struct` values.
Let's start with a dataframe that captures the average rating of a few movies across some states in
the US:
{{code_block('user-guide/expressions/structs','ratings_df',['DataFrame'])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:ratings_df"
```
## Encountering the data type `Struct`
A common operation that will lead to a `Struct` column is the ever so popular `value_counts`
function that is commonly used in exploratory data analysis. Checking the number of times a state
appears in the data is done as so:
{{code_block('user-guide/expressions/structs','state_value_counts',['value_counts'])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:state_value_counts"
```
Quite unexpected an output, especially if coming from tools that do not have such a data type. We're
not in peril, though. To get back to a more familiar output, all we need to do is use the function
`unnest` on the `Struct` column:
{{code_block('user-guide/expressions/structs','struct_unnest',['unnest'])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:struct_unnest"
```
The function `unnest` will turn each field of the `Struct` into its own column.
!!! note "Why `value_counts` returns a `Struct`"
Polars expressions always operate on a single series and return another series.
`Struct` is the data type that allows us to provide multiple columns as input to an expression, or to output multiple columns from an expression.
Thus, we can use the data type `Struct` to specify each value and its count when we use `value_counts`.
## Inferring the data type `Struct` from dictionaries
When building series or dataframes, Polars will convert dictionaries to the data type `Struct`:
{{code_block('user-guide/expressions/structs','series_struct',['Series'])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:series_struct"
```
The number of fields, their names, and their types, are inferred from the first dictionary seen.
Subsequent incongruences can result in `null` values or in errors:
{{code_block('user-guide/expressions/structs','series_struct_error',['Series'])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:series_struct_error"
```
## Extracting individual values of a `Struct`
Let's say that we needed to obtain just the field `"Movie"` from the `Struct` in the series that we
created above. We can use the function `field` to do so:
{{code_block('user-guide/expressions/structs','series_struct_extract',['struct.field'])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:series_struct_extract"
```
## Renaming individual fields of a `Struct`
What if we need to rename individual fields of a `Struct` column? We use the function
`rename_fields`:
{{code_block('user-guide/expressions/structs','series_struct_rename',['struct.rename_fields'])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:series_struct_rename"
```
To be able to actually see that the field names were changed, we will create a dataframe where the
only column is the result and then we use the function `unnest` so that each field becomes its own
column. The column names will reflect the renaming operation we just did:
{{code_block('user-guide/expressions/structs','struct-rename-check',['struct.rename_fields'])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:struct-rename-check"
```
## Practical use-cases of `Struct` columns
### Identifying duplicate rows
Let's get back to the `ratings` data. We want to identify cases where there are duplicates at a
â€œMovieâ€� and â€œTheatreâ€� level.
This is where the data type `Struct` shines:
{{code_block('user-guide/expressions/structs','struct_duplicates',['is_duplicated', 'struct'])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:struct_duplicates"
```
We can identify the unique cases at this level also with `is_unique`!
### Multi-column ranking
Suppose, given that we know there are duplicates, we want to choose which rating gets a higher
priority. We can say that the column â€œCountâ€� is the most important, and if there is a tie in the
column â€œCountâ€� then we consider the column â€œAvg_Ratingâ€�.
We can then do:
{{code_block('user-guide/expressions/structs','struct_ranking',['is_duplicated', 'struct'])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:struct_ranking"
```
That's a pretty complex set of requirements done very elegantly in Polars! To learn more about the
function `over`, used above, [see the user guide section on window functions](window-functions.md).
### Using multiple columns in a single expression
As mentioned earlier, the data type `Struct` is also useful if you need to pass multiple columns as
input to an expression. As an example, suppose we want to compute
[the Ackermann function](https://en.wikipedia.org/wiki/Ackermann_function) on two columns of a
dataframe. There is no way of composing Polars expressions to compute the Ackermann function[^1], so
we define a custom function:
{{code_block('user-guide/expressions/structs', 'ack', [])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:ack"
```
Now, to compute the values of the Ackermann function on those arguments, we start by creating a
`Struct` with fields `m` and `n` and then use the function `map_elements` to apply the function
`ack` to each value:
{{code_block('user-guide/expressions/structs','struct-ack',[], ['map_elements'], [])}}
```python exec="on" result="text" session="expressions/structs"
--8<-- "python/user-guide/expressions/structs.py:struct-ack"
```
Refer to
[this section of the user guide to learn more about applying user-defined Python functions to your data](user-defined-python-functions.md).
[^1]: To say that something cannot be done is quite a bold claim. If you prove us wrong, please let
us know!
---
polars/docs/source/user-guide/expressions/user-defined-python-functions.md
---
# User-defined Python functions
Polars expressions are quite powerful and flexible, so there is much less need for custom Python
functions compared to other libraries. Still, you may need to pass an expression's state to a third
party library or apply your black box function to data in Polars.
In this part of the documentation we'll be using two APIs that allows you to do this:
- [:material-api: `map_elements`](https://docs.pola.rs/py-polars/html/reference/expressions/api/polars.Expr.map_elements.html):
Call a function separately on each value in the `Series`.
- [:material-api: `map_batches`](https://docs.pola.rs/py-polars/html/reference/expressions/api/polars.Expr.map_batches.html):
Always passes the full `Series` to the function.
## Processing individual values with `map_elements()`
Let's start with the simplest case: we want to process each value in a `Series` individually. Here
is our data:
{{code_block('user-guide/expressions/user-defined-functions','dataframe',[])}}
```python exec="on" result="text" session="user-guide/udf"
--8<-- "python/user-guide/expressions/user-defined-functions.py:setup"
--8<-- "python/user-guide/expressions/user-defined-functions.py:dataframe"
```
We'll call `math.log()` on each individual value:
{{code_block('user-guide/expressions/user-defined-functions','individual_log',[])}}
```python exec="on" result="text" session="user-guide/udf"
--8<-- "python/user-guide/expressions/user-defined-functions.py:individual_log"
```
While this works, `map_elements()` has two problems:
1. **Limited to individual items:** Often you'll want to have a calculation that needs to operate on
the whole `Series`, rather than individual items one by one.
2. **Performance overhead:** Even if you do want to process each item individually, calling a
function for each individual item is slow; all those extra function calls add a lot of overhead.
Let's start by solving the first problem, and then we'll see how to solve the second problem.
## Processing a whole `Series` with `map_batches()`
We want to run a custom function on the contents of a whole `Series`. For demonstration purposes,
let's say we want to calculate the difference between the mean of a `Series` and each value.
We can use the `map_batches()` API to run this function on either the full `Series` or individual
groups in a `group_by()`:
{{code_block('user-guide/expressions/user-defined-functions','diff_from_mean',[])}}
```python exec="on" result="text" session="user-guide/udf"
--8<-- "python/user-guide/expressions/user-defined-functions.py:diff_from_mean"
```
## Fast operations with user-defined functions
The problem with a pure-Python implementation is that it's slow. In general, you want to minimize
how much Python code you call if you want fast results.
To maximize speed, you'll want to make sure that you're using a function written in a compiled
language. For numeric calculations Polars supports a pair of interfaces defined by NumPy called
["ufuncs"](https://numpy.org/doc/stable/reference/ufuncs.html) and
["generalized ufuncs"](https://numpy.org/neps/nep-0005-generalized-ufuncs.html). The former runs on
each item individually, and the latter accepts a whole NumPy array, which allows for more flexible
operations.
[NumPy](https://numpy.org/doc/stable/reference/ufuncs.html) and other libraries like
[SciPy](https://docs.scipy.org/doc/scipy/reference/special.html#module-scipy.special) come with
pre-written ufuncs you can use with Polars. For example:
{{code_block('user-guide/expressions/user-defined-functions','np_log',[])}}
```python exec="on" result="text" session="user-guide/udf"
--8<-- "python/user-guide/expressions/user-defined-functions.py:np_log"
```
Notice that we can use `map_batches()`, because `numpy.log()` is able to run on both individual
items and on whole NumPy arrays. This means it will run much faster than our original example, since
we only have a single Python call and then all processing happens in a fast low-level language.
## Example: A fast custom function using Numba
The pre-written functions NumPy provides are helpful, but our goal is to write our own functions.
For example, let's say we want a fast version of our `diff_from_mean()` example above. The easiest
way to write this in Python is to use [Numba](https://numba.readthedocs.io/en/stable/), which allows
you to write custom functions in (a subset) of Python while still getting the benefit of compiled
code.
In particular, Numba provides a decorator called
[`@guvectorize`](https://numba.readthedocs.io/en/stable/user/vectorize.html#the-guvectorize-decorator).
This creates a generalized ufunc by compiling a Python function to fast machine code, in a way that
allows it to be used by Polars.
In the following example the `diff_from_mean_numba()` will be compiled to fast machine code at
import time, which will take a little time. After that all calls to the function will run quickly.
The `Series` will be converted to a NumPy array before being passed to the function:
{{code_block('user-guide/expressions/user-defined-functions','diff_from_mean_numba',[])}}
```python exec="on" result="text" session="user-guide/udf"
--8<-- "python/user-guide/expressions/user-defined-functions.py:diff_from_mean_numba"
```
## Missing data is not allowed when calling generalized ufuncs
Before being passed to a user-defined function like `diff_from_mean_numba()`, a `Series` will be
converted to a NumPy array. Unfortunately, NumPy arrays don't have a concept of missing data. If
there is missing data in the original `Series`, this means the resulting array won't actually match
the `Series`.
If you're calculating results item by item, this doesn't matter. For example, `numpy.log()` gets
called on each individual value separately, so those missing values don't change the calculation.
But if the result of a user-defined function depend on multiple values in the `Series`, it's not
clear what exactly should happen with the missing values.
Therefore, when calling generalized ufuncs such as Numba functions decorated with `@guvectorize`,
Polars will raise an error if you try to pass in a `Series` with missing data. How do you get rid of
missing data? Either [fill it in](missing-data.md) or
[drop it](https://docs.pola.rs/py-polars/html/reference/dataframe/api/polars.DataFrame.drop_nulls.html)
before calling your custom function.
## Combining multiple column values
If you want to pass multiple columns to a user-defined function, you can use `Struct`s, which are
[covered in detail in a different section](structs.md). The basic idea is to combine multiple
columns into a `Struct`, and then the function can extract the columns back out:
{{code_block('user-guide/expressions/user-defined-functions','combine',[])}}
```python exec="on" result="text" session="user-guide/udf"
--8<-- "python/user-guide/expressions/user-defined-functions.py:combine"
```
## Streaming calculations
Passing the full `Series` to the user-defined function has a cost: it may use a lot of memory, as
its contents are copied into a NumPy array. You can use the `is_elementwise=True` argument to
[:material-api: `map_batches`](https://docs.pola.rs/py-polars/html/reference/expressions/api/polars.Expr.map_batches.html)
to stream results into the function, which means it might not get all values at once.
!!! note
The `is_elementwise` argument can lead to incorrect results if set incorrectly.
If you set `is_elementwise=True`, make sure that your function actually operates
element-by-element (e.g. "calculate the logarithm of each value") - our example function `diff_from_mean()`, for instance, does not.
## Return types
Custom Python functions are often black boxes; Polars doesn't know what your function is doing or
what it will return. The return data type is therefore automatically inferred. We do that by waiting
for the first non-null value. That value will then be used to determine the type of the resulting
`Series`.
The mapping of Python types to Polars data types is as follows:
- `int` -> `Int64`
- `float` -> `Float64`
- `bool` -> `Boolean`
- `str` -> `String`
- `list[tp]` -> `List[tp]` (where the inner type is inferred with the same rules)
- `dict[str, [tp]]` -> `struct`
- `Any` -> `object` (Prevent this at all times)
Rust types map as follows:
- `i32` or `i64` -> `Int64`
- `f32` or `f64` -> `Float64`
- `bool` -> `Boolean`
- `String` or `str` -> `String`
- `Vec` -> `List[tp]` (where the inner type is inferred with the same rules)
You can pass a `return_dtype` argument to
[:material-api: `map_batches`](https://docs.pola.rs/py-polars/html/reference/expressions/api/polars.Expr.map_batches.html)
if you want to override the inferred type.
---
polars/docs/source/user-guide/expressions/window-functions.md
---
# Window functions
Window functions are expressions with superpowers. They allow you to perform aggregations on groups
within the context `select`. Let's get a feel for what that means.
First, we load a PokÃ©mon dataset:
{{code_block('user-guide/expressions/window','pokemon',['read_csv'])}}
```python exec="on" result="text" session="user-guide/window"
--8<-- "python/user-guide/expressions/window.py:pokemon"
```
## Operations per group
Window functions are ideal when we want to perform an operation within a group. For instance,
suppose we want to rank our PokÃ©mon by the column â€œSpeedâ€�. However, instead of a global ranking, we
want to rank the speed within each group defined by the column â€œType 1â€�. We write the expression to
rank the data by the column â€œSpeedâ€� and then we add the function `over` to specify that this should
happen over the unique values of the column â€œType 1â€�:
{{code_block('user-guide/expressions/window','rank',['over'])}}
```python exec="on" result="text" session="user-guide/window"
--8<-- "python/user-guide/expressions/window.py:rank"
```
To help visualise this operation, you may imagine that Polars selects the subsets of the data that
share the same value for the column â€œType 1â€� and then computes the ranking expression only for those
values. Then, the results for that specific group are projected back to the original rows and Polars
does this for all of the existing groups. The diagram below highlights the ranking computation for
the PokÃ©mon with â€œType 1â€� equal to â€œGrassâ€�.
--8<-- "docs/source/user-guide/expressions/speed_rank_by_type.svg"
Note how the row for the PokÃ©mon â€œGolbatâ€� has a â€œSpeedâ€� value of `90`, which is greater than the
value `80` of the PokÃ©mon â€œVenusaurâ€�, and yet the latter was ranked 1 because â€œGolbatâ€� and â€œVenusarâ€�
do not share the same value for the column â€œType 1â€�.
The function `over` accepts an arbitrary number of expressions to specify the groups over which to
perform the computations. We can repeat the ranking above, but over the combination of the columns
â€œType 1â€� and â€œType 2â€� for a more fine-grained ranking:
{{code_block('user-guide/expressions/window','rank-multiple',['over'])}}
```python exec="on" result="text" session="user-guide/window"
--8<-- "python/user-guide/expressions/window.py:rank-multiple"
```
In general, the results you get with the function `over` can also be achieved with
[an aggregation](aggregation.md) followed by a call to the function `explode`, although the rows
would be in a different order:
{{code_block('user-guide/expressions/window','rank-explode',['explode'])}}
```python exec="on" result="text" session="user-guide/window"
--8<-- "python/user-guide/expressions/window.py:rank-explode"
```
This shows that, usually, `group_by` and `over` produce results of different shapes:
- `group_by` usually produces a resulting dataframe with as many rows as groups used for
aggregating; and
- `over` usually produces a dataframe with the same number of rows as the original.
The function `over` does not always produce results with the same number of rows as the original
dataframe, and that is what we explore next.
## Mapping results to dataframe rows
The function `over` accepts a parameter `mapping_strategy` that determines how the results of the
expression over the group are mapped back to the rows of the dataframe.
### `group_to_rows`
The default behaviour is `"group_to_rows"`: the result of the expression over the group should be
the same length as the group and the results are mapped back to the rows of that group.
If the order of the rows is not relevant, the option `"explode"` is more performant. Instead of
mapping the resulting values to the original rows, Polars creates a new dataframe where values from
the same group are next to each other. To help understand the distinction, consider the following
dataframe:
```python exec="on" result="text" session="user-guide/window"
--8<-- "python/user-guide/expressions/window.py:athletes"
```
We can sort the athletes by rank within their own countries. If we do so, the Dutch athletes were in
the second, third, and sixth, rows, and they will remain there. What will change is the order of the
names of the athletes, which goes from â€œBâ€�, â€œCâ€�, and â€œFâ€�, to â€œBâ€�, â€œFâ€�, and â€œCâ€�:
{{code_block('user-guide/expressions/window','athletes-sort-over-country',['over'])}}
```python exec="on" result="text" session="user-guide/window"
--8<-- "python/user-guide/expressions/window.py:athletes-sort-over-country"
```
The diagram below represents this transformation:
--8<-- "docs/source/user-guide/expressions/athletes_over_country.svg"
### `explode`
If we set the parameter `mapping_strategy` to `"explode"`, then athletes of the same country are
grouped together, but the final order of the rows â€“ with respect to the countries â€“ will not be the
same, as the diagram shows:
--8<-- "docs/source/user-guide/expressions/athletes_over_country_explode.svg"
Because Polars does not need to keep track of the positions of the rows of each group, using
`"explode"` is typically faster than `"group_to_rows"`. However, using `"explode"` also requires
more care because it implies reordering the other columns that we wish to keep. The code that
produces this result follows
{{code_block('user-guide/expressions/window','athletes-explode',['over'])}}
```python exec="on" result="text" session="user-guide/window"
--8<-- "python/user-guide/expressions/window.py:athletes-explode"
```
### `join`
Another possible value for the parameter `mapping_strategy` is `"join"`, which aggregates the
resulting values in a list and repeats the list over all rows of the same group:
{{code_block('user-guide/expressions/window','athletes-join',['over'])}}
```python exec="on" result="text" session="user-guide/window"
--8<-- "python/user-guide/expressions/window.py:athletes-join"
```
## Windowed aggregation expressions
In case the expression applied to the values of a group produces a scalar value, the scalar is
broadcast across the rows of the group:
{{code_block('user-guide/expressions/window','pokemon-mean',['over'])}}
```python exec="on" result="text" session="user-guide/window"
--8<-- "python/user-guide/expressions/window.py:pokemon-mean"
```
## More examples
For more exercises, below are some window functions for us to compute:
- sort all PokÃ©mon by type;
- select the first `3` PokÃ©mon per type as `"Type 1"`;
- sort the PokÃ©mon within a type by speed in descending order and select the first `3` as
`"fastest/group"`;
- sort the PokÃ©mon within a type by attack in descending order and select the first `3` as
`"strongest/group"`; and
- sort the PokÃ©mon within a type by name and select the first `3` as `"sorted_by_alphabet"`.
{{code_block('user-guide/expressions/window','examples',['over'])}}
```python exec="on" result="text" session="user-guide/window"
--8<-- "python/user-guide/expressions/window.py:examples"
```
---
polars/docs/source/user-guide/concepts/_streaming.md
---
# Streaming
One additional benefit of the lazy API is that it allows queries to be executed in a streaming
manner. Instead of processing all the data at once, Polars can execute the query in batches allowing
you to process datasets that do not fit in memory.
To tell Polars we want to execute a query in streaming mode we pass the `engine="streaming"`
argument to `collect`
{{code_block('user-guide/concepts/streaming','streaming',['collect'])}}
## When is streaming available?
Streaming is still in development. We can ask Polars to execute any lazy query in streaming mode.
However, not all lazy operations support streaming. If there is an operation for which streaming is
not supported, Polars will run the query in non-streaming mode.
Streaming is supported for many operations including:
- `filter`, `slice`, `head`, `tail`
- `with_columns`, `select`
- `group_by`
- `join`
- `unique`
- `sort`
- `explode`, `unpivot`
- `scan_csv`, `scan_parquet`, `scan_ipc`
This list is not exhaustive. Polars is in active development, and more operations can be added
without explicit notice.
### Example with supported operations
To determine which parts of your query are streaming, use the `explain` method. Below is an example
that demonstrates how to inspect the query plan. More information about the query plan can be found
in the chapter on the [Lazy API](https://docs.pola.rs/user-guide/lazy/query-plan/).
{{code_block('user-guide/concepts/streaming', 'example',['explain'])}}
```python exec="on" result="text" session="user-guide/streaming"
--8<-- "python/user-guide/concepts/streaming.py:import"
--8<-- "python/user-guide/concepts/streaming.py:streaming"
--8<-- "python/user-guide/concepts/streaming.py:example"
```
### Example with non-streaming operations
{{code_block('user-guide/concepts/streaming', 'example2',['explain'])}}
```python exec="on" result="text" session="user-guide/streaming"
--8<-- "python/user-guide/concepts/streaming.py:import"
--8<-- "python/user-guide/concepts/streaming.py:example2"
```
---
polars/docs/source/user-guide/concepts/data-types-and-structures.md
---
# Data types and structures
## Data types
Polars supports a variety of data types that fall broadly under the following categories:
- Numeric data types: signed integers, unsigned integers, floating point numbers, and decimals.
- Nested data types: lists, structs, and arrays.
- Temporal: dates, datetimes, times, and time deltas.
- Miscellaneous: strings, binary data, Booleans, categoricals, enums, and objects.
All types support missing values represented by the special value `null`. This is not to be
conflated with the special value `NaN` in floating number data types; see the
[section about floating point numbers](#floating-point-numbers) for more information.
You can also find a
[full table with all data types supported in the appendix](#appendix-full-data-types-table) with
notes on when to use each data type and with links to relevant parts of the documentation.
## Series
The core base data structures provided by Polars are series and dataframes. A series is a
1-dimensional homogeneous data structure. By â€œhomogeneousâ€� we mean that all elements inside a series
have the same data type. The snippet below shows how to create a named series:
{{code_block('user-guide/concepts/data-types-and-structures','series',['Series'])}}
```python exec="on" result="text" session="user-guide/data-types-and-structures"
--8<-- "python/user-guide/concepts/data-types-and-structures.py:series"
```
When creating a series, Polars will infer the data type from the values you provide. You can specify
a concrete data type to override the inference mechanism:
{{code_block('user-guide/concepts/data-types-and-structures','series-dtype',['Series'])}}
```python exec="on" result="text" session="user-guide/data-types-and-structures"
--8<-- "python/user-guide/concepts/data-types-and-structures.py:series-dtype"
```
## Dataframe
A dataframe is a 2-dimensional heterogeneous data structure that contains uniquely named series. By
holding your data in a dataframe you will be able to use the Polars API to write queries that
manipulate your data. You will be able to do this by using the
[contexts and expressions provided by Polars](expressions-and-contexts.md) that we will talk about
next.
The snippet below shows how to create a dataframe from a dictionary of lists:
{{code_block('user-guide/concepts/data-types-and-structures','df',['DataFrame'])}}
```python exec="on" result="text" session="user-guide/data-types-and-structures"
--8<-- "python/user-guide/concepts/data-types-and-structures.py:df"
```
### Inspecting a dataframe
In this subsection we will show some useful methods to quickly inspect a dataframe. We will use the
dataframe we created earlier as a starting point.
#### Head
The function `head` shows the first rows of a dataframe. By default, you get the first 5 rows but
you can also specify the number of rows you want:
{{code_block('user-guide/concepts/data-types-and-structures','head',['head'])}}
```python exec="on" result="text" session="user-guide/data-types-and-structures"
--8<-- "python/user-guide/concepts/data-types-and-structures.py:head"
```
#### Glimpse
The function `glimpse` is another function that shows the values of the first few rows of a
dataframe, but formats the output differently from `head`. Here, each line of the output corresponds
to a single column, making it easier to take inspect wider dataframes:
=== ":fontawesome-brands-python: Python"
[:material-api: `glimpse`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.glimpse.html)
```python
--8<-- "python/user-guide/concepts/data-types-and-structures.py:glimpse"
```
```python exec="on" result="text" session="user-guide/data-types-and-structures"
--8<-- "python/user-guide/concepts/data-types-and-structures.py:glimpse"
```
!!! info
`glimpse` is only available for Python users.
#### Tail
The function `tail` shows the last rows of a dataframe. By default, you get the last 5 rows but you
can also specify the number of rows you want, similar to how `head` works:
{{code_block('user-guide/concepts/data-types-and-structures','tail',['tail'])}}
```python exec="on" result="text" session="user-guide/data-types-and-structures"
--8<-- "python/user-guide/concepts/data-types-and-structures.py:tail"
```
#### Sample
If you think the first or last rows of your dataframe are not representative of your data, you can
use `sample` to get an arbitrary number of randomly selected rows from the DataFrame. Note that the
rows are not necessarily returned in the same order as they appear in the dataframe:
{{code_block('user-guide/concepts/data-types-and-structures','sample',['sample'])}}
```python exec="on" result="text" session="user-guide/data-types-and-structures"
--8<-- "python/user-guide/concepts/data-types-and-structures.py:sample"
```
#### Describe
You can also use `describe` to compute summary statistics for all columns of your dataframe:
{{code_block('user-guide/concepts/data-types-and-structures','describe',['describe'])}}
```python exec="on" result="text" session="user-guide/data-types-and-structures"
--8<-- "python/user-guide/concepts/data-types-and-structures.py:describe"
```
## Schema
When talking about data (in a dataframe or otherwise) we can refer to its schema. The schema is a
mapping of column or series names to the data types of those same columns or series.
You can check the schema of a dataframe with `schema`:
{{code_block('user-guide/concepts/data-types-and-structures','schema',[])}}
```python exec="on" result="text" session="user-guide/data-types-and-structures"
--8<-- "python/user-guide/concepts/data-types-and-structures.py:schema"
```
Much like with series, Polars will infer the schema of a dataframe when you create it but you can
override the inference system if needed.
In Python, you can specify an explicit schema by using a dictionary to map column names to data
types. You can use the value `None` if you do not wish to override inference for a given column:
```python
--8<-- "python/user-guide/concepts/data-types-and-structures.py:schema-def"
```
```python exec="on" result="text" session="user-guide/data-types-and-structures"
--8<-- "python/user-guide/concepts/data-types-and-structures.py:schema-def"
```
If you only need to override the inference of some columns, the parameter `schema_overrides` tends
to be more convenient because it lets you omit columns for which you do not want to override the
inference:
```python
--8<-- "python/user-guide/concepts/data-types-and-structures.py:schema_overrides"
```
```python exec="on" result="text" session="user-guide/data-types-and-structures"
--8<-- "python/user-guide/concepts/data-types-and-structures.py:schema_overrides"
```
## Data types internals
Polars utilizes the [Arrow Columnar Format](https://arrow.apache.org/docs/format/Columnar.html) for
its data orientation. Following this specification allows Polars to transfer data to/from other
tools that also use the Arrow specification with little to no overhead.
Polars gets most of its performance from its query engine, the optimizations it performs on your
query plans, and from the parallelization that it employs when running
[your expressions](expressions-and-contexts.md#expressions).
## Floating point numbers
Polars generally follows the IEEE 754 floating point standard for `Float32` and `Float64`, with some
exceptions:
- Any `NaN` compares equal to any other `NaN`, and greater than any non-`NaN` value.
- Operations do not guarantee any particular behavior on the sign of zero or `NaN`, nor on the
payload of `NaN` values. This is not just limited to arithmetic operations, e.g. a sort or group
by operation may canonicalize all zeroes to +0 and all `NaN`s to a positive `NaN` without payload
for efficient equality checks.
Polars always attempts to provide reasonably accurate results for floating point computations but
does not provide guarantees on the error unless mentioned otherwise. Generally speaking 100%
accurate results are infeasibly expensive to achieve (requiring much larger internal representations
than 64-bit floats), and thus some error is always to be expected.
## Appendix: full data types table
| Type(s) | Details |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `Boolean` | Boolean type that is bit packed efficiently. |
| `Int8`, `Int16`, `Int32`, `Int64` | Varying-precision signed integer types. |
| `UInt8`, `UInt16`, `UInt32`, `UInt64` | Varying-precision unsigned integer types. |
| `Float32`, `Float64` | Varying-precision signed floating point numbers. |
| `Decimal` | Decimal 128-bit type with optional precision and non-negative scale. Use this if you need fine-grained control over the precision of your floats and the operations you make on them. See [Python's `decimal.Decimal`](https://docs.python.org/3/library/decimal.html) for documentation on what a decimal data type is. |
| `String` | Variable length UTF-8 encoded string data, typically Human-readable. |
| `Binary` | Stores arbitrary, varying length raw binary data. |
| `Date` | Represents a calendar date. |
| `Time` | Represents a time of day. |
| `Datetime` | Represents a calendar date and time of day. |
| `Duration` | Represents a time duration. |
| `Array` | Arrays with a known, fixed shape per series; akin to numpy arrays. [Learn more about how arrays and lists differ and how to work with both](../expressions/lists-and-arrays.md). |
| `List` | Homogeneous 1D container with variable length. [Learn more about how arrays and lists differ and how to work with both](../expressions/lists-and-arrays.md). |
| `Object` | Wraps arbitrary Python objects. |
| `Categorical` | Efficient encoding of string data where the categories are inferred at runtime. [Learn more about how categoricals and enums differ and how to work with both](../expressions/categorical-data-and-enums.md). |
| `Enum` | Efficient ordered encoding of a set of predetermined string categories. [Learn more about how categoricals and enums differ and how to work with both](../expressions/categorical-data-and-enums.md). |
| `Struct` | Composite product type that can store multiple fields. [Learn more about the data type `Struct` in its dedicated documentation section.](../expressions/structs.md). |
| `Null` | Represents null values. |
---
polars/docs/source/user-guide/concepts/expressions-and-contexts.md
---
# Expressions and contexts
Polars has developed its own Domain Specific Language (DSL) for transforming data. The language is
very easy to use and allows for complex queries that remain human readable. Expressions and
contexts, which will be introduced here, are very important in achieving this readability while also
allowing the Polars query engine to optimize your queries to make them run as fast as possible.
## Expressions
In Polars, an _expression_ is a lazy representation of a data transformation. Expressions are
modular and flexible, which means you can use them as building blocks to build more complex
expressions. Here is an example of a Polars expression:
```python
--8<-- "python/user-guide/concepts/expressions.py:expression"
```
As you might be able to guess, this expression takes a column named â€œweightâ€� and divides its values
by the square of the values in a column â€œheightâ€�, computing a person's BMI.
The code above expresses an abstract computation that we can save in a variable, manipulate further,
or just print:
```python
--8<-- "python/user-guide/concepts/expressions.py:print-expr"
```
```python exec="on" result="text" session="user-guide/concepts/expressions-and-contexts"
--8<-- "python/user-guide/concepts/expressions.py:expression"
--8<-- "python/user-guide/concepts/expressions.py:print-expr"
```
Because expressions are lazy, no computations have taken place yet. That's what we need contexts
for.
## Contexts
Polars expressions need a _context_ in which they are executed to produce a result. Depending on the
context it is used in, the same Polars expression can produce different results. In this section, we
will learn about the four most common contexts that Polars provides[^1]:
1. `select`
2. `with_columns`
3. `filter`
4. `group_by`
We use the dataframe below to show how each of the contexts works.
{{code_block('user-guide/concepts/expressions','df',[])}}
```python exec="on" result="text" session="user-guide/concepts/expressions-and-contexts"
--8<-- "python/user-guide/concepts/expressions.py:df"
```
### `select`
The selection context `select` applies expressions over columns. The context `select` may produce
new columns that are aggregations, combinations of other columns, or literals:
{{code_block('user-guide/concepts/expressions','select-1',['select'])}}
```python exec="on" result="text" session="user-guide/concepts/expressions-and-contexts"
--8<-- "python/user-guide/concepts/expressions.py:select-1"
```
The expressions in a context `select` must produce series that are all the same length or they must
produce a scalar. Scalars will be broadcast to match the length of the remaining series. Literals,
like the number used above, are also broadcast.
Note that broadcasting can also occur within expressions. For instance, consider the expression
below:
{{code_block('user-guide/concepts/expressions','select-2',['select'])}}
```python exec="on" result="text" session="user-guide/concepts/expressions-and-contexts"
--8<-- "python/user-guide/concepts/expressions.py:select-2"
```
Both the subtraction and the division use broadcasting within the expression because the
subexpressions that compute the mean and the standard deviation evaluate to single values.
The context `select` is very flexible and powerful and allows you to evaluate arbitrary expressions
independent of, and in parallel to, each other. This is also true of the other contexts that we will
see next.
### `with_columns`
The context `with_columns` is very similar to the context `select`. The main difference between the
two is that the context `with_columns` creates a new dataframe that contains the columns from the
original dataframe and the new columns according to its input expressions, whereas the context
`select` only includes the columns selected by its input expressions:
{{code_block('user-guide/concepts/expressions','with_columns-1',['with_columns'])}}
```python exec="on" result="text" session="user-guide/concepts/expressions-and-contexts"
--8<-- "python/user-guide/concepts/expressions.py:with_columns-1"
```
Because of this difference between `select` and `with_columns`, the expressions used in a context
`with_columns` must produce series that have the same length as the original columns in the
dataframe, whereas it is enough for the expressions in the context `select` to produce series that
have the same length among them.
### `filter`
The context `filter` filters the rows of a dataframe based on one or more expressions that evaluate
to the Boolean data type.
{{code_block('user-guide/concepts/expressions','filter-1',['filter'])}}
```python exec="on" result="text" session="user-guide/concepts/expressions-and-contexts"
--8<-- "python/user-guide/concepts/expressions.py:filter-1"
```
### `group_by` and aggregations
In the context `group_by`, rows are grouped according to the unique values of the grouping
expressions. You can then apply expressions to the resulting groups, which may be of variable
lengths.
When using the context `group_by`, you can use an expression to compute the groupings dynamically:
{{code_block('user-guide/concepts/expressions','group_by-1',['group_by'])}}
```python exec="on" result="text" session="user-guide/concepts/expressions-and-contexts"
--8<-- "python/user-guide/concepts/expressions.py:group_by-1"
```
After using `group_by` we use `agg` to apply aggregating expressions to the groups. Since in the
example above we only specified the name of a column, we get the groups of that column as lists.
We can specify as many grouping expressions as we'd like and the context `group_by` will group the
rows according to the distinct values across the expressions specified. Here, we group by a
combination of decade of birth and whether the person is shorter than 1.7 metres:
{{code_block('user-guide/concepts/expressions','group_by-2',['group_by'])}}
```python exec="on" result="text" session="user-guide/concepts/expressions-and-contexts"
--8<-- "python/user-guide/concepts/expressions.py:group_by-2"
```
The resulting dataframe, after applying aggregating expressions, contains one column per each
grouping expression on the left and then as many columns as needed to represent the results of the
aggregating expressions. In turn, we can specify as many aggregating expressions as we want:
{{code_block('user-guide/concepts/expressions','group_by-3',['group_by'])}}
```python exec="on" result="text" session="user-guide/concepts/expressions-and-contexts"
--8<-- "python/user-guide/concepts/expressions.py:group_by-3"
```
See also `group_by_dynamic` and `rolling` for other grouping contexts.
## Expression expansion
The last example contained two grouping expressions and three aggregating expressions, and yet the
resulting dataframe contained six columns instead of five. If we look closely, the last aggregating
expression mentioned two different columns: â€œweightâ€� and â€œheightâ€�.
Polars expressions support a feature called _expression expansion_. Expression expansion is like a
shorthand notation for when you want to apply the same transformation to multiple columns. As we
have seen, the expression
```python
pl.col("weight", "height").mean().name.prefix("avg_")
```
will compute the mean value of the columns â€œweightâ€� and â€œheightâ€� and will rename them as
â€œavg_weightâ€� and â€œavg_heightâ€�, respectively. In fact, the expression above is equivalent to using
the two following expressions:
```python
[
pl.col("weight").mean().alias("avg_weight"),
pl.col("height").mean().alias("avg_height"),
]
```
In this case, this expression expands into two independent expressions that Polars can execute in
parallel. In other cases, we may not be able to know in advance how many independent expressions an
expression will unfold into.
Consider this simple but elucidative example:
```python
(pl.col(pl.Float64) * 1.1).name.suffix("*1.1")
```
This expression will multiply all columns with data type `Float64` by `1.1`. The number of columns
this applies to depends on the schema of each dataframe. In the case of the dataframe we have been
using, it applies to two columns:
{{code_block('user-guide/concepts/expressions','expression-expansion-1',['group_by'])}}
```python exec="on" result="text" session="user-guide/concepts/expressions-and-contexts"
--8<-- "python/user-guide/concepts/expressions.py:expression-expansion-1"
```
In the case of the dataframe `df2` below, the same expression expands to 0 columns because no column
has the data type `Float64`:
{{code_block('user-guide/concepts/expressions','expression-expansion-2',['group_by'])}}
```python exec="on" result="text" session="user-guide/concepts/expressions-and-contexts"
--8<-- "python/user-guide/concepts/expressions.py:expression-expansion-2"
```
It is equally easy to imagine a scenario where the same expression would expand to dozens of
columns.
Next, you will learn about
[the lazy API and the function `explain`](lazy-api.md#previewing-the-query-plan), which you can use
to preview what an expression will expand to given a schema.
## Conclusion
Because expressions are lazy, when you use an expression inside a context Polars can try to simplify
your expression before running the data transformation it expresses. Separate expressions within a
context are embarrassingly parallel and Polars will take advantage of that, while also parallelizing
expression execution when using expression expansion. Further performance gains can be obtained when
using [the lazy API of Polars](lazy-api.md), which is introduced next.
We have only scratched the surface of the capabilities of expressions. There are a ton more
expressions and they can be combined in a variety of ways. See the
[section on expressions](../expressions/index.md) for a deeper dive on the different types of
expressions available.
[^1]: There are additional List and SQL contexts which are covered later in this guide. But for
simplicity, we leave them out of scope for now.
---
polars/docs/source/user-guide/concepts/index.md
---
# Concepts
This chapter describes the core concepts of the Polars API. Understanding these will help you
optimise your queries on a daily basis. We will cover the following topics:
- [Data types and structures](data-types-and-structures.md)
- [Expressions and contexts](expressions-and-contexts.md)
- [Lazy API](lazy-api.md)
---
polars/docs/source/user-guide/concepts/lazy-api.md
---
# Lazy API
Polars supports two modes of operation: lazy and eager. The examples so far have used the eager API,
in which the query is executed immediately. In the lazy API, the query is only evaluated once it is
_collected_. Deferring the execution to the last minute can have significant performance advantages
and is why the lazy API is preferred in most cases. Let us demonstrate this with an example:
{{code_block('user-guide/concepts/lazy-vs-eager','eager',['read_csv'])}}
In this example we use the eager API to:
1. Read the iris [dataset](https://archive.ics.uci.edu/dataset/53/iris).
1. Filter the dataset based on sepal length.
1. Calculate the mean of the sepal width per species.
Every step is executed immediately returning the intermediate results. This can be very wasteful as
we might do work or load extra data that is not being used. If we instead used the lazy API and
waited on execution until all the steps are defined then the query planner could perform various
optimizations. In this case:
- Predicate pushdown: Apply filters as early as possible while reading the dataset, thus only
reading rows with sepal length greater than 5.
- Projection pushdown: Select only the columns that are needed while reading the dataset, thus
removing the need to load additional columns (e.g., petal length and petal width).
{{code_block('user-guide/concepts/lazy-vs-eager','lazy',['scan_csv'])}}
These will significantly lower the load on memory & CPU thus allowing you to fit bigger datasets in
memory and process them faster. Once the query is defined you call `collect` to inform Polars that
you want to execute it. You can
[learn more about the lazy API in its dedicated chapter](../lazy/index.md).
!!! info "Eager API"
In many cases the eager API is actually calling the lazy API under the hood and immediately collecting the result. This has the benefit that within the query itself optimization(s) made by the query planner can still take place.
## When to use which
In general, the lazy API should be preferred unless you are either interested in the intermediate
results or are doing exploratory work and don't know yet what your query is going to look like.
## Previewing the query plan
When using the lazy API you can use the function `explain` to ask Polars to create a description of
the query plan that will be executed once you collect the results. This can be useful if you want to
see what types of optimizations Polars performs on your queries. We can ask Polars to explain the
query `q` we defined above:
{{code_block('user-guide/concepts/lazy-vs-eager','explain',['explain'])}}
```python exec="on" result="text" session="user-guide/concepts/lazy-api"
--8<-- "python/user-guide/concepts/lazy-vs-eager.py:import"
--8<-- "python/user-guide/concepts/lazy-vs-eager.py:lazy"
--8<-- "python/user-guide/concepts/lazy-vs-eager.py:explain"
```
Immediately, we can see in the explanation that Polars did apply predicate pushdown, as it is only
reading rows where the sepal length is greater than 5, and it did apply projection pushdown, as it
is only reading the columns that are needed by the query.
The function `explain` can also be used to see how expression expansion will unfold in the context
of a given schema. Consider the example expression from the
[section on expression expansion](expressions-and-contexts.md#expression-expansion):
```python
(pl.col(pl.Float64) * 1.1).name.suffix("*1.1")
```
We can use `explain` to see how this expression would evaluate against an arbitrary schema:
=== ":fontawesome-brands-python: Python"
[:material-api: `explain`](https://docs.pola.rs/api/python/stable/reference/lazyframe/api/polars.LazyFrame.explain.html)
```python
--8<-- "python/user-guide/concepts/lazy-vs-eager.py:explain-expression-expansion"
```
```python exec="on" result="text" session="user-guide/concepts/lazy-api"
--8<-- "python/user-guide/concepts/lazy-vs-eager.py:explain-expression-expansion"
```
---
polars/docs/source/user-guide/transformations/concatenation.md
---
# Concatenation
There are a number of ways to concatenate data from separate DataFrames:
- two dataframes with **the same columns** can be **vertically** concatenated to make a **longer**
dataframe
- two dataframes with **non-overlapping columns** can be **horizontally** concatenated to make a
**wider** dataframe
- two dataframes with **different numbers of rows and columns** can be **diagonally** concatenated
to make a dataframe which might be longer and/ or wider. Where column names overlap values will be
vertically concatenated. Where column names do not overlap new rows and columns will be added.
Missing values will be set as `null`
## Vertical concatenation - getting longer
In a vertical concatenation you combine all of the rows from a list of `DataFrames` into a single
longer `DataFrame`.
{{code_block('user-guide/transformations/concatenation','vertical',['concat'])}}
```python exec="on" result="text" session="user-guide/transformations/concatenation"
--8<-- "python/user-guide/transformations/concatenation.py:setup"
--8<-- "python/user-guide/transformations/concatenation.py:vertical"
```
Vertical concatenation fails when the dataframes do not have the same column names.
## Horizontal concatenation - getting wider
In a horizontal concatenation you combine all of the columns from a list of `DataFrames` into a
single wider `DataFrame`.
{{code_block('user-guide/transformations/concatenation','horizontal',['concat'])}}
```python exec="on" result="text" session="user-guide/transformations/concatenation"
--8<-- "python/user-guide/transformations/concatenation.py:horizontal"
```
Horizontal concatenation fails when dataframes have overlapping columns.
When dataframes have different numbers of rows, columns will be padded with `null` values at the end
up to the maximum length.
{{code_block('user-guide/transformations/concatenation','horizontal_different_lengths',['concat'])}}
```python exec="on" result="text" session="user-guide/transformations/concatenation"
--8<-- "python/user-guide/transformations/concatenation.py:horizontal_different_lengths"
```
## Diagonal concatenation - getting longer, wider and `null`ier
In a diagonal concatenation you combine all of the row and columns from a list of `DataFrames` into
a single longer and/or wider `DataFrame`.
{{code_block('user-guide/transformations/concatenation','cross',['concat'])}}
```python exec="on" result="text" session="user-guide/transformations/concatenation"
--8<-- "python/user-guide/transformations/concatenation.py:cross"
```
Diagonal concatenation generates nulls when the column names do not overlap.
When the dataframe shapes do not match and we have an overlapping semantic key then
[we can join the dataframes](joins.md) instead of concatenating them.
## Rechunking
Before a concatenation we have two dataframes `df1` and `df2`. Each column in `df1` and `df2` is in
one or more chunks in memory. By default, during concatenation the chunks in each column are not
made contiguous. This makes the concat operation faster and consume less memory but it may slow down
future operations that would benefit from having the data be in contiguous memory. The process of
copying the fragmented chunks into a single new chunk is known as **rechunking**. Rechunking is an
expensive operation. Prior to version 0.20.26, the default was to perform a rechunk but in new
versions, the default is not to. If you do want Polars to rechunk the concatenated `DataFrame` you
specify `rechunk = True` when doing the concatenation.
---
polars/docs/source/user-guide/transformations/index.md
---
# Transformations
The focus of this section is to describe different types of data transformations and provide some
examples on how to use them.
- [Joins](joins.md)
- [Concatenation](concatenation.md)
- [Pivot](pivot.md)
- [Unpivot](unpivot.md)
---
polars/docs/source/user-guide/transformations/joins.md
---
# Joins
A join operation combines columns from one or more dataframes into a new dataframe. The different
â€œjoining strategiesâ€� and matching criteria used by the different types of joins influence how
columns are combined and also what rows are included in the result of the join operation.
The most common type of join is an â€œequi joinâ€�, in which rows are matched by a key expression.
Polars supports several joining strategies for equi joins, which determine exactly how we handle the
matching of rows. Polars also supports â€œnon-equi joinsâ€�, a type of join where the matching criterion
is not an equality, and a type of join where rows are matched by key proximity, called â€œasof joinâ€�.
## Quick reference table
The table below acts as a quick reference for people who know what they are looking for. If you want
to learn about joins in general and how to work with them in Polars, feel free to skip the table and
keep reading below.
=== ":fontawesome-brands-python: Python"
[:material-api: `join`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.join.html)
[:material-api: `join_where`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.join_where.html)
[:material-api: `join_asof`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.join_asof.html)
=== ":fontawesome-brands-rust: Rust"
[:material-api: `join`](https://docs.pola.rs/api/rust/dev/polars/prelude/trait.DataFrameJoinOps.html#method.join)
([:material-flag-plus: semi_anti_join](/user-guide/installation/#feature-flags "Enable the feature flag semi_anti_join for semi and for anti joins"){.feature-flag} needed for some options.)
[:material-api: `join_asof_by`](https://docs.pola.rs/api/rust/dev/polars/prelude/trait.AsofJoinBy.html#method.join_asof_by)
[:material-flag-plus: Available on feature asof_join](/user-guide/installation/#feature-flags "To use this functionality enable the feature flag asof_join"){.feature-flag}
[:material-api: `join_where`](https://docs.rs/polars/latest/polars/prelude/struct.JoinBuilder.html#method.join_where)
[:material-flag-plus: Available on feature iejoin](/user-guide/installation/#feature-flags "To use this functionality enable the feature flag iejoin"){.feature-flag}
| Type | Function | Brief description |
| --------------------- | -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Equi inner join | `join(..., how="inner")` | Keeps rows that matched both on the left and right. |
| Equi left outer join | `join(..., how="left")` | Keeps all rows from the left plus matching rows from the right. Non-matching rows from the left have their right columns filled with `null`. |
| Equi right outer join | `join(..., how="right")` | Keeps all rows from the right plus matching rows from the left. Non-matching rows from the right have their left columns filled with `null`. |
| Equi full join | `join(..., how="full")` | Keeps all rows from either dataframe, regardless of whether they match or not. Non-matching rows from one side have the columns from the other side filled with `null`. |
| Equi semi join | `join(..., how="semi")` | Keeps rows from the left that have a match on the right. |
| Equi anti join | `join(..., how="anti")` | Keeps rows from the left that do not have a match on the right. |
| Non-equi inner join | `join_where` | Finds all possible pairings of rows from the left and right that satisfy the given predicate(s). |
| Asof join | `join_asof`/`join_asof_by` | Like a left outer join, but matches on the nearest key instead of on exact key matches. |
| Cartesian product | `join(..., how="cross")` | Computes the [Cartesian product](https://en.wikipedia.org/wiki/Cartesian_product) of the two dataframes. |
## Equi joins
In an equi join, rows are matched by checking equality of a key expression. You can do an equi join
with the function `join` by specifying the name of the column to be used as key. For the examples,
we will be loading some (modified) Monopoly property data.
First, we load a dataframe that contains property names and their colour group in the game:
{{code_block('user-guide/transformations/joins','props_groups',[])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:prep-data"
--8<-- "python/user-guide/transformations/joins.py:props_groups"
```
Next, we load a dataframe that contains property names and their price in the game:
{{code_block('user-guide/transformations/joins','props_prices',[])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:props_prices"
```
Now, we join both dataframes to create a dataframe that contains property names, colour groups, and
prices:
{{code_block('user-guide/transformations/joins','equi-join',['join'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:equi-join"
```
The result has four rows but both dataframes used in the operation had five rows. Polars uses a
joining strategy to determine what happens with rows that have multiple matches or with rows that
have no match at all. By default, Polars computes an â€œinner joinâ€� but there are
[other join strategies that we show next](#join-strategies).
In the example above, the two dataframes conveniently had the column we wish to use as key with the
same name and with the values in the exact same format. Suppose, for the sake of argument, that one
of the dataframes had a differently named column and the other had the property names in lower case:
{{code_block('user-guide/transformations/joins','props_groups2',['Expr.str'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:props_groups2"
```
{{code_block('user-guide/transformations/joins','props_prices2',[])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:props_prices2"
```
In a situation like this, where we may want to perform the same join as before, we can leverage
`join`'s flexibility and specify arbitrary expressions to compute the joining key on the left and on
the right, allowing one to compute row keys dynamically:
{{code_block('user-guide/transformations/joins', 'join-key-expression', ['join', 'Expr.str'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:join-key-expression"
```
Because we are joining on the right with an expression, Polars preserves the column â€œproperty_nameâ€�
from the left and the column â€œnameâ€� from the right so we can have access to the original values that
the key expressions were applied to.
## Join strategies
When computing a join with `df1.join(df2, ...)`, we can specify one of many different join
strategies. A join strategy specifies what rows to keep from each dataframe based on whether they
match rows from the other dataframe.
### Inner join
In an inner join the resulting dataframe only contains the rows from the left and right dataframes
that matched. That is the default strategy used by `join` and above we can see an example of that.
We repeat the example here and explicitly specify the join strategy:
{{code_block('user-guide/transformations/joins','inner-join',['join'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:inner-join"
```
The result does not include the row from `props_groups` that contains â€œThe Shireâ€� and the result
also does not include the row from `props_prices` that contains â€œSesame Streetâ€�.
### Left join
A left outer join is a join where the result contains all the rows from the left dataframe and the
rows of the right dataframe that matched any rows from the left dataframe.
{{code_block('user-guide/transformations/joins','left-join',['join'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:left-join"
```
If there are any rows from the left dataframe that have no matching rows on the right dataframe,
they get the value `null` on the new columns.
### Right join
Computationally speaking, a right outer join is exactly the same as a left outer join, but with the
arguments swapped. Here is an example:
{{code_block('user-guide/transformations/joins','right-join',['join'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:right-join"
```
We show that `df1.join(df2, how="right", ...)` is the same as `df2.join(df1, how="left", ...)`, up
to the order of the columns of the result, with the computation below:
{{code_block('user-guide/transformations/joins','left-right-join-equals',['join'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:left-right-join-equals"
```
### Full join
A full outer join will keep all of the rows from the left and right dataframes, even if they don't
have matching rows in the other dataframe:
{{code_block('user-guide/transformations/joins','full-join',['join'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:full-join"
```
In this case, we see that we get two columns `property_name` and `property_name_right` to make up
for the fact that we are matching on the column `property_name` of both dataframes and there are
some names for which there are no matches. The two columns help differentiate the source of each row
data. If we wanted to force `join` to coalesce the two columns `property_name` into a single column,
we could set `coalesce=True` explicitly:
{{code_block('user-guide/transformations/joins','full-join-coalesce',['join'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:full-join-coalesce"
```
When not set, the parameter `coalesce` is determined automatically from the join strategy and the
key(s) specified, which is why the inner, left, and right, joins acted as if `coalesce=True`, even
though we didn't set it.
### Semi join
A semi join will return the rows of the left dataframe that have a match in the right dataframe, but
we do not actually join the matching rows:
{{code_block('user-guide/transformations/joins', 'semi-join', [], ['join'],
['join-semi_anti_join_flag'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:semi-join"
```
A semi join acts as a sort of row filter based on a second dataframe.
### Anti join
Conversely, an anti join will return the rows of the left dataframe that do not have a match in the
right dataframe:
{{code_block('user-guide/transformations/joins', 'anti-join', [], ['join'],
['join-semi_anti_join_flag'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:anti-join"
```
## Non-equi joins
In a non-equi join matches between the left and right dataframes are computed differently. Instead
of looking for matches on key expressions, we provide a single predicate that determines what rows
of the left dataframe can be paired up with what rows of the right dataframe.
For example, consider the following Monopoly players and their current cash:
{{code_block('user-guide/transformations/joins','players',[])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:players"
```
Using a non-equi join we can easily build a dataframe with all the possible properties that each
player could be interested in buying. We use the function `join_where` to compute a non-equi join:
{{code_block('user-guide/transformations/joins','non-equi',['join_where'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:non-equi"
```
You can provide multiple expressions as predicates but they all must use comparison operators that
evaluate to a Boolean result and must refer to columns from both dataframes.
!!! note
`join_where` is still experimental and doesn't yet support arbitrary Boolean expressions as predicates.
## Asof join
An `asof` join is like a left join except that we match on nearest key rather than equal keys. In
Polars we can do an asof join with the `join_asof` method.
For the asof join we will consider a scenario inspired by the stock market. Suppose a stock market
broker has a dataframe called `df_trades` showing transactions it has made for different stocks.
{{code_block('user-guide/transformations/joins','df_trades',[])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:df_trades"
```
The broker has another dataframe called `df_quotes` showing prices it has quoted for these stocks:
{{code_block('user-guide/transformations/joins','df_quotes',[])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:df_quotes"
```
You want to produce a dataframe showing for each trade the most recent quote provided _on or before_
the time of the trade. You do this with `join_asof` (using the default `strategy = "backward"`). To
avoid joining between trades on one stock with a quote on another you must specify an exact
preliminary join on the stock column with `by="stock"`.
{{code_block('user-guide/transformations/joins','asof', [], ['join_asof'], ['join_asof_by'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:asof"
```
If you want to make sure that only quotes within a certain time range are joined to the trades you
can specify the `tolerance` argument. In this case we want to make sure that the last preceding
quote is within 1 minute of the trade so we set `tolerance = "1m"`.
{{code_block('user-guide/transformations/joins','asof-tolerance', [], ['join_asof'],
['join_asof_by'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:asof-tolerance"
```
## Cartesian product
Polars allows you to compute the
[Cartesian product](https://en.wikipedia.org/wiki/Cartesian_product) of two dataframes, producing a
dataframe where all rows of the left dataframe are paired up with all the rows of the right
dataframe. To compute the Cartesian product of two dataframes, you can pass the strategy
`how="cross"` to the function `join` without specifying any of `on`, `left_on`, and `right_on`:
{{code_block('user-guide/transformations/joins','cartesian-product',[],['join'],['cross_join'])}}
```python exec="on" result="text" session="transformations/joins"
--8<-- "python/user-guide/transformations/joins.py:cartesian-product"
```
---
polars/docs/source/user-guide/transformations/pivot.md
---
# Pivots
Pivot a column in a `DataFrame` and perform one of the following aggregations:
- first
- last
- sum
- min
- max
- mean
- median
- len
The pivot operation consists of a group by one, or multiple columns (these will be the new y-axis),
the column that will be pivoted (this will be the new x-axis) and an aggregation.
## Dataset
{{code_block('user-guide/transformations/pivot','df',['DataFrame'])}}
```python exec="on" result="text" session="user-guide/transformations/pivot"
--8<-- "python/user-guide/transformations/pivot.py:setup"
--8<-- "python/user-guide/transformations/pivot.py:df"
```
## Eager
{{code_block('user-guide/transformations/pivot','eager',['pivot'])}}
```python exec="on" result="text" session="user-guide/transformations/pivot"
--8<-- "python/user-guide/transformations/pivot.py:eager"
```
## Lazy
A Polars `LazyFrame` always need to know the schema of a computation statically (before collecting
the query). As a pivot's output schema depends on the data, and it is therefore impossible to
determine the schema without running the query.
Polars could have abstracted this fact for you just like Spark does, but we don't want you to shoot
yourself in the foot with a shotgun. The cost should be clear upfront.
{{code_block('user-guide/transformations/pivot','lazy',['pivot'])}}
```python exec="on" result="text" session="user-guide/transformations/pivot"
--8<-- "python/user-guide/transformations/pivot.py:lazy"
```
---
polars/docs/source/user-guide/transformations/unpivot.md
---
# Unpivots
Unpivot unpivots a DataFrame from wide format to long format
## Dataset
{{code_block('user-guide/transformations/unpivot','df',['DataFrame'])}}
```python exec="on" result="text" session="user-guide/transformations/unpivot"
--8<-- "python/user-guide/transformations/unpivot.py:df"
```
## Eager + lazy
`Eager` and `lazy` have the same API.
{{code_block('user-guide/transformations/unpivot','unpivot',['unpivot'])}}
```python exec="on" result="text" session="user-guide/transformations/unpivot"
--8<-- "python/user-guide/transformations/unpivot.py:unpivot"
```
---
polars/docs/source/user-guide/transformations/time-series/filter.md
---
# Filtering
Filtering date columns works in the same way as with other types of columns using the `.filter`
method.
Polars uses Python's native `datetime`, `date` and `timedelta` for equality comparisons between the
datatypes `pl.Datetime`, `pl.Date` and `pl.Duration`.
In the following example we use a time series of Apple stock prices.
{{code_block('user-guide/transformations/time-series/filter','df',['read_csv'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/filter"
--8<-- "python/user-guide/transformations/time-series/filter.py:df"
```
## Filtering by single dates
We can filter by a single date using an equality comparison in a filter expression:
{{code_block('user-guide/transformations/time-series/filter','filter',['filter'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/filter"
--8<-- "python/user-guide/transformations/time-series/filter.py:filter"
```
Note we are using the lowercase `datetime` method rather than the uppercase `Datetime` data type.
## Filtering by a date range
We can filter by a range of dates using the `is_between` method in a filter expression with the
start and end dates:
{{code_block('user-guide/transformations/time-series/filter','range',['filter','is_between'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/filter"
--8<-- "python/user-guide/transformations/time-series/filter.py:range"
```
## Filtering with negative dates
Say you are working with an archeologist and are dealing in negative dates. Polars can parse and
store them just fine, but the Python `datetime` library does not. So for filtering, you should use
attributes in the `.dt` namespace:
{{code_block('user-guide/transformations/time-series/filter','negative',['str.to_date'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/filter"
--8<-- "python/user-guide/transformations/time-series/filter.py:negative"
```
---
polars/docs/source/user-guide/transformations/time-series/parsing.md
---
# Parsing
Polars has native support for parsing time series data and doing more sophisticated operations such
as temporal grouping and resampling.
## Datatypes
Polars has the following datetime datatypes:
- `Date`: Date representation e.g. 2014-07-08. It is internally represented as days since UNIX epoch
encoded by a 32-bit signed integer.
- `Datetime`: Datetime representation e.g. 2014-07-08 07:00:00. It is internally represented as a 64
bit integer since the Unix epoch and can have different units such as ns, us, ms.
- `Duration`: A time delta type that is created when subtracting `Date/Datetime`. Similar to
`timedelta` in Python.
- `Time`: Time representation, internally represented as nanoseconds since midnight.
## Parsing dates from a file
When loading from a CSV file Polars attempts to parse dates and times if the `try_parse_dates` flag
is set to `True`:
{{code_block('user-guide/transformations/time-series/parsing','df',['read_csv'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/parsing"
--8<-- "python/user-guide/transformations/time-series/parsing.py:setup"
--8<-- "python/user-guide/transformations/time-series/parsing.py:df"
```
This flag will trigger schema inference on a number of rows, as configured by the
`infer_schema_length` setting (100 rows by default). Schema inference is computationally expensive
and can slow down file loading if a high number of rows is used.
On the other hand binary formats such as parquet have a schema that is respected by Polars.
## Casting strings to dates
You can also cast a column of datetimes encoded as strings to a datetime type. You do this by
calling the string `str.to_date` method and passing the format of the date string:
{{code_block('user-guide/transformations/time-series/parsing','cast',['read_csv','str.to_date'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/parsing"
--8<-- "python/user-guide/transformations/time-series/parsing.py:cast"
```
[The format string specification can be found here.](https://docs.rs/chrono/latest/chrono/format/strftime/index.html).
## Extracting date features from a date column
You can extract data features such as the year or day from a date column using the `.dt` namespace:
{{code_block('user-guide/transformations/time-series/parsing','extract',['dt.year'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/parsing"
--8<-- "python/user-guide/transformations/time-series/parsing.py:extract"
```
## Mixed offsets
If you have mixed offsets (say, due to crossing daylight saving time), then you can use `utc=True`
and then convert to your time zone:
{{code_block('user-guide/transformations/time-series/parsing','mixed',['str.to_datetime','dt.convert_time_zone'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/parsing"
--8<-- "python/user-guide/transformations/time-series/parsing.py:mixed"
```
---
polars/docs/source/user-guide/transformations/time-series/resampling.md
---
# Resampling
We can resample by either:
- upsampling (moving data to a higher frequency)
- downsampling (moving data to a lower frequency)
- combinations of these e.g. first upsample and then downsample
## Downsampling to a lower frequency
Polars views downsampling as a special case of the **group_by** operation and you can do this with
`group_by_dynamic` and `group_by_rolling` -
[see the temporal group by page for examples](rolling.md).
## Upsampling to a higher frequency
Let's go through an example where we generate data at 30 minute intervals:
{{code_block('user-guide/transformations/time-series/resampling','df',['DataFrame','date_range'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/resampling"
--8<-- "python/user-guide/transformations/time-series/resampling.py:setup"
--8<-- "python/user-guide/transformations/time-series/resampling.py:df"
```
Upsampling can be done by defining the new sampling interval. By upsampling we are adding in extra
rows where we do not have data. As such upsampling by itself gives a DataFrame with nulls. These
nulls can then be filled with a fill strategy or interpolation.
### Upsampling strategies
In this example we upsample from the original 30 minutes to 15 minutes and then use a `forward`
strategy to replace the nulls with the previous non-null value:
{{code_block('user-guide/transformations/time-series/resampling','upsample',['upsample'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/resampling"
--8<-- "python/user-guide/transformations/time-series/resampling.py:upsample"
```
In this example we instead fill the nulls by linear interpolation:
{{code_block('user-guide/transformations/time-series/resampling','upsample2',['upsample','interpolate','fill_null'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/resampling"
--8<-- "python/user-guide/transformations/time-series/resampling.py:upsample2"
```
---
polars/docs/source/user-guide/transformations/time-series/rolling.md
---
# Grouping
## Grouping by fixed windows
We can calculate temporal statistics using `group_by_dynamic` to group rows into days/months/years
etc.
### Annual average example
In following simple example we calculate the annual average closing price of Apple stock prices. We
first load the data from CSV:
{{code_block('user-guide/transformations/time-series/rolling','df',['upsample'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/rolling"
--8<-- "python/user-guide/transformations/time-series/rolling.py:setup"
--8<-- "python/user-guide/transformations/time-series/rolling.py:df"
```
!!! info
The dates are sorted in ascending order - if they are not sorted in this way the `group_by_dynamic` output will not be correct!
To get the annual average closing price we tell `group_by_dynamic` that we want to:
- group by the `Date` column on an annual (`1y`) basis
- take the mean values of the `Close` column for each year:
{{code_block('user-guide/transformations/time-series/rolling','group_by',['group_by_dynamic'])}}
The annual average closing price is then:
```python exec="on" result="text" session="user-guide/transformations/ts/rolling"
--8<-- "python/user-guide/transformations/time-series/rolling.py:group_by"
```
### Parameters for `group_by_dynamic`
A dynamic window is defined by a:
- **every**: indicates the interval of the window
- **period**: indicates the duration of the window
- **offset**: can be used to offset the start of the windows
The value for `every` sets how often the groups start. The time period values are flexible - for
example we could take:
- the average over 2 year intervals by replacing `1y` with `2y`
- the average over 18 month periods by replacing `1y` with `1y6mo`
We can also use the `period` parameter to set how long the time period for each group is. For
example, if we set the `every` parameter to be `1y` and the `period` parameter to be `2y` then we
would get groups at one year intervals where each groups spanned two years.
If the `period` parameter is not specified then it is set equal to the `every` parameter so that if
the `every` parameter is set to be `1y` then each group spans `1y` as well.
Because _**every**_ does not have to be equal to _**period**_, we can create many groups in a very
flexible way. They may overlap or leave boundaries between them.
Let's see how the windows for some parameter combinations would look. Let's start out boring. ðŸ¥±
- every: 1 day -> `"1d"`
- period: 1 day -> `"1d"`
```text
this creates adjacent windows of the same size
|--|
|--|
|--|
```
- every: 1 day -> `"1d"`
- period: 2 days -> `"2d"`
```text
these windows have an overlap of 1 day
|----|
|----|
|----|
```
- every: 2 days -> `"2d"`
- period: 1 day -> `"1d"`
```text
this would leave gaps between the windows
data points that in these gaps will not be a member of any group
|--|
|--|
|--|
```
#### `truncate`
The `truncate` parameter is a Boolean variable that determines what datetime value is associated
with each group in the output. In the example above the first data point is on 23rd February 1981.
If `truncate = True` (the default) then the date for the first year in the annual average is 1st
January 1981. However, if `truncate = False` then the date for the first year in the annual average
is the date of the first data point on 23rd February 1981. Note that `truncate` only affects what's
shown in the `Date` column and does not affect the window boundaries.
### Using expressions in `group_by_dynamic`
We aren't restricted to using simple aggregations like `mean` in a group by operation - we can use
the full range of expressions available in Polars.
In the snippet below we create a `date range` with every **day** (`"1d"`) in 2021 and turn this into
a `DataFrame`.
Then in the `group_by_dynamic` we create dynamic windows that start every **month** (`"1mo"`) and
have a window length of `1` month. The values that match these dynamic windows are then assigned to
that group and can be aggregated with the powerful expression API.
Below we show an example where we use **group_by_dynamic** to compute:
- the number of days until the end of the month
- the number of days in a month
{{code_block('user-guide/transformations/time-series/rolling','group_by_dyn',['group_by_dynamic','DataFrame.explode','date_range'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/rolling"
--8<-- "python/user-guide/transformations/time-series/rolling.py:group_by_dyn"
```
## Grouping by rolling windows
The rolling operation, `rolling`, is another entrance to the `group_by`/`agg` context. But different
from the `group_by_dynamic` where the windows are fixed by a parameter `every` and `period`. In a
`rolling`, the windows are not fixed at all! They are determined by the values in the
`index_column`.
So imagine having a time column with the values `{2021-01-06, 2021-01-10}` and a `period="5d"` this
would create the following windows:
```text
2021-01-01 2021-01-06
|----------|
2021-01-05 2021-01-10
|----------|
```
Because the windows of a rolling group by are always determined by the values in the `DataFrame`
column, the number of groups is always equal to the original `DataFrame`.
## Combining group by operations
Rolling and dynamic group by operations can be combined with normal group by operations.
Below is an example with a dynamic group by.
{{code_block('user-guide/transformations/time-series/rolling','group_by_roll',['DataFrame'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/rolling"
--8<-- "python/user-guide/transformations/time-series/rolling.py:group_by_roll"
```
{{code_block('user-guide/transformations/time-series/rolling','group_by_dyn2',['group_by_dynamic'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/rolling"
--8<-- "python/user-guide/transformations/time-series/rolling.py:group_by_dyn2"
```
---
polars/docs/source/user-guide/transformations/time-series/timezones.md
---
---
hide:
- toc
---
# Time zones
!!! quote "Tom Scott"
You really should never, ever deal with time zones if you can help it.
The `Datetime` datatype can have a time zone associated with it. Examples of valid time zones are:
- `None`: no time zone, also known as "time zone naive".
- `UTC`: Coordinated Universal Time.
- `Asia/Kathmandu`: time zone in "area/location" format. See the
[list of tz database time zones](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones) to
see what's available.
Caution: Fixed offsets such as +02:00, should not be used for handling time zones. It's advised to
use the "Area/Location" format mentioned above, as it can manage timezones more effectively.
Note that, because a `Datetime` can only have a single time zone, it is impossible to have a column
with multiple time zones. If you are parsing data with multiple offsets, you may want to pass
`utc=True` to convert them all to a common time zone (`UTC`), see
[parsing dates and times](parsing.md).
The main methods for setting and converting between time zones are:
- `dt.convert_time_zone`: convert from one time zone to another.
- `dt.replace_time_zone`: set/unset/change time zone.
Let's look at some examples of common operations:
{{code_block('user-guide/transformations/time-series/timezones','example',['str.to_datetime','dt.replace_time_zone'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/timezones"
--8<-- "python/user-guide/transformations/time-series/timezones.py:setup"
--8<-- "python/user-guide/transformations/time-series/timezones.py:example"
```
{{code_block('user-guide/transformations/time-series/timezones','example2',['dt.convert_time_zone','dt.replace_time_zone'])}}
```python exec="on" result="text" session="user-guide/transformations/ts/timezones"
--8<-- "python/user-guide/transformations/time-series/timezones.py:example2"
```
---
polars/docs/source/user-guide/sql/create.md
---
# CREATE
In Polars, the `SQLContext` provides a way to execute SQL statements against `LazyFrames` and
`DataFrames` using SQL syntax. One of the SQL statements that can be executed using `SQLContext` is
the `CREATE TABLE` statement, which is used to create a new table.
The syntax for the `CREATE TABLE` statement in Polars is as follows:
```
CREATE TABLE table_name
AS
SELECT ...
```
In this syntax, `table_name` is the name of the new table that will be created, and `SELECT ...` is
a SELECT statement that defines the data that will be inserted into the table.
Here's an example of how to use the `CREATE TABLE` statement in Polars:
{{code_block('user-guide/sql/create','create',['SQLregister','SQLexecute'])}}
```python exec="on" result="text" session="user-guide/sql"
--8<-- "python/user-guide/sql/create.py:setup"
--8<-- "python/user-guide/sql/create.py:create"
```
In this example, we use the `execute()` method of the `SQLContext` to execute a `CREATE TABLE`
statement that creates a new table called `older_people` based on a SELECT statement that selects
all rows from the `my_table` DataFrame where the `age` column is greater than 30.
!!! note Result
Note that the result of a `CREATE TABLE` statement is not the table itself. The table is registered in the `SQLContext`. In case you want to turn the table back to a `DataFrame` you can use a `SELECT * FROM ...` statement
---
polars/docs/source/user-guide/sql/cte.md
---
# Common Table Expressions
Common Table Expressions (CTEs) are a feature of SQL that allow you to define a temporary named
result set that can be referenced within a SQL statement. CTEs provide a way to break down complex
SQL queries into smaller, more manageable pieces, making them easier to read, write, and maintain.
A CTE is defined using the `WITH` keyword followed by a comma-separated list of subqueries, each of
which defines a named result set that can be used in subsequent queries. The syntax for a CTE is as
follows:
```
WITH cte_name AS (
subquery
)
SELECT ...
```
In this syntax, `cte_name` is the name of the CTE, and `subquery` is the subquery that defines the
result set. The CTE can then be referenced in subsequent queries as if it were a table or view.
CTEs are particularly useful when working with complex queries that involve multiple levels of
subqueries, as they allow you to break down the query into smaller, more manageable pieces that are
easier to understand and debug. Additionally, CTEs can help improve query performance by allowing
the database to optimize and cache the results of subqueries, reducing the number of times they need
to be executed.
Polars supports Common Table Expressions (CTEs) using the WITH clause in SQL syntax. Below is an
example
{{code_block('user-guide/sql/cte','cte',['SQLregister','SQLexecute'])}}
```python exec="on" result="text" session="user-guide/sql/cte"
--8<-- "python/user-guide/sql/cte.py:setup"
--8<-- "python/user-guide/sql/cte.py:cte"
```
In this example, we use the `execute()` method of the `SQLContext` to execute a SQL query that
includes a CTE. The CTE selects all rows from the `my_table` LazyFrame where the `age` column is
greater than 30 and gives it the alias `older_people`. We then execute a second SQL query that
selects all rows from the `older_people` CTE where the `name` column starts with the letter 'C'.
---
polars/docs/source/user-guide/sql/intro.md
---
# Introduction
While Polars supports interaction with SQL, it's recommended that users familiarize themselves with
the [expression syntax](../concepts/expressions-and-contexts.md#expressions) to produce more
readable and expressive code. As the DataFrame interface is primary, new features are typically
added to the expression API first. However, if you already have an existing SQL codebase or prefer
the use of SQL, Polars does offers support for this.
!!! note Execution
There is no separate SQL engine because Polars translates SQL queries into [expressions](../concepts/expressions-and-contexts.md#expressions), which are then executed using its own engine. This approach ensures that Polars maintains its performance and scalability advantages as a native DataFrame library, while still providing users with the ability to work with SQL.
## Context
Polars uses the `SQLContext` object to manage SQL queries. The context contains a mapping of
`DataFrame` and `LazyFrame` identifier names to their corresponding datasets[^1]. The example below
starts a `SQLContext`:
{{code_block('user-guide/sql/intro','context',['SQLContext'])}}
```python exec="on" session="user-guide/sql"
--8<-- "python/user-guide/sql/intro.py:setup"
--8<-- "python/user-guide/sql/intro.py:context"
```
## Register Dataframes
There are several ways to register DataFrames during `SQLContext` initialization.
- register all `LazyFrame` and `DataFrame` objects in the global namespace.
- register explicitly via a dictionary mapping, or kwargs.
{{code_block('user-guide/sql/intro','register_context',['SQLContext'])}}
```python exec="on" session="user-guide/sql"
--8<-- "python/user-guide/sql/intro.py:register_context"
```
We can also register Pandas DataFrames by converting them to Polars first.
{{code_block('user-guide/sql/intro','register_pandas',['SQLContext'])}}
```python exec="on" session="user-guide/sql"
--8<-- "python/user-guide/sql/intro.py:register_pandas"
```
!!! note Pandas
Converting a Pandas DataFrame backed by Numpy will trigger a potentially expensive conversion; however, if the Pandas DataFrame is already backed by Arrow then the conversion will be significantly cheaper (and in some cases close to free).
Once the `SQLContext` is initialized, we can register additional Dataframes or unregister existing
Dataframes with:
- `register`
- `register_globals`
- `register_many`
- `unregister`
## Execute queries and collect results
SQL queries are always executed in lazy mode to take advantage of the full set of query planning
optimizations, so we have two options to collect the result:
- Set the parameter `eager_execution` to True in `SQLContext`; this ensures that Polars
automatically collects the LazyFrame results from `execute` calls.
- Set the parameter `eager` to True when executing a query with `execute`, or explicitly collect the
result using `collect`.
We execute SQL queries by calling `execute` on a `SQLContext`.
{{code_block('user-guide/sql/intro','execute',['SQLregister','SQLexecute'])}}
```python exec="on" result="text" session="user-guide/sql"
--8<-- "python/user-guide/sql/intro.py:execute"
```
## Execute queries from multiple sources
SQL queries can be executed just as easily from multiple sources. In the example below, we register:
- a CSV file (loaded lazily)
- a NDJSON file (loaded lazily)
- a Pandas DataFrame
And join them together using SQL. Lazy reading allows to only load the necessary rows and columns
from the files.
In the same way, it's possible to register cloud datalakes (S3, Azure Data Lake). A PyArrow dataset
can point to the datalake, then Polars can read it with `scan_pyarrow_dataset`.
{{code_block('user-guide/sql/intro','execute_multiple_sources',['SQLregister','SQLexecute'])}}
```python exec="on" result="text" session="user-guide/sql"
--8<-- "python/user-guide/sql/intro.py:prepare_multiple_sources"
--8<-- "python/user-guide/sql/intro.py:execute_multiple_sources"
--8<-- "python/user-guide/sql/intro.py:clean_multiple_sources"
```
[^1]: Additionally it also tracks the [common table expressions](./cte.md) as well.
## Compatibility
Polars does not support the complete SQL specification, but it does support a subset of the most
common statement types.
!!! note Dialect
Where possible, Polars aims to follow PostgreSQL syntax definitions and function behaviour.
For example, here is a non-exhaustive list of some of the supported functionality:
- Write a `CREATE` statements: `CREATE TABLE xxx AS ...`
- Write a `SELECT` statements containing:`WHERE`,`ORDER`,`LIMIT`,`GROUP BY`,`UNION` and `JOIN`
clauses ...
- Write Common Table Expressions (CTE's) such as: `WITH tablename AS`
- Explain a query: `EXPLAIN SELECT ...`
- List registered tables: `SHOW TABLES`
- Drop a table: `DROP TABLE tablename`
- Truncate a table: `TRUNCATE TABLE tablename`
The following are some features that are not yet supported:
- `INSERT`, `UPDATE` or `DELETE` statements
- Meta queries such as `ANALYZE`
In the upcoming sections we will cover each of the statements in more detail.
---
polars/docs/source/user-guide/sql/select.md
---
# SELECT
In Polars SQL, the `SELECT` statement is used to retrieve data from a table into a `DataFrame`. The
basic syntax of a `SELECT` statement in Polars SQL is as follows:
```sql
SELECT column1, column2, ...
FROM table_name;
```
Here, `column1`, `column2`, etc. are the columns that you want to select from the table. You can
also use the wildcard `*` to select all columns. `table_name` is the name of the table or that you
want to retrieve data from. In the sections below we will cover some of the more common SELECT
variants
{{code_block('user-guide/sql/select','df',['SQLregister','SQLexecute'])}}
```python exec="on" result="text" session="user-guide/sql/select"
--8<-- "python/user-guide/sql/select.py:setup"
--8<-- "python/user-guide/sql/select.py:df"
```
### GROUP BY
The `GROUP BY` statement is used to group rows in a table by one or more columns and compute
aggregate functions on each group.
{{code_block('user-guide/sql/select','group_by',['SQLexecute'])}}
```python exec="on" result="text" session="user-guide/sql/select"
--8<-- "python/user-guide/sql/select.py:group_by"
```
### ORDER BY
The `ORDER BY` statement is used to sort the result set of a query by one or more columns in
ascending or descending order.
{{code_block('user-guide/sql/select','orderby',['SQLexecute'])}}
```python exec="on" result="text" session="user-guide/sql/select"
--8<-- "python/user-guide/sql/select.py:orderby"
```
### JOIN
{{code_block('user-guide/sql/select','join',['SQLregister_many','SQLexecute'])}}
```python exec="on" result="text" session="user-guide/sql/select"
--8<-- "python/user-guide/sql/select.py:join"
```
### Functions
Polars provides a wide range of SQL functions, including:
- Mathematical functions: `ABS`, `EXP`, `LOG`, `ASIN`, `ACOS`, `ATAN`, etc.
- String functions: `LOWER`, `UPPER`, `LTRIM`, `RTRIM`, `STARTS_WITH`,`ENDS_WITH`.
- Aggregation functions: `SUM`, `AVG`, `MIN`, `MAX`, `COUNT`, `STDDEV`, `FIRST` etc.
- Array functions: `EXPLODE`, `UNNEST`,`ARRAY_SUM`,`ARRAY_REVERSE`, etc.
For a full list of supported functions go the
[API documentation](https://docs.rs/polars-sql/latest/src/polars_sql/keywords.rs.html). The example
below demonstrates how to use a function in a query
{{code_block('user-guide/sql/select','functions',['SQLexecute'])}}
```python exec="on" result="text" session="user-guide/sql/select"
--8<-- "python/user-guide/sql/select.py:functions"
```
### Table Functions
In the examples earlier we first generated a DataFrame which we registered in the `SQLContext`.
Polars also support directly reading from CSV, Parquet, JSON and IPC in your SQL query using table
functions `read_xxx`.
{{code_block('user-guide/sql/select','tablefunctions',['SQLexecute'])}}
```python exec="on" result="text" session="user-guide/sql/select"
--8<-- "python/user-guide/sql/select.py:tablefunctions"
```
---
polars/docs/source/user-guide/sql/show.md
---
# SHOW TABLES
In Polars, the `SHOW TABLES` statement is used to list all the tables that have been registered in
the current `SQLContext`. When you register a DataFrame with the `SQLContext`, you give it a name
that can be used to refer to the DataFrame in subsequent SQL statements. The `SHOW TABLES` statement
allows you to see a list of all the registered tables, along with their names.
The syntax for the `SHOW TABLES` statement in Polars is as follows:
```
SHOW TABLES
```
Here's an example of how to use the `SHOW TABLES` statement in Polars:
{{code_block('user-guide/sql/show','show',['SQLregister','SQLexecute'])}}
```python exec="on" result="text" session="user-guide/sql/show"
--8<-- "python/user-guide/sql/show.py:setup"
--8<-- "python/user-guide/sql/show.py:show"
```
In this example, we create two DataFrames and register them with the `SQLContext` using different
names. We then execute a `SHOW TABLES` statement using the `execute()` method of the `SQLContext`
object, which returns a DataFrame containing a list of all the registered tables and their names.
The resulting DataFrame is then printed using the `print()` function.
Note that the `SHOW TABLES` statement only lists tables that have been registered with the current
`SQLContext`. If you register a DataFrame with a different `SQLContext` or in a different Python
session, it will not appear in the list of tables returned by `SHOW TABLES`.
---
polars/docs/source/user-guide/migration/pandas.md
---
# Coming from Pandas
Here we set out the key points that anyone who has experience with pandas and wants to try Polars
should know. We include both differences in the concepts the libraries are built on and differences
in how you should write Polars code compared to pandas code.
## Differences in concepts between Polars and pandas
### Polars does not have a multi-index/index
pandas gives a label to each row with an index. Polars does not use an index and each row is indexed
by its integer position in the table.
Polars aims to have predictable results and readable queries, as such we think an index does not
help us reach that objective. We believe the semantics of a query should not change by the state of
an index or a `reset_index` call.
In Polars a DataFrame will always be a 2D table with heterogeneous data-types. The data-types may
have nesting, but the table itself will not. Operations like resampling will be done by specialized
functions or methods that act like 'verbs' on a table explicitly stating the columns that that
'verb' operates on. As such, it is our conviction that not having indices make things simpler, more
explicit, more readable and less error-prone.
Note that an 'index' data structure as known in databases will be used by Polars as an optimization
technique.
### Polars adheres to the Apache Arrow memory format to represent data in memory while pandas uses NumPy arrays
Polars represents data in memory according to the Arrow memory spec while pandas by default
represents data in memory with NumPy arrays. Apache Arrow is an emerging standard for in-memory
columnar analytics that can accelerate data load times, reduce memory usage and accelerate
calculations.
Polars can convert data to NumPy format with the `to_numpy` method.
### Polars has more support for parallel operations than pandas
Polars exploits the strong support for concurrency in Rust to run many operations in parallel. While
some operations in pandas are multi-threaded the core of the library is single-threaded and an
additional library such as `Dask` must be used to parallelize operations. Polars is faster than all
open source solutions that parallelize pandas code.
### Polars has support for different engines
Polars has native support for an engine optimized for in-memory processing and a streaming engine
optimized for large scale data processing. Furthermore Polars has native integration with a CuDF
supported engine. All these engines benefit from Polars' query optimizer and Polars ensures semantic
correctness between all those engines. In pandas the implementation can dispatch between numpy and
Pyarrow, but because of pandas' loose strictness guarantees, the data-type outputs and semantics
between those backends can differ. This can lead to subtle bugs.
### Polars can lazily evaluate queries and apply query optimization
Eager evaluation is when code is evaluated as soon as you run the code. Lazy evaluation is when
running a line of code means that the underlying logic is added to a query plan rather than being
evaluated.
Polars supports eager evaluation and lazy evaluation whereas pandas only supports eager evaluation.
The lazy evaluation mode is powerful because Polars carries out automatic query optimization when it
examines the query plan and looks for ways to accelerate the query or reduce memory usage.
`Dask` also supports lazy evaluation when it generates a query plan.
### Polars is strict
Polars is strict about data types. Data type resolution in Polars is dependent on the operation
graph, whereas pandas converts types loosely (e.g. new missing data can lead to integer columns
being converted to floats). This strictness leads to fewer bugs and more predictable behavior.
### Polars has a more verstatile API
Polars is built on expressions and allows expression inputs in almost all operations. This means
that when you understand how expressions work, your knowledge in Polars extrapolates. Pandas doesn't
have an expression system and often requires Python `lambda`s to express the complexity you want.
Polars sees the requirement of a Python `lambda` as a lack of expressiveness of its API, and tries
to give you native support whenever possible.
## Key syntax differences
Users coming from pandas generally need to know one thing...
```
polars != pandas
```
If your Polars code looks like it could be pandas code, it might run, but it likely runs slower than
it should.
Let's go through some typical pandas code and see how we might rewrite it in Polars.
### Selecting data
As there is no index in Polars there is no `.loc` or `iloc` method in Polars - and there is also no
`SettingWithCopyWarning` in Polars.
However, the best way to select data in Polars is to use the expression API. For example, if you
want to select a column in pandas, you can do one of the following:
```python
df["a"]
df.loc[:,"a"]
```
but in Polars you would use the `.select` method:
```python
df.select("a")
```
If you want to select rows based on the values then in Polars you use the `.filter` method:
```python
df.filter(pl.col("a") < 10)
```
As noted in the section on expressions below, Polars can run operations in `.select` and `filter` in
parallel and Polars can carry out query optimization on the full set of data selection criteria.
### Be lazy
Working in lazy evaluation mode is straightforward and should be your default in Polars as the lazy
mode allows Polars to do query optimization.
We can run in lazy mode by either using an implicitly lazy function (such as `scan_csv`) or
explicitly using the `lazy` method.
Take the following simple example where we read a CSV file from disk and do a group by. The CSV file
has numerous columns but we just want to do a group by on one of the id columns (`id1`) and then sum
by a value column (`v1`). In pandas this would be:
```python
df = pd.read_csv(csv_file, usecols=["id1","v1"])
grouped_df = df.loc[:,["id1","v1"]].groupby("id1").sum("v1")
```
In Polars you can build this query in lazy mode with query optimization and evaluate it by replacing
the eager pandas function `read_csv` with the implicitly lazy Polars function `scan_csv`:
```python
df = pl.scan_csv(csv_file)
grouped_df = df.group_by("id1").agg(pl.col("v1").sum()).collect()
```
Polars optimizes this query by identifying that only the `id1` and `v1` columns are relevant and so
will only read these columns from the CSV. By calling the `.collect` method at the end of the second
line we instruct Polars to eagerly evaluate the query.
If you do want to run this query in eager mode you can just replace `scan_csv` with `read_csv` in
the Polars code.
Read more about working with lazy evaluation in the [lazy API](../lazy/using.md) section.
### Express yourself
A typical pandas script consists of multiple data transformations that are executed sequentially.
However, in Polars these transformations can be executed in parallel using expressions.
#### Column assignment
We have a dataframe `df` with a column called `value`. We want to add two new columns, a column
called `tenXValue` where the `value` column is multiplied by 10 and a column called `hundredXValue`
where the `value` column is multiplied by 100.
In pandas this would be:
```python
df.assign(
tenXValue=lambda df_: df_.value * 10,
hundredXValue=lambda df_: df_.value * 100
)
```
These column assignments are executed sequentially.
In Polars we add columns to `df` using the `.with_columns` method:
```python
df.with_columns(
tenXValue=pl.col("value") * 10,
hundredXValue=pl.col("value") * 100,
)
```
These column assignments are executed in parallel.
#### Column assignment based on predicate
In this case we have a dataframe `df` with columns `a`,`b` and `c`. We want to re-assign the values
in column `a` based on a condition. When the value in column `c` is equal to 2 then we replace the
value in `a` with the value in `b`.
In pandas this would be:
```python
df.assign(a=lambda df_: df_["a"].mask(df_["c"] == 2, df_["b"]))
```
while in Polars this would be:
```python
df.with_columns(
pl.when(pl.col("c") == 2)
.then(pl.col("b"))
.otherwise(pl.col("a")).alias("a")
)
```
Polars can compute every branch of an `if -> then -> otherwise` in parallel. This is valuable, when
the branches get more expensive to compute.
#### Filtering
We want to filter the dataframe `df` with housing data based on some criteria.
In pandas you filter the dataframe by passing Boolean expressions to the `query` method:
```python
df.query("m2_living > 2500 and price < 300000")
```
or by directly evaluating a mask:
```python
df[(df["m2_living"] > 2500) & (df["price"] < 300000)]
```
while in Polars you call the `filter` method:
```python
df.filter(
(pl.col("m2_living") > 2500) & (pl.col("price") < 300000)
)
```
The query optimizer in Polars can also detect if you write multiple filters separately and combine
them into a single filter in the optimized plan.
## pandas transform
The pandas documentation demonstrates an operation on a group by called `transform`. In this case we
have a dataframe `df` and we want a new column showing the number of rows in each group.
In pandas we have:
```python
df = pd.DataFrame({
"c": [1, 1, 1, 2, 2, 2, 2],
"type": ["m", "n", "o", "m", "m", "n", "n"],
})
df["size"] = df.groupby("c")["type"].transform(len)
```
Here pandas does a group by on `"c"`, takes column `"type"`, computes the group length and then
joins the result back to the original `DataFrame` producing:
```
c type size
0 1 m 3
1 1 n 3
2 1 o 3
3 2 m 4
4 2 m 4
5 2 n 4
6 2 n 4
```
In Polars the same can be achieved with `window` functions:
```python
df.with_columns(
pl.col("type").count().over("c").alias("size")
)
```
```
shape: (7, 3)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”�
â”‚ c â”† type â”† size â”‚
â”‚ --- â”† --- â”† --- â”‚
â”‚ i64 â”† str â”† u32 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† m â”† 3 â”‚
â”‚ 1 â”† n â”† 3 â”‚
â”‚ 1 â”† o â”† 3 â”‚
â”‚ 2 â”† m â”† 4 â”‚
â”‚ 2 â”† m â”† 4 â”‚
â”‚ 2 â”† n â”† 4 â”‚
â”‚ 2 â”† n â”† 4 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```
Because we can store the whole operation in a single expression, we can combine several `window`
functions and even combine different groups!
Polars will cache window expressions that are applied over the same group, so storing them in a
single `with_columns` is both convenient **and** optimal. In the following example we look at a case
where we are calculating group statistics over `"c"` twice:
```python
df.with_columns(
pl.col("c").count().over("c").alias("size"),
pl.col("c").sum().over("type").alias("sum"),
pl.col("type").reverse().over("c").alias("reverse_type")
)
```
```
shape: (7, 5)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ c â”† type â”† size â”† sum â”† reverse_type â”‚
â”‚ --- â”† --- â”† --- â”† --- â”† --- â”‚
â”‚ i64 â”† str â”† u32 â”† i64 â”† str â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† m â”† 3 â”† 5 â”† o â”‚
â”‚ 1 â”† n â”† 3 â”† 5 â”† n â”‚
â”‚ 1 â”† o â”† 3 â”† 1 â”† m â”‚
â”‚ 2 â”† m â”† 4 â”† 5 â”† n â”‚
â”‚ 2 â”† m â”† 4 â”† 5 â”† n â”‚
â”‚ 2 â”† n â”† 4 â”† 5 â”† m â”‚
â”‚ 2 â”† n â”† 4 â”† 5 â”† m â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## Missing data
pandas uses `NaN` and/or `None` values to indicate missing values depending on the dtype of the
column. In addition the behaviour in pandas varies depending on whether the default dtypes or
optional nullable arrays are used. In Polars missing data corresponds to a `null` value for all data
types.
For float columns Polars permits the use of `NaN` values. These `NaN` values are not considered to
be missing data but instead a special floating point value.
In pandas an integer column with missing values is cast to be a float column with `NaN` values for
the missing values (unless using optional nullable integer dtypes). In Polars any missing values in
an integer column are simply `null` values and the column remains an integer column.
See the [missing data](../expressions/missing-data.md) section for more details.
## Pipe littering
A common usage in pandas is utilizing `pipe` to apply some function to a `DataFrame`. Copying this
coding style to Polars is unidiomatic and leads to suboptimal query plans.
The snippet below shows a common pattern in pandas.
```python
def add_foo(df: pd.DataFrame) -> pd.DataFrame:
df["foo"] = ...
return df
def add_bar(df: pd.DataFrame) -> pd.DataFrame:
df["bar"] = ...
return df
def add_ham(df: pd.DataFrame) -> pd.DataFrame:
df["ham"] = ...
return df
(df
.pipe(add_foo)
.pipe(add_bar)
.pipe(add_ham)
)
```
If we do this in polars, we would create 3 `with_columns` contexts, that forces Polars to run the 3
pipes sequentially, utilizing zero parallelism.
The way to get similar abstractions in polars is creating functions that create expressions. The
snippet below creates 3 expressions that run on a single context and thus are allowed to run in
parallel.
```python
def get_foo(input_column: str) -> pl.Expr:
return pl.col(input_column).some_computation().alias("foo")
def get_bar(input_column: str) -> pl.Expr:
return pl.col(input_column).some_computation().alias("bar")
def get_ham(input_column: str) -> pl.Expr:
return pl.col(input_column).some_computation().alias("ham")
# This single context will run all 3 expressions in parallel
df.with_columns(
get_ham("col_a"),
get_bar("col_b"),
get_foo("col_c"),
)
```
If you need the schema in the functions that generate the expressions, you can utilize a single
`pipe`:
```python
from collections import OrderedDict
def get_foo(input_column: str, schema: OrderedDict) -> pl.Expr:
if "some_col" in schema:
# branch_a
...
else:
# branch b
...
def get_bar(input_column: str, schema: OrderedDict) -> pl.Expr:
if "some_col" in schema:
# branch_a
...
else:
# branch b
...
def get_ham(input_column: str) -> pl.Expr:
return pl.col(input_column).some_computation().alias("ham")
# Use pipe (just once) to get hold of the schema of the LazyFrame.
lf.pipe(lambda lf: lf.with_columns(
get_ham("col_a"),
get_bar("col_b", lf.schema),
get_foo("col_c", lf.schema),
)
```
Another benefit of writing functions that return expressions, is that these functions are composable
as expressions can be chained and partially applied, leading to much more flexibility in the design.
---
polars/docs/source/user-guide/migration/spark.md
---
# Coming from Apache Spark
## Column-based API vs. Row-based API
Whereas the `Spark` `DataFrame` is analogous to a collection of rows, a Polars `DataFrame` is closer
to a collection of columns. This means that you can combine columns in Polars in ways that are not
possible in `Spark`, because `Spark` preserves the relationship of the data in each row.
Consider this sample dataset:
```python
import polars as pl
df = pl.DataFrame({
"foo": ["a", "b", "c", "d", "d"],
"bar": [1, 2, 3, 4, 5],
})
dfs = spark.createDataFrame(
[
("a", 1),
("b", 2),
("c", 3),
("d", 4),
("d", 5),
],
schema=["foo", "bar"],
)
```
### Example 1: Combining `head` and `sum`
In Polars you can write something like this:
```python
df.select(
pl.col("foo").sort().head(2),
pl.col("bar").filter(pl.col("foo") == "d").sum()
)
```
Output:
```
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ foo â”† bar â”‚
â”‚ --- â”† --- â”‚
â”‚ str â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ a â”† 9 â”‚
â”œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ”¤
â”‚ b â”† 9 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
The expressions on columns `foo` and `bar` are completely independent. Since the expression on `bar`
returns a single value, that value is repeated for each value output by the expression on `foo`. But
`a` and `b` have no relation to the data that produced the sum of `9`.
To do something similar in `Spark`, you'd need to compute the sum separately and provide it as a
literal:
```python
from pyspark.sql.functions import col, sum, lit
bar_sum = (
dfs
.where(col("foo") == "d")
.groupBy()
.agg(sum(col("bar")))
.take(1)[0][0]
)
(
dfs
.orderBy("foo")
.limit(2)
.withColumn("bar", lit(bar_sum))
.show()
)
```
Output:
```
+---+---+
|foo|bar|
+---+---+
| a| 9|
| b| 9|
+---+---+
```
### Example 2: Combining Two `head`s
In Polars you can combine two different `head` expressions on the same DataFrame, provided that they
return the same number of values.
```python
df.select(
pl.col("foo").sort().head(2),
pl.col("bar").sort(descending=True).head(2),
)
```
Output:
```
shape: (3, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ foo â”† bar â”‚
â”‚ --- â”† --- â”‚
â”‚ str â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ a â”† 5 â”‚
â”œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ”¤
â”‚ b â”† 4 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
Again, the two `head` expressions here are completely independent, and the pairing of `a` to `5` and
`b` to `4` results purely from the juxtaposition of the two columns output by the expressions.
To accomplish something similar in `Spark`, you would need to generate an artificial key that
enables you to join the values in this way.
```python
from pyspark.sql import Window
from pyspark.sql.functions import row_number
foo_dfs = (
dfs
.withColumn(
"rownum",
row_number().over(Window.orderBy("foo"))
)
)
bar_dfs = (
dfs
.withColumn(
"rownum",
row_number().over(Window.orderBy(col("bar").desc()))
)
)
(
foo_dfs.alias("foo")
.join(bar_dfs.alias("bar"), on="rownum")
.select("foo.foo", "bar.bar")
.limit(2)
.show()
)
```
Output:
```
+---+---+
|foo|bar|
+---+---+
| a| 5|
| b| 4|
+---+---+
```
### Example 3: Composing expressions
Polars allows you compose expressions quite liberally. For example, if you want to find the rolling
mean of a lagged variable, you can compose `shift` and `rolling_mean` and evaluate them in a single
`over` expression:
```python
df.with_columns(
feature=pl.col('price').shift(7).rolling_mean(7).over('store', order_by='date')
)
```
In PySpark however this is not allowed. They allow composing expressions such as
`F.mean(F.abs("price")).over(window)` because `F.abs` is an elementwise function, but not
`F.mean(F.lag("price", 1)).over(window)` because `F.lag` is a window function. To produce the same
result, both `F.lag` and `F.mean` need their own window.
```python
from pyspark.sql import Window
from pyspark.sql import functions as F
window = Window().partitionBy("store").orderBy("date")
rolling_window = window.rowsBetween(-6, 0)
(
df.withColumn("lagged_price", F.lag("price", 7).over(window)).withColumn(
"feature",
F.when(
F.count("lagged_price").over(rolling_window) >= 7,
F.mean("lagged_price").over(rolling_window),
),
)
)
```
---
polars/docs/source/user-guide/plugins/expr_plugins.md
---
# Expression Plugins
Expression plugins are the preferred way to create user defined functions. They allow you to compile
a Rust function and register that as an expression into the Polars library. The Polars engine will
dynamically link your function at runtime and your expression will run almost as fast as native
expressions. Note that this works without any interference of Python and thus no GIL contention.
They will benefit from the same benefits default expressions have:
- Optimization
- Parallelism
- Rust native performance
To get started we will see what is needed to create a custom expression.
## Our first custom expression: Pig Latin
For our first expression we are going to create a pig latin converter. Pig latin is a silly language
where in every word the first letter is removed, added to the back and finally "ay" is added. So the
word "pig" would convert to "igpay".
We could of course already do that with expressions, e.g.
`col("name").str.slice(1) + col("name").str.slice(0, 1) + "ay"`, but a specialized function for this
would perform better and allows us to learn about the plugins.
### Setting up
We start with a new library as the following `Cargo.toml` file
```toml
[package]
name = "expression_lib"
version = "0.1.0"
edition = "2021"
[lib]
name = "expression_lib"
crate-type = ["cdylib"]
[dependencies]
polars = { version = "*" }
pyo3 = { version = "*", features = ["extension-module", "abi3-py38"] }
pyo3-polars = { version = "*", features = ["derive"] }
serde = { version = "*", features = ["derive"] }
```
### Writing the expression
In this library we create a helper function that converts a `&str` to pig-latin, and we create the
function that we will expose as an expression. To expose a function we must add the
`#[polars_expr(output_type=DataType)]` attribute and the function must always accept
`inputs: &[Series]` as its first argument.
```rust
// src/expressions.rs
use polars::prelude::*;
use pyo3_polars::derive::polars_expr;
use std::fmt::Write;
fn pig_latin_str(value: &str, output: &mut String) {
if let Some(first_char) = value.chars().next() {
write!(output, "{}{}ay", &value[1..], first_char).unwrap()
}
}
#[polars_expr(output_type=String)]
fn pig_latinnify(inputs: &[Series]) -> PolarsResult {
let ca = inputs[0].str()?;
let out: StringChunked = ca.apply_into_string_amortized(pig_latin_str);
Ok(out.into_series())
}
```
Note that we use `apply_into_string_amortized`, as opposed to `apply_values`, to avoid allocating a
new string for each row. If your plugin takes in multiple inputs, operates elementwise, and produces
a `String` output, then you may want to look at the `binary_elementwise_into_string_amortized`
utility function in `polars::prelude::arity`.
This is all that is needed on the Rust side. On the Python side we must setup a folder with the same
name as defined in the `Cargo.toml`, in this case "expression_lib". We will create a folder in the
same directory as our Rust `src` folder named `expression_lib` and we create an
`expression_lib/__init__.py`. The resulting file structure should look something like this:
```
â”œâ”€â”€ ðŸ“� expression_lib/ # name must match "lib.name" in Cargo.toml
| â””â”€â”€ __init__.py
|
â”œâ”€â”€ ðŸ“�src/
| â”œâ”€â”€ lib.rs
| â””â”€â”€ expressions.rs
|
â”œâ”€â”€ Cargo.toml
â””â”€â”€ pyproject.toml
```
Then we create a new class `Language` that will hold the expressions for our new `expr.language`
namespace. The function name of our expression can be registered. Note that it is important that
this name is correct, otherwise the main Polars package cannot resolve the function name.
Furthermore we can set additional keyword arguments that explain to Polars how this expression
behaves. In this case we tell Polars that this function is elementwise. This allows Polars to run
this expression in batches. Whereas for other operations this would not be allowed, think for
instance of a sort, or a slice.
```python
# expression_lib/__init__.py
from pathlib import Path
from typing import TYPE_CHECKING
import polars as pl
from polars.plugins import register_plugin_function
from polars._typing import IntoExpr
PLUGIN_PATH = Path(__file__).parent
def pig_latinnify(expr: IntoExpr) -> pl.Expr:
"""Pig-latinnify expression."""
return register_plugin_function(
plugin_path=PLUGIN_PATH,
function_name="pig_latinnify",
args=expr,
is_elementwise=True,
)
```
We can then compile this library in our environment by installing `maturin` and running
`maturin develop --release`.
And that's it. Our expression is ready to use!
```python
import polars as pl
from expression_lib import pig_latinnify
df = pl.DataFrame(
{
"convert": ["pig", "latin", "is", "silly"],
}
)
out = df.with_columns(pig_latin=pig_latinnify("convert"))
```
Alternatively, you can
[register a custom namespace](https://docs.pola.rs/api/python/stable/reference/api/polars.api.register_expr_namespace.html#polars.api.register_expr_namespace),
which enables you to write:
```python
out = df.with_columns(
pig_latin=pl.col("convert").language.pig_latinnify(),
)
```
## Accepting kwargs
If you want to accept `kwargs` (keyword arguments) in a polars expression, all you have to do is
define a Rust `struct` and make sure that it derives `serde::Deserialize`.
```rust
/// Provide your own kwargs struct with the proper schema and accept that type
/// in your plugin expression.
#[derive(Deserialize)]
pub struct MyKwargs {
float_arg: f64,
integer_arg: i64,
string_arg: String,
boolean_arg: bool,
}
/// If you want to accept `kwargs`. You define a `kwargs` argument
/// on the second position in you plugin. You can provide any custom struct that is deserializable
/// with the pickle protocol (on the Rust side).
#[polars_expr(output_type=String)]
fn append_kwargs(input: &[Series], kwargs: MyKwargs) -> PolarsResult {
let input = &input[0];
let input = input.cast(&DataType::String)?;
let ca = input.str().unwrap();
Ok(ca
.apply_into_string_amortized(|val, buf| {
write!(
buf,
"{}-{}-{}-{}-{}",
val, kwargs.float_arg, kwargs.integer_arg, kwargs.string_arg, kwargs.boolean_arg
)
.unwrap()
})
.into_series())
}
```
On the Python side the kwargs can be passed when we register the plugin.
```python
def append_args(
expr: IntoExpr,
float_arg: float,
integer_arg: int,
string_arg: str,
boolean_arg: bool,
) -> pl.Expr:
"""
This example shows how arguments other than `Series` can be used.
"""
return register_plugin_function(
plugin_path=PLUGIN_PATH,
function_name="append_kwargs",
args=expr,
kwargs={
"float_arg": float_arg,
"integer_arg": integer_arg,
"string_arg": string_arg,
"boolean_arg": boolean_arg,
},
is_elementwise=True,
)
```
## Output data types
Output data types of course don't have to be fixed. They often depend on the input types of an
expression. To accommodate this you can provide the `#[polars_expr()]` macro with an
`output_type_func` argument that points to a function. This function can map input fields `&[Field]`
to an output `Field` (name and data type).
In the snippet below is an example where we use the utility `FieldsMapper` to help with this
mapping.
```rust
use polars_plan::dsl::FieldsMapper;
fn haversine_output(input_fields: &[Field]) -> PolarsResult {
FieldsMapper::new(input_fields).map_to_float_dtype()
}
#[polars_expr(output_type_func=haversine_output)]
fn haversine(inputs: &[Series]) -> PolarsResult {
let out = match inputs[0].dtype() {
DataType::Float32 => {
let start_lat = inputs[0].f32().unwrap();
let start_long = inputs[1].f32().unwrap();
let end_lat = inputs[2].f32().unwrap();
let end_long = inputs[3].f32().unwrap();
crate::distances::naive_haversine(start_lat, start_long, end_lat, end_long)?
.into_series()
}
DataType::Float64 => {
let start_lat = inputs[0].f64().unwrap();
let start_long = inputs[1].f64().unwrap();
let end_lat = inputs[2].f64().unwrap();
let end_long = inputs[3].f64().unwrap();
crate::distances::naive_haversine(start_lat, start_long, end_lat, end_long)?
.into_series()
}
_ => polars_bail!(InvalidOperation: "only supported for float types"),
};
Ok(out)
}
```
That's all you need to know to get started. Take a look at
[this repo](https://github.com/pola-rs/pyo3-polars/tree/main/example/derive_expression) to see how
this all fits together, and at
[this tutorial](https://marcogorelli.github.io/polars-plugins-tutorial/) to gain a more thorough
understanding.
---
polars/docs/source/user-guide/plugins/index.md
---
# Plugins
Polars allows you to extend its functionality with either Expression plugins or IO plugins.
- [Expression plugins](./expr_plugins.md)
- [IO plugins](./io_plugins.md)
## Community plugins
Here is a curated (non-exhaustive) list of community-implemented plugins.
### Various
- [polars-xdt](https://github.com/pola-rs/polars-xdt) Polars plugin with extra datetime-related
functionality which isn't quite in-scope for the main library
- [polars-hash](https://github.com/ion-elgreco/polars-hash) Stable non-cryptographic and
cryptographic hashing functions for Polars
### Data science
- [polars-distance](https://github.com/ion-elgreco/polars-distance) Polars plugin for pairwise
distance functions
- [polars-ds](https://github.com/abstractqqq/polars_ds_extension) Polars extension aiming to
simplify common numerical/string data analysis procedures
### Geo
- [polars-st](https://github.com/Oreilles/polars-st) Polars ST provides spatial operations on Polars
DataFrames, Series and Expressions. Just like Shapely and Geopandas.
- [polars-reverse-geocode](https://github.com/MarcoGorelli/polars-reverse-geocode) Offline reverse
geocoder for finding the closest city to a given (latitude, longitude) pair.
- [polars-h3](https://github.com/Filimoa/polars-h3) This is a Polars extension that adds support for
the H3 discrete global grid system, so you can index points and geometries to hexagons directly in
Polars.
## Other material
- [Ritchie Vink - Keynote on Polars Plugins](https://youtu.be/jKW-CBV7NUM)
- [Polars plugins tutorial](https://marcogorelli.github.io/polars-plugins-tutorial/) Learn how to
write a plugin by going through some very simple and minimal examples
- [cookiecutter-polars-plugin](https://github.com/MarcoGorelli/cookiecutter-polars-plugins) Project
template for Polars Plugins
---
polars/docs/source/user-guide/plugins/io_plugins.md
---
# IO Plugins
Besides [expression plugins](./expr_plugins.md), we also support IO plugins. These allow you to
register different file formats as sources to the Polars engines. Because sources can move data zero
copy via Arrow FFI and sources can produce large chunks of data before returning, we've decided to
interface to IO plugins via Python for now, as we don't think the short time the GIL is needed
should lead to any contention.
E.g. an IO source can read their dataframe's in rust and only at the rendez-vous move the data
zero-copy having only a short time the GIL is needed.
## Use case
You want IO plugins if you have a source file not supported by Polars and you want to benefit from
optimizations like projection pushdown, predicate pushdown, early stopping and support of our
streaming engine.
## Example
So let's write a simple, very bad, custom CSV source and register that as an IO plugin. I want to
stress that this is a very bad example and is only given for learning purposes.
First we define some imports we need:
```python
# Use python for csv parsing.
import csv
import polars as pl
# Used to register a new generator on every instantiation.
from polars.io.plugins import register_io_source
from typing import Iterator
import io
```
### Parsing the schema
Every `scan` function in Polars has to be able to provide the schema of the data it reads. For this
simple csv parser we will always read the data as `pl.String`. The only thing that differs are the
field names and the number of fields.
```python
def parse_schema(csv_str: str) -> pl.Schema:
first_line = csv_str.split("\n")[0]
return pl.Schema({k: pl.String for k in first_line.split(",")})
```
If we run this with small csv file `"a,b,c\n1,2,3"` we get the schema:
`Schema([('a', String), ('b', String), ('c', String)])`.
```python
>>> print(parse_schema("a,b,c\n1,2,3"))
Schema([('a', String), ('b', String), ('c', String)])
```
### Writing the source
Next up is the actual source. For this we create an outer and an inner function. The outer function
`my_scan_csv` is the user facing function. This function will accept the file name and other
potential arguments you would need for reading the source. For csv files, these arguments could be
"delimiter", "quote_char" and such.
This outer function calls `register_io_source` which accepts a `callable` and a `schema`. The schema
is the Polars schema of the complete source file (independent of projection pushdown).
The callable is a function that will return a generator that produces `pl.DataFrame` objects.
The arguments of this function are predefined and this function must accept:
- `with_columns`
Columns that are projected. The reader must project these columns if applied
- `predicate`
Polars expression. The reader must filter their rows accordingly.
- `n_rows`
Materialize only n rows from the source. The reader can stop when `n_rows` are read.
- `batch_size`
A hint of the ideal batch size the reader's generator must produce.
The inner function is the actual implementation of the IO source and can also call into Rust/C++ or
wherever the IO plugin is written. If you want to see an IO source implemented in Rust, take a look
at our [plugins repository](https://github.com/pola-rs/pyo3-polars/tree/main/example/io_plugin).
```python
def my_scan_csv(csv_str: str) -> pl.LazyFrame:
schema = parse_schema(csv_str)
def source_generator(
with_columns: list[str] | None,
predicate: pl.Expr | None,
n_rows: int | None,
batch_size: int | None,
) -> Iterator[pl.DataFrame]:
"""
Generator function that creates the source.
This function will be registered as IO source.
"""
if batch_size is None:
batch_size = 100
# Initialize the reader.
reader = csv.reader(io.StringIO(csv_str), delimiter=',')
# Skip the header.
_ = next(reader)
# Ensure we don't read more rows than requested from the engine
while n_rows is None or n_rows > 0:
if n_rows is not None:
batch_size = min(batch_size, n_rows)
rows = []
for _ in range(batch_size):
try:
row = next(reader)
except StopIteration:
n_rows = 0
break
rows.append(row)
df = pl.from_records(rows, schema=schema, orient="row")
n_rows -= df.height
# If we would make a performant reader, we would not read these
# columns at all.
if with_columns is not None:
df = df.select(with_columns)
# If the source supports predicate pushdown, the expression can be parsed
# to skip rows/groups.
if predicate is not None:
df = df.filter(predicate)
yield df
return register_io_source(io_source=source_generator, schema=schema)
```
### Taking it for a (very slow) spin
Finally we can test our source:
```python
csv_str1 = """a,b,c,d
1,2,3,4
9,10,11,2
1,2,3,4
1,122,3,4"""
print(my_scan_csv(csv_str1).collect())
csv_str2 = """a,b
1,2
9,10
1,2
1,122"""
print(my_scan_csv(csv_str2).head(2).collect())
```
Running the script above would print the following output to the console:
```
shape: (4, 4)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ a â”† b â”† c â”† d â”‚
â”‚ --- â”† --- â”† --- â”† --- â”‚
â”‚ str â”† str â”† str â”† str â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† 2 â”† 3 â”† 4 â”‚
â”‚ 9 â”† 10 â”† 11 â”† 2 â”‚
â”‚ 1 â”† 2 â”† 3 â”† 4 â”‚
â”‚ 1 â”† 122 â”† 3 â”† 4 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
shape: (2, 2)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”�
â”‚ a â”† b â”‚
â”‚ --- â”† --- â”‚
â”‚ str â”† str â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•¡
â”‚ 1 â”† 2 â”‚
â”‚ 9 â”† 10 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```
## Further reading
- [Rust example (distribution source)](https://github.com/pola-rs/pyo3-polars/tree/main/example/io_plugin)
---
polars/docs/source/user-guide/lazy/execution.md
---
# Query execution
Our example query on the Reddit dataset is:
{{code_block('user-guide/lazy/execution','df',['scan_csv'])}}
If we were to run the code above on the Reddit CSV the query would not be evaluated. Instead Polars
takes each line of code, adds it to the internal query graph and optimizes the query graph.
When we execute the code Polars executes the optimized query graph by default.
### Execution on the full dataset
We can execute our query on the full dataset by calling the `.collect` method on the query.
{{code_block('user-guide/lazy/execution','collect',['scan_csv','collect'])}}
```text
shape: (14_029, 6)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ id â”† name â”† created_utc â”† updated_on â”† comment_karma â”† link_karma â”‚
â”‚ --- â”† --- â”† --- â”† --- â”† --- â”† --- â”‚
â”‚ i64 â”† str â”† i64 â”† i64 â”† i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 6 â”† TAOJIANLONG_JASONBROKEN â”† 1397113510 â”† 1536527864 â”† 4 â”† 0 â”‚
â”‚ 17 â”† SSAIG_JASONBROKEN â”† 1397113544 â”† 1536527864 â”† 1 â”† 0 â”‚
â”‚ 19 â”† FDBVFDSSDGFDS_JASONBROKEN â”† 1397113552 â”† 1536527864 â”† 3 â”† 0 â”‚
â”‚ 37 â”† IHATEWHOWEARE_JASONBROKEN â”† 1397113636 â”† 1536527864 â”† 61 â”† 0 â”‚
â”‚ â€¦ â”† â€¦ â”† â€¦ â”† â€¦ â”† â€¦ â”† â€¦ â”‚
â”‚ 1229384 â”† DSFOX â”† 1163177415 â”† 1536497412 â”† 44411 â”† 7917 â”‚
â”‚ 1229459 â”† NEOCARTY â”† 1163177859 â”† 1536533090 â”† 40 â”† 0 â”‚
â”‚ 1229587 â”† TEHSMA â”† 1163178847 â”† 1536497412 â”† 14794 â”† 5707 â”‚
â”‚ 1229621 â”† JEREMYLOW â”† 1163179075 â”† 1536497412 â”† 411 â”† 1063 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
Above we see that from the 10 million rows there are 14,029 rows that match our predicate.
With the default `collect` method Polars processes all of your data as one batch. This means that
all the data has to fit into your available memory at the point of peak memory usage in your query.
!!! warning "Reusing `LazyFrame` objects"
Remember that `LazyFrame`s are query plans i.e. a promise on computation and is not guaranteed to cache common subplans. This means that every time you reuse it in separate downstream queries after it is defined, it is computed all over again. If you define an operation on a `LazyFrame` that doesn't maintain row order (such as a `group_by`), then the order will also change every time it is run. To avoid this, use `maintain_order=True` arguments for such operations.
### Execution on larger-than-memory data
If your data requires more memory than you have available Polars may be able to process the data in
batches using _streaming_ mode. To use streaming mode you simply pass the `engine="streaming"`
argument to `collect`
{{code_block('user-guide/lazy/execution','stream',['scan_csv','collect'])}}
### Execution on a partial dataset
While you're writing, optimizing or checking your query on a large dataset, querying all available
data may lead to a slow development process.
Instead, you can scan a subset of your partitions or use `.head`/`.collect` at the beginning and end
of your query, respectively. Keep in mind that the results of aggregations and filters on subsets of
your data may not be representative of the result you would get on the full data.
{{code_block('user-guide/lazy/execution','partial',['scan_csv','collect','head'])}}
```text
shape: (1, 6)
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”�
â”‚ id â”† name â”† created_utc â”† updated_on â”† comment_karma â”† link_karma â”‚
â”‚ --- â”† --- â”† --- â”† --- â”† --- â”† --- â”‚
â”‚ i64 â”† str â”† i64 â”† i64 â”† i64 â”† i64 â”‚
â•žâ•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•ªâ•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•�â•¡
â”‚ 6 â”† TAOJIANLONG_JASONBROKEN â”† 1397113510 â”† 1536527864 â”† 4 â”† 0 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## Diverging queries
It is very common that a query diverges at one point. In these cases it is recommended to use
`collect_all` as they will ensure that diverging queries execute only once.
```python
# Some expensive LazyFrame
lf: LazyFrame
lf_1 = LazyFrame.select(pl.all().sum())
lf_2 = lf.some_other_computation()
pl.collect_all([lf_1, lf_2]) # this will execute lf only once!
```
---
polars/docs/source/user-guide/lazy/gpu.md
---
# GPU Support
Polars provides an in-memory, GPU-accelerated execution engine for the Lazy API in Python using
[RAPIDS cuDF](https://docs.rapids.ai/api/cudf/stable/) on NVIDIA GPUs. This functionality is
available in Open Beta, is undergoing rapid development, and is currently a single GPU
implementation.
If you install Polars with the [GPU feature flag](../installation.md), you can trigger GPU-based
execution by running `.collect(engine="gpu")` instead of `.collect()`.
{{ code_header("python", [], []) }}
```python
--8<-- "python/user-guide/lazy/gpu.py:setup"
result = q.collect(engine="gpu")
print(result)
```
```python exec="on" result="text" session="user-guide/lazy"
--8<-- "python/user-guide/lazy/gpu.py:setup"
--8<-- "python/user-guide/lazy/gpu.py:simple-result"
```
Learn more in the [GPU Support guide](../gpu-support.md).
---
polars/docs/source/user-guide/lazy/index.md
---
# Lazy
The Lazy chapter is a guide for working with `LazyFrames`. It covers the functionalities like how to
use it and how to optimise it. You can also find more information about the query plan or gain more
insight in the streaming capabilities.
- [Using lazy API](using.md)
- [Optimisations](optimizations.md)
- [Schemas](schemas.md)
- [Query plan](query-plan.md)
- [Execution](execution.md)
- [Sources & Sinks](sources_sinks.md)
- [GPU Support](gpu.md)
---
polars/docs/source/user-guide/lazy/multiplexing.md
---
# Multiplexing queries
```python exec="on" result="text" session="user-guide/lazy/multiplexing"
--8<-- "python/user-guide/lazy/multiplexing.py:setup"
```
In the [Sources and Sinks](./sources_sinks.md) page, we already discussed multiplexing as a way to
split a query into multiple sinks. This page will go a bit deeper in this concept, as it is
important to understand when combining `LazyFrame`s with procedural programming constructs.
When dealing with eager dataframes, it is very common to keep state in a temporary variable. Let's
look at the following example. Below we create a `DataFrame` with 10 unique elements in a random
order (so that Polars doesn't hit any fast paths for sorted keys).
{{code_block('user-guide/lazy/multiplexing','dataframe',[])}}
```python exec="on" result="text" session="user-guide/lazy/multiplexing"
--8<-- "python/user-guide/lazy/multiplexing.py:dataframe"
```
## Eager
If you deal with the Polars eager API, making a variable and iterating over that temporary
`DataFrame` gives the result you expect, as the result of the group-by is stored in `df1`. Even
though the output order is unstable, it doesn't matter as it is eagerly evaluated. The follow
snippet therefore doesn't raise and the assert passes.
{{code_block('user-guide/lazy/multiplexing','eager',[])}}
## Lazy
Now if we tried this naively with `LazyFrame`s, this would fail.
{{code_block('user-guide/lazy/multiplexing','lazy',[])}}
```python
AssertionError: DataFrames are different (value mismatch for column 'n')
[left]: [9, 2, 0, 5, 3, 1, 7, 8, 6, 4]
[right]: [0, 9, 6, 8, 2, 5, 4, 3, 1, 7]
```
The reason this fails is that `lf1` doesn't contain the materialized result of
`df.lazy().group_by("n").len()`, it instead holds the query plan in that variable.
```python exec="on" session="user-guide/lazy/multiplexing"
--8<-- "python/user-guide/lazy/multiplexing.py:plan_0"
```
This means that every time we branch of this `LazyFrame` and call `collect` we re-evaluate the
group-by. Besides being expensive, this also leads to unexpected results if you assume that the
output is stable (which isn't the case here).
In the example above you are actually evaluating 2 query plans:
**Plan 1**
```python exec="on" session="user-guide/lazy/multiplexing"
--8<-- "python/user-guide/lazy/multiplexing.py:plan_1"
```
**Plan 2**
```python exec="on" session="user-guide/lazy/multiplexing"
--8<-- "python/user-guide/lazy/multiplexing.py:plan_2"
```
## Combine the query plans
To circumvent this, we must give Polars the opportunity to look at all the query plans in a single
optimization and execution pass. This can be done by passing the diverging `LazyFrame`'s to the
`collect_all` function.
{{code_block('user-guide/lazy/multiplexing','collect_all',[])}}
If we explain the combined queries with `pl.explain_all`, we can also observe that they are shared
under a single "SINK_MULTIPLE" evaluation and that the optimizer has recognized that parts of the
query come from the same subplan, indicated by the inserted "CACHE" nodes.
```python exec="on" result="text" session="user-guide/lazy/multiplexing"
--8<-- "python/user-guide/lazy/multiplexing.py:explain_all"
```
Combining related subplans in a single execution unit with `pl.collect_all` can thus lead to large
performance increases and allows diverging query plans, storing temporary tables, and a more
procedural programming style.
---
polars/docs/source/user-guide/lazy/optimizations.md
---
# Optimizations
If you use Polars' lazy API, Polars will run several optimizations on your query. Some of them are
executed up front, others are determined just in time as the materialized data comes in.
Here is a non-complete overview of optimizations done by polars, what they do and how often they
run.
| Optimization | Explanation | runs |
| -------------------------- | ------------------------------------------------------------------------------------------------------------ | ----------------------------- |
| Predicate pushdown | Applies filters as early as possible/ at scan level. | 1 time |
| Projection pushdown | Select only the columns that are needed at the scan level. | 1 time |
| Slice pushdown | Only load the required slice from the scan level. Don't materialize sliced outputs (e.g. join.head(10)). | 1 time |
| Common subplan elimination | Cache subtrees/file scans that are used by multiple subtrees in the query plan. | 1 time |
| Simplify expressions | Various optimizations, such as constant folding and replacing expensive operations with faster alternatives. | until fixed point |
| Join ordering | Estimates the branches of joins that should be executed first in order to reduce memory pressure. | 1 time |
| Type coercion | Coerce types such that operations succeed and run on minimal required memory. | until fixed point |
| Cardinality estimation | Estimates cardinality in order to determine optimal group by strategy. | 0/n times; dependent on query |
---
polars/docs/source/user-guide/lazy/query-plan.md
---
# Query plan
For any lazy query Polars has both:
- a non-optimized plan with the set of steps code as we provided it and
- an optimized plan with changes made by the query optimizer
We can understand both the non-optimized and optimized query plans with visualization and by
printing them as text.
```python exec="on" result="text" session="user-guide/lazy/query-plan"
--8<-- "python/user-guide/lazy/query-plan.py:setup"
```
Below we consider the following query:
{{code_block('user-guide/lazy/query-plan','plan',[])}}
```python exec="on" session="user-guide/lazy/query-plan"
--8<-- "python/user-guide/lazy/query-plan.py:plan"
```
## Non-optimized query plan
### Graphviz visualization
To create visualizations of the query plan,
[Graphviz should be installed](https://graphviz.org/download/) and added to your PATH.
First we visualize the non-optimized plan by setting `optimized=False`.
{{code_block('user-guide/lazy/query-plan','showplan',['show_graph'])}}
```python exec="on" session="user-guide/lazy/query-plan"
--8<-- "python/user-guide/lazy/query-plan.py:createplan"
```
The query plan visualization should be read from bottom to top. In the visualization:
- each box corresponds to a stage in the query plan
- the `sigma` stands for `SELECTION` and indicates any filter conditions
- the `pi` stands for `PROJECTION` and indicates choosing a subset of columns
### Printed query plan
We can also print the non-optimized plan with `explain(optimized=False)`
{{code_block('user-guide/lazy/query-plan','describe',['explain'])}}
```python exec="on" session="user-guide/lazy/query-plan"
--8<-- "python/user-guide/lazy/query-plan.py:describe"
```
```text
FILTER [(col("comment_karma")) > (0)] FROM WITH_COLUMNS:
[col("name").str.uppercase()]
CSV SCAN data/reddit.csv
PROJECT */6 COLUMNS
```
The printed plan should also be read from bottom to top. This non-optimized plan is roughly equal
to:
- read from the `data/reddit.csv` file
- read all 6 columns (where the * wildcard in PROJECT \*/6 COLUMNS means take all columns)
- transform the `name` column to uppercase
- apply a filter on the `comment_karma` column
## Optimized query plan
Now we visualize the optimized plan with `show_graph`.
{{code_block('user-guide/lazy/query-plan','show',['show_graph'])}}
```python exec="on" session="user-guide/lazy/query-plan"
--8<-- "python/user-guide/lazy/query-plan.py:createplan2"
```
We can also print the optimized plan with `explain`
{{code_block('user-guide/lazy/query-plan','optimized',['explain'])}}
```text
WITH_COLUMNS:
[col("name").str.uppercase()]
CSV SCAN data/reddit.csv
PROJECT */6 COLUMNS
SELECTION: [(col("comment_karma")) > (0)]
```
The optimized plan is to:
- read the data from the Reddit CSV
- apply the filter on the `comment_karma` column while the CSV is being read line-by-line
- transform the `name` column to uppercase
In this case the query optimizer has identified that the `filter` can be applied while the CSV is
read from disk rather than reading the whole file into memory and then applying the filter. This
optimization is called _Predicate Pushdown_.
---
polars/docs/source/user-guide/lazy/schemas.md
---
# Schema
The schema of a Polars `DataFrame` or `LazyFrame` sets out the names of the columns and their
datatypes. You can see the schema with the `.collect_schema` method on a `DataFrame` or `LazyFrame`
{{code_block('user-guide/lazy/schema','schema',['LazyFrame'])}}
```python exec="on" result="text" session="user-guide/lazy/schemas"
--8<-- "python/user-guide/lazy/schema.py:setup"
--8<-- "python/user-guide/lazy/schema.py:schema"
```
The schema plays an important role in the lazy API.
## Type checking in the lazy API
One advantage of the lazy API is that Polars will check the schema before any data is processed.
This check happens when you execute your lazy query.
We see how this works in the following simple example where we call the `.round` expression on the
string column `foo`.
{{code_block('user-guide/lazy/schema','lazyround',['with_columns'])}}
The `.round` expression is only valid for columns with a numeric data type. Calling `.round` on a
string column means the operation will raise an `InvalidOperationError` when we evaluate the query
with `collect`. This schema check happens before the data is processed when we call `collect`.
{{code_block('user-guide/lazy/schema','typecheck',[])}}
```python exec="on" result="text" session="user-guide/lazy/schemas"
--8<-- "python/user-guide/lazy/schema.py:lazyround"
--8<-- "python/user-guide/lazy/schema.py:typecheck"
```
If we executed this query in eager mode the error would only be found once the data had been
processed in all earlier steps.
When we execute a lazy query Polars checks for any potential `InvalidOperationError` before the
time-consuming step of actually processing the data in the pipeline.
## The lazy API must know the schema
In the lazy API the Polars query optimizer must be able to infer the schema at every step of a query
plan. This means that operations where the schema is not knowable in advance cannot be used with the
lazy API.
The classic example of an operation where the schema is not knowable in advance is a `.pivot`
operation. In a `.pivot` the new column names come from data in one of the columns. As these column
names cannot be known in advance a `.pivot` is not available in the lazy API.
## Dealing with operations not available in the lazy API
If your pipeline includes an operation that is not available in the lazy API it is normally best to:
- run the pipeline in lazy mode up until that point
- execute the pipeline with `.collect` to materialize a `DataFrame`
- do the non-lazy operation on the `DataFrame`
- convert the output back to a `LazyFrame` with `.lazy` and continue in lazy mode
We show how to deal with a non-lazy operation in this example where we:
- create a simple `DataFrame`
- convert it to a `LazyFrame` with `.lazy`
- do a transformation using `.with_columns`
- execute the query before the pivot with `.collect` to get a `DataFrame`
- do the `.pivot` on the `DataFrame`
- convert back in lazy mode
- do a `.filter`
- finish by executing the query with `.collect` to get a `DataFrame`
{{code_block('user-guide/lazy/schema','lazyeager',['collect','lazy','pivot','filter'])}}
```python exec="on" result="text" session="user-guide/lazy/schemas"
--8<-- "python/user-guide/lazy/schema.py:lazyeager"
```
---
polars/docs/source/user-guide/lazy/sources_sinks.md
---
# Sources and sinks
## Scan
When using the `LazyFrame` API, it is important to favor `scan_*` (`scan_parquet`, `scan_csv`, etc.)
over `read_*`. A Polars `scan` is lazy and will delay execution until the query is collected. The
benefit of this, is that the Polars optimizer can push optimization into the readers. They can skip
reading columns and rows that aren't required. Another benefit is that, during streaming execution,
the engine already can process batches before the file is completely read.
## Sink
Sinks can execute a query and stream the results to storage (being disk or cloud). The benefit of
sinking data to storage is that you don't necessarily have to store all data in RAM, but can process
data in batches.
If we would want to convert many csv files to parquet, whilst dropping the missing data, we could do
something like the query below. We use a partitioning strategy that defines how many rows may be in
a single parquet file, before we generate a new file
```python
lf = scan_csv("my_dataset/*.csv").filter(pl.all().is_not_null())
lf.sink_parquet(
pl.PartitionMaxSize(
"my_table_{part}.parquet"
max_size=512_000
)
)
```
This will create the following files on disk:
```text
my_table_0.parquet
my_table_1.parquet
...
my_table_n.parquet
```
## Multiplexing sinks
Sinks can also multiplex. Meaning that we write to different sinks in a single query. In the code
snippet below, we take a `LazyFrame` and sink it into 2 sinks at the same time.
```python
# Some expensive computation
lf: LazyFrame
q1 = lf.sink_parquet(.., lazy=True)
q2 = lf.sink_ipc(.., lazy=True)
lf.collect_all([q1, q2])
```
---
polars/docs/source/user-guide/lazy/using.md
---
# Usage
With the lazy API, Polars doesn't run each query line-by-line but instead processes the full query
end-to-end. To get the most out of Polars it is important that you use the lazy API because:
- the lazy API allows Polars to apply automatic query optimization with the query optimizer
- the lazy API allows you to work with larger than memory datasets using streaming
- the lazy API can catch schema errors before processing the data
Here we see how to use the lazy API starting from either a file or an existing `DataFrame`.
## Using the lazy API from a file
In the ideal case we would use the lazy API right from a file as the query optimizer may help us to
reduce the amount of data we read from the file.
We create a lazy query from the Reddit CSV data and apply some transformations.
By starting the query with `pl.scan_csv` we are using the lazy API.
{{code_block('user-guide/lazy/using','dataframe',['scan_csv','with_columns','filter','col'])}}
A `pl.scan_` function is available for a number of file types including CSV, IPC, Parquet and JSON.
In this query we tell Polars that we want to:
- load data from the Reddit CSV file
- convert the `name` column to uppercase
- apply a filter to the `comment_karma` column
The lazy query will not be executed at this point. See this page on
[executing lazy queries](execution.md) for more on running lazy queries.
## Using the lazy API from a `DataFrame`
An alternative way to access the lazy API is to call `.lazy` on a `DataFrame` that has already been
created in memory.
{{code_block('user-guide/lazy/using','fromdf',['lazy'])}}
By calling `.lazy` we convert the `DataFrame` to a `LazyFrame`.
---
polars/docs/source/user-guide/io/bigquery.md
---
# Google BigQuery
To read or write from GBQ, additional dependencies are needed:
=== ":fontawesome-brands-python: Python"
```shell
$ pip install google-cloud-bigquery
```
## Read
We can load a query into a `DataFrame` like this:
{{code_block('user-guide/io/bigquery','read',['from_arrow'])}}
## Write
{{code_block('user-guide/io/bigquery','write',[])}}
---
polars/docs/source/user-guide/io/cloud-storage.md
---
# Cloud storage
Polars can read and write to AWS S3, Azure Blob Storage and Google Cloud Storage. The API is the
same for all three storage providers.
To read from cloud storage, additional dependencies may be needed depending on the use case and
cloud storage provider:
=== ":fontawesome-brands-python: Python"
```shell
$ pip install fsspec s3fs adlfs gcsfs
```
=== ":fontawesome-brands-rust: Rust"
```shell
$ cargo add aws_sdk_s3 aws_config tokio --features tokio/full
```
## Reading from cloud storage
Polars supports reading Parquet, CSV, IPC and NDJSON files from cloud storage:
{{code_block('user-guide/io/cloud-storage','read_parquet',['read_parquet','read_csv','read_ipc'])}}
## Scanning from cloud storage with query optimisation
Using `pl.scan_*` functions to read from cloud storage can benefit from
[predicate and projection pushdowns](../lazy/optimizations.md), where the query optimizer will apply
them before the file is downloaded. This can significantly reduce the amount of data that needs to
be downloaded. The query evaluation is triggered by calling `collect`.
{{code_block('user-guide/io/cloud-storage','scan_parquet_query',[])}}
## Cloud authentication
Polars is able to automatically load default credential configurations for some cloud providers. For
cases when this does not happen, it is possible to manually configure the credentials for Polars to
use for authentication. This can be done in a few ways:
### Using `storage_options`:
- Credentials can be passed as configuration keys in a dict with the `storage_options` parameter:
{{code_block('user-guide/io/cloud-storage','scan_parquet_storage_options_aws',['scan_parquet'])}}
### Using one of the available `CredentialProvider*` utility classes
- There may be a utility class `pl.CredentialProvider*` that provides the required authentication
functionality. For example, `pl.CredentialProviderAWS` supports selecting AWS profiles, as well as
assuming an IAM role:
{{code_block('user-guide/io/cloud-storage','credential_provider_class',['scan_parquet',
'CredentialProviderAWS'])}}
### Using a custom `credential_provider` function
- Some environments may require custom authentication logic (e.g. AWS IAM role-chaining). For these
cases a Python function can be provided for Polars to use to retrieve credentials:
{{code_block('user-guide/io/cloud-storage','credential_provider_custom_func',['scan_parquet'])}}
- Example for Azure:
{{code_block('user-guide/io/cloud-storage','credential_provider_custom_func_azure',['scan_parquet',
'CredentialProviderAzure'])}}
## Scanning with PyArrow
We can also scan from cloud storage using PyArrow. This is particularly useful for partitioned
datasets such as Hive partitioning.
We first create a PyArrow dataset and then create a `LazyFrame` from the dataset.
{{code_block('user-guide/io/cloud-storage','scan_pyarrow_dataset',['scan_pyarrow_dataset'])}}
## Writing to cloud storage
We can write a `DataFrame` to cloud storage in Python using s3fs for S3, adlfs for Azure Blob
Storage and gcsfs for Google Cloud Storage. In this example, we write a Parquet file to S3.
{{code_block('user-guide/io/cloud-storage','write_parquet',['write_parquet'])}}
---
polars/docs/source/user-guide/io/csv.md
---
# CSV
## Read & write
Reading a CSV file should look familiar:
{{code_block('user-guide/io/csv','read',['read_csv'])}}
Writing a CSV file is similar with the `write_csv` function:
{{code_block('user-guide/io/csv','write',['write_csv'])}}
## Scan
Polars allows you to _scan_ a CSV input. Scanning delays the actual parsing of the file and instead
returns a lazy computation holder called a `LazyFrame`.
{{code_block('user-guide/io/csv','scan',['scan_csv'])}}
If you want to know why this is desirable, you can read more about these Polars optimizations
[here](../concepts/lazy-api.md).
---
polars/docs/source/user-guide/io/database.md
---
# Databases
## Read from a database
Polars can read from a database using the `pl.read_database_uri` and `pl.read_database` functions.
### Difference between `read_database_uri` and `read_database`
Use `pl.read_database_uri` if you want to specify the database connection with a connection string
called a `uri`. For example, the following snippet shows a query to read all columns from the `foo`
table in a Postgres database where we use the `uri` to connect:
{{code_block('user-guide/io/database','read_uri',['read_database_uri'])}}
On the other hand, use `pl.read_database` if you want to connect via a connection engine created
with a library like SQLAlchemy.
{{code_block('user-guide/io/database','read_cursor',['read_database'])}}
Note that `pl.read_database_uri` is likely to be faster than `pl.read_database` if you are using a
SQLAlchemy or DBAPI2 connection as these connections may load the data row-wise into Python before
copying the data again to the column-wise Apache Arrow format.
### Engines
Polars doesn't manage connections and data transfer from databases by itself. Instead, external
libraries (known as _engines_) handle this.
When using `pl.read_database`, you specify the engine when you create the connection object. When
using `pl.read_database_uri`, you can specify one of two engines to read from the database:
- [ConnectorX](https://github.com/sfu-db/connector-x) and
- [ADBC](https://arrow.apache.org/docs/format/ADBC.html)
Both engines have native support for Apache Arrow and so can read data directly into a Polars
`DataFrame` without copying the data.
#### ConnectorX
ConnectorX is the default engine and
[supports numerous databases](https://github.com/sfu-db/connector-x#sources) including Postgres,
Mysql, SQL Server and Redshift. ConnectorX is written in Rust and stores data in Arrow format to
allow for zero-copy to Polars.
To read from one of the supported databases with `ConnectorX` you need to activate the additional
dependency `ConnectorX` when installing Polars or install it manually with
```shell
$ pip install connectorx
```
#### ADBC
ADBC (Arrow Database Connectivity) is an engine supported by the Apache Arrow project. ADBC aims to
be both an API standard for connecting to databases and libraries implementing this standard in a
range of languages.
It is still early days for ADBC so support for different databases is limited. At present, drivers
for ADBC are only available for [Postgres](https://pypi.org/project/adbc-driver-postgresql/),
[SQLite](https://pypi.org/project/adbc-driver-sqlite/) and
[Snowflake](https://pypi.org/project/adbc-driver-snowflake/). To install ADBC, you need to install
the driver for your database. For example, to install the driver for SQLite, you run:
```shell
$ pip install adbc-driver-sqlite
```
As ADBC is not the default engine, you must specify the engine as an argument to
`pl.read_database_uri`.
{{code_block('user-guide/io/database','adbc',['read_database_uri'])}}
## Write to a database
We can write to a database with Polars using the `pl.write_database` function.
### Engines
As with reading from a database above, Polars uses an _engine_ to write to a database. The currently
supported engines are:
- [SQLAlchemy](https://www.sqlalchemy.org/) and
- Arrow Database Connectivity (ADBC)
#### SQLAlchemy
With the default engine SQLAlchemy you can write to any database supported by SQLAlchemy. To use
this engine you need to install SQLAlchemy and Pandas
```shell
$ pip install SQLAlchemy pandas
```
In this example, we write the `DataFrame` to a table called `records` in the database
{{code_block('user-guide/io/database','write',['write_database'])}}
In the SQLAlchemy approach, Polars converts the `DataFrame` to a Pandas `DataFrame` backed by
PyArrow and then uses SQLAlchemy methods on a Pandas `DataFrame` to write to the database.
#### ADBC
ADBC can also be used to write to a database. Writing is supported for the same databases that
support reading with ADBC. As shown above, you need to install the appropriate ADBC driver for your
database.
{{code_block('user-guide/io/database','write_adbc',['write_database'])}}
---
polars/docs/source/user-guide/io/excel.md
---
# Excel
Polars can read and write to Excel files from Python. From a performance perspective, we recommend
using other formats if possible, such as Parquet or CSV files.
## Read
Polars does not have a native Excel reader. Instead, it uses an external library called an "engine"
to parse Excel files into a form that Polars can parse. The available engines are:
- fastexcel: This engine is based on the Rust [calamine](https://github.com/tafia/calamine) crate
and is (by far) the fastest reader.
- xlsx2csv: This reader parses the .xlsx file to an in-memory CSV that Polars then reads with its
own CSV reader.
- openpyxl: Typically slower than xls2csv, but can provide more flexibility for files that are
difficult to parse.
We recommend working with the default fastexcel engine. The xlsx2csv and openpyxl engines are slower
but may have more features for parsing tricky data. These engines may be helpful if the fastexcel
reader does not work for a specific Excel file.
To use one of these engines, the appropriate Python package must be installed as an additional
dependency.
=== ":fontawesome-brands-python: Python"
```shell
$ pip install fastexcel xlsx2csv openpyxl
```
The default engine for reading .xslx files is fastexcel. This engine uses the Rust calamine crate to
read .xslx files into an Apache Arrow in-memory representation that Polars can read without needing
to copy the data.
{{code_block('user-guide/io/excel','read',['read_excel'])}}
We can specify the sheet name to read with the `sheet_name` argument. If we do not specify a sheet
name, the first sheet will be read.
{{code_block('user-guide/io/excel','read_sheet_name',['read_excel'])}}
## Write
We need the xlswriter library installed as an additional dependency to write to Excel files.
=== ":fontawesome-brands-python: Python"
```shell
$ pip install xlsxwriter
```
Writing to Excel files is not currently available in Rust Polars, though it is possible to
[use this crate](https://docs.rs/crate/xlsxwriter/latest) to write to Excel files from Rust.
Writing a `DataFrame` to an Excel file is done with the `write_excel` method:
{{code_block('user-guide/io/excel','write',['write_excel'])}}
The name of the worksheet can be specified with the `worksheet` argument.
{{code_block('user-guide/io/excel','write_sheet_name',['write_excel'])}}
Polars can create rich Excel files with multiple sheets and formatting. For more details, see the
API docs for `write_excel`.
---
polars/docs/source/user-guide/io/hive.md
---
## Scanning hive partitioned data
Polars supports scanning hive partitioned parquet and IPC datasets, with planned support for other
formats in the future.
Hive partition parsing is enabled by default if `scan_parquet` receives a single directory path,
otherwise it is disabled by default. This can be explicitly configured using the `hive_partitioning`
parameter.
### Scanning a hive directory
For this example the following directory structure is used:
```python exec="on" result="text" session="user-guide/io/hive"
--8<-- "python/user-guide/io/hive.py:init_paths"
```
Simply pass the directory to `scan_parquet`, and all files will be loaded with the hive parts in the
path included in the output:
{{code_block('user-guide/io/hive','scan_dir',['scan_parquet'])}}
```python exec="on" result="text" session="user-guide/io/hive"
--8<-- "python/user-guide/io/hive.py:scan_dir"
```
### Handling mixed files
Passing a directory to `scan_parquet` may not work if there are files with different extensions in
the directory.
For this example the following directory structure is used:
```python exec="on" result="text" session="user-guide/io/hive"
--8<-- "python/user-guide/io/hive.py:show_mixed_paths"
```
{{code_block('user-guide/io/hive','scan_dir_err',['scan_parquet'])}}
The above fails as `description.txt` is not a valid parquet file:
```python exec="on" result="text" session="user-guide/io/hive"
--8<-- "python/user-guide/io/hive.py:scan_dir_err"
```
In this situation, a glob pattern can be used to be more specific about which files to load. Note
that `hive_partitioning` must explicitly set to `True`:
{{code_block('user-guide/io/hive','scan_glob',['scan_parquet'])}}
```python exec="on" result="text" session="user-guide/io/hive"
--8<-- "python/user-guide/io/hive.py:scan_glob"
```
### Scanning file paths with hive parts
`hive_partitioning` is not enabled by default for file paths:
{{code_block('user-guide/io/hive','scan_file_no_hive',['scan_parquet'])}}
```python exec="on" result="text" session="user-guide/io/hive"
--8<-- "python/user-guide/io/hive.py:scan_file_no_hive"
```
Pass `hive_partitioning=True` to enable hive partition parsing:
{{code_block('user-guide/io/hive','scan_file_hive',['scan_parquet'])}}
```python exec="on" result="text" session="user-guide/io/hive"
--8<-- "python/user-guide/io/hive.py:scan_file_hive"
```
## Writing hive partitioned data
> Note: The following functionality is considered _unstable_, and is subject to change.
Polars supports writing hive partitioned parquet datasets, with planned support for other formats.
### Example
For this example the following DataFrame is used:
{{code_block('user-guide/io/hive','write_parquet_partitioned_show_data',[])}}
```python exec="on" result="text" session="user-guide/io/hive"
--8<-- "python/user-guide/io/hive.py:write_parquet_partitioned_show_data"
```
We will write it to a hive-partitioned parquet dataset, partitioned by the columns `a` and `b`:
{{code_block('user-guide/io/hive','write_parquet_partitioned',['write_parquet'])}}
```python exec="on" result="text" session="user-guide/io/hive"
--8<-- "python/user-guide/io/hive.py:write_parquet_partitioned"
```
The output is a hive partitioned parquet dataset with the following paths:
```python exec="on" result="text" session="user-guide/io/hive"
--8<-- "python/user-guide/io/hive.py:write_parquet_partitioned_show_paths"
```
---
polars/docs/source/user-guide/io/hugging-face.md
---
# Hugging Face
## Scanning datasets from Hugging Face
All cloud-enabled scan functions, and their `read_` counterparts transparently support scanning from
Hugging Face:
| Scan | Read |
| --------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| [scan_parquet](https://docs.pola.rs/api/python/stable/reference/api/polars.scan_parquet.html) | [read_parquet](https://docs.pola.rs/api/python/stable/reference/api/polars.read_parquet.html) |
| [scan_csv](https://docs.pola.rs/api/python/stable/reference/api/polars.scan_csv.html) | [read_csv](https://docs.pola.rs/api/python/stable/reference/api/polars.read_csv.html) |
| [scan_ndjson](https://docs.pola.rs/api/python/stable/reference/api/polars.scan_ndjson.html) | [read_ndjson](https://docs.pola.rs/api/python/stable/reference/api/polars.read_ndjson.html) |
| [scan_ipc](https://docs.pola.rs/api/python/stable/reference/api/polars.scan_ipc.html) | [read_ipc](https://docs.pola.rs/api/python/stable/reference/api/polars.read_ipc.html) |
### Path format
To scan from Hugging Face, a `hf://` path can be passed to the scan functions. The `hf://` path
format is defined as `hf://BUCKET/REPOSITORY@REVISION/PATH`, where:
- `BUCKET` is one of `datasets` or `spaces`
- `REPOSITORY` is the location of the repository, this is usually in the format of
`username/repo_name`. A branch can also be optionally specified by appending `@branch`
- `REVISION` is the name of the branch (or commit) to use. This is optional and defaults to `main`
if not given.
- `PATH` is a file or directory path, or a glob pattern from the repository root.
Example `hf://` paths:
| Path | Path components |
| ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| hf://datasets/nameexhaustion/polars-docs/iris.csv | Bucket: datasetsRepository: nameexhaustion/polars-docsBranch: mainPath: iris.csv [Web URL](https://huggingface.co/datasets/nameexhaustion/polars-docs/tree/main/) |
| hf://datasets/nameexhaustion/polars-docs@foods/\*.csv | Bucket: datasetsRepository: nameexhaustion/polars-docsBranch: foodsPath: \*.csv [Web URL](https://huggingface.co/datasets/nameexhaustion/polars-docs/tree/foods/) |
| hf://datasets/nameexhaustion/polars-docs/hive_dates/ | Bucket: datasetsRepository: nameexhaustion/polars-docsBranch: mainPath: hive_dates/ [Web URL](https://huggingface.co/datasets/nameexhaustion/polars-docs/tree/main/hive_dates/) |
| hf://spaces/nameexhaustion/polars-docs/orders.feather | Bucket: spacesRepository: nameexhaustion/polars-docsBranch: mainPath: orders.feather [Web URL](https://huggingface.co/spaces/nameexhaustion/polars-docs/tree/main/) |
### Authentication
A Hugging Face API key can be passed to Polars to access private locations using either of the
following methods:
- Passing a `token` in `storage_options` to the scan function, e.g.
`scan_parquet(..., storage_options={'token': ''})`
- Setting the `HF_TOKEN` environment variable, e.g. `export HF_TOKEN=`
### Examples
#### CSV
```python exec="on" result="text" session="user-guide/io/hugging-face"
--8<-- "python/user-guide/io/hugging-face.py:setup"
```
{{code_block('user-guide/io/hugging-face','scan_iris_csv',['scan_csv'])}}
```python exec="on" result="text" session="user-guide/io/hugging-face"
--8<-- "python/user-guide/io/hugging-face.py:scan_iris_repr"
```
See this file at
[https://huggingface.co/datasets/nameexhaustion/polars-docs/blob/main/iris.csv](https://huggingface.co/datasets/nameexhaustion/polars-docs/blob/main/iris.csv)
#### NDJSON
{{code_block('user-guide/io/hugging-face','scan_iris_ndjson',['scan_ndjson'])}}
```python exec="on" result="text" session="user-guide/io/hugging-face"
--8<-- "python/user-guide/io/hugging-face.py:scan_iris_repr"
```
See this file at
[https://huggingface.co/datasets/nameexhaustion/polars-docs/blob/main/iris.jsonl](https://huggingface.co/datasets/nameexhaustion/polars-docs/blob/main/iris.jsonl)
#### Parquet
{{code_block('user-guide/io/hugging-face','scan_parquet_hive_repr',['scan_parquet'])}}
```python exec="on" result="text" session="user-guide/io/hugging-face"
--8<-- "python/user-guide/io/hugging-face.py:scan_parquet_hive_repr"
```
See this folder at
[https://huggingface.co/datasets/nameexhaustion/polars-docs/tree/main/hive_dates/](https://huggingface.co/datasets/nameexhaustion/polars-docs/tree/main/hive_dates/)
#### IPC
{{code_block('user-guide/io/hugging-face','scan_ipc',['scan_ipc'])}}
```python exec="on" result="text" session="user-guide/io/hugging-face"
--8<-- "python/user-guide/io/hugging-face.py:scan_ipc_repr"
```
See this file at
[https://huggingface.co/spaces/nameexhaustion/polars-docs/blob/main/orders.feather](https://huggingface.co/spaces/nameexhaustion/polars-docs/blob/main/orders.feather)
---
polars/docs/source/user-guide/io/index.md
---
# IO
Reading and writing your data is crucial for a DataFrame library. In this chapter you will learn
more on how to read and write to different file formats that are supported by Polars.
- [CSV](csv.md)
- [Excel](excel.md)
- [Parquet](parquet.md)
- [Json](json.md)
- [Multiple](multiple.md)
- [Hive](hive.md)
- [Database](database.md)
- [Cloud storage](cloud-storage.md)
- [Google Big Query](bigquery.md)
- [Hugging Face](hugging-face.md)
---
polars/docs/source/user-guide/io/json.md
---
# JSON files
Polars can read and write both standard JSON and newline-delimited JSON (NDJSON).
## Read
### JSON
Reading a JSON file should look familiar:
{{code_block('user-guide/io/json','read',['read_json'])}}
### Newline Delimited JSON
JSON objects that are delimited by newlines can be read into Polars in a much more performant way
than standard json.
Polars can read an NDJSON file into a `DataFrame` using the `read_ndjson` function:
{{code_block('user-guide/io/json','readnd',['read_ndjson'])}}
## Write
{{code_block('user-guide/io/json','write',['write_json','write_ndjson'])}}
## Scan
Polars allows you to _scan_ a JSON input **only for newline delimited json**. Scanning delays the
actual parsing of the file and instead returns a lazy computation holder called a `LazyFrame`.
{{code_block('user-guide/io/json','scan',['scan_ndjson'])}}
---
polars/docs/source/user-guide/io/multiple.md
---
## Dealing with multiple files.
Polars can deal with multiple files differently depending on your needs and memory strain.
Let's create some files to give us some context:
{{code_block('user-guide/io/multiple','create',['write_csv'])}}
## Reading into a single `DataFrame`
To read multiple files into a single `DataFrame`, we can use globbing patterns:
{{code_block('user-guide/io/multiple','read',['read_csv'])}}
```python exec="on" result="text" session="user-guide/io/multiple"
--8<-- "python/user-guide/io/multiple.py:create"
--8<-- "python/user-guide/io/multiple.py:read"
```
To see how this works we can take a look at the query plan. Below we see that all files are read
separately and concatenated into a single `DataFrame`. Polars will try to parallelize the reading.
{{code_block('user-guide/io/multiple','graph',['show_graph'])}}
```python exec="on" session="user-guide/io/multiple"
--8<-- "python/user-guide/io/multiple.py:creategraph"
```
## Reading and processing in parallel
If your files don't have to be in a single table you can also build a query plan for each file and
execute them in parallel on the Polars thread pool.
All query plan execution is embarrassingly parallel and doesn't require any communication.
{{code_block('user-guide/io/multiple','glob',['scan_csv'])}}
```python exec="on" result="text" session="user-guide/io/multiple"
--8<-- "python/user-guide/io/multiple.py:glob"
```
---
polars/docs/source/user-guide/io/parquet.md
---
# Parquet
Loading or writing [`Parquet` files](https://parquet.apache.org/) is lightning fast as the layout of
data in a Polars `DataFrame` in memory mirrors the layout of a Parquet file on disk in many
respects.
Unlike CSV, Parquet is a columnar format. This means that the data is stored in columns rather than
rows. This is a more efficient way of storing data as it allows for better compression and faster
access to data.
## Read
We can read a `Parquet` file into a `DataFrame` using the `read_parquet` function:
{{code_block('user-guide/io/parquet','read',['read_parquet'])}}
## Write
{{code_block('user-guide/io/parquet','write',['write_parquet'])}}
## Scan
Polars allows you to _scan_ a `Parquet` input. Scanning delays the actual parsing of the file and
instead returns a lazy computation holder called a `LazyFrame`.
{{code_block('user-guide/io/parquet','scan',['scan_parquet'])}}
If you want to know why this is desirable, you can read more about those Polars optimizations
[here](../concepts/lazy-api.md).
When we scan a `Parquet` file stored in the cloud, we can also apply predicate and projection
pushdowns. This can significantly reduce the amount of data that needs to be downloaded. For
scanning a Parquet file in the cloud, see
[Cloud storage](cloud-storage.md/#scanning-from-cloud-storage-with-query-optimisation).
---
polars/docs/source/user-guide/io/sheets_colab.md
---
# Google Sheets (via Colab)
Google Colab provides a utility class to read from and write to Google Sheets.
## Opening and reading from a sheet
We can open existing sheets by initializing `sheets.InteractiveSheet` with either:
- the `url` parameter, for example
https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/
- the `sheet_id` parameter for example 1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms
By default the left-most worksheets will be used, we can change this by providing either
`worksheet_id` or `worksheet_name`.
The first time in each session that we use `InteractiveSheet` we will need to give Colab permission
to edit our drive assets on our behalf.
{{code_block('user-guide/io/sheets_colab','open',[])}}
## Creating a new sheet
When you don't provide the source of the spreadsheet one will be created for you.
{{code_block('user-guide/io/sheets_colab','create_title',[])}}
When you pass the `df` parameter the data will be written to the sheet immediately.
{{code_block('user-guide/io/sheets_colab','create_df',[])}}
## Writing to a sheet
By default the `update` method will clear the worksheet and write the dataframe in the top left
corner.
{{code_block('user-guide/io/sheets_colab','update',[])}}
We can modify where the data is written with the `location` parameter and whether the worksheet is
cleared before with `clear`.
{{code_block('user-guide/io/sheets_colab','update_loc',[])}}
A good way to write multiple dataframes onto a worksheet in a loop is:
{{code_block('user-guide/io/sheets_colab','update_loop',[])}}
This clears the worksheet then writes the dataframes next to each other, one every five columns.
---
