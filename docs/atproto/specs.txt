atproto/src/app/[locale]/specs/record-key/en.mdx
---
export const metadata = {
  title: 'Record Key',
  description:
    'Identifier for individual records in a collection',
}

# Record Keys

A **Record Key** (sometimes shortened to `rkey`) is used to name and reference an individual record within the same collection of an atproto repository. It ends up as a segment in AT URIs, and in the repo MST path. {{ className: 'lead' }}

A few different Record Key naming schemes are supported. Every record Lexicon schema will indicate which of the record key types should be used, depending on the needs and semantics of the record collection. {{ className: 'lead' }}


### Record Key Type: `tid`

This is the most common record naming scheme, using [TID ("timestamp identifier") syntax](/specs/tid), such as `3jzfcijpj2z2a`. TIDs are usually generated from a local clock at the time of record creation, with some additional mechanisms to ensure they always increment and are not reused or duplicated with the same collection in a given repository.

The original creation timestamp of a record can be inferred if it has a TID record key, but remember that these keys can be specified by the end user and could have any value, so they should not be trusted.

The same TID may be used for records in different collections in the same repository. This usually indicates some relationship between the two records (eg, a "sidecar" extension record).

An early motivation for the TID scheme was to provide a loose temporal ordering of records, which improves storage efficiency of the repository data structure (MST).

### Record Key Type: `nsid`

For cases where the record key must be a valid [NSID](/specs/nsid).

### Record Key Type: `literal:<value>`

This key type is used when there should be only a single record in the collection, with a fixed, well-known Record Key.

The most common value is `self`, specified as `literal:self` in a Lexicon schema.

### Record Key Type: `any`

Any string meeting the overall Record Key schema requirements (see below) is allowed. This is the most flexible type of Record Key.

This may be used to encode semantics in the name, for example, a domain name, integer, or (transformed) AT URI. This enables de-duplication and known-URI lookups.


### Record Key Syntax

Lexicon string type: `record-key`

Regardless of the type, Record Keys must fulfill some baseline syntax constraints:

- restricted to a subset of ASCII characters — the allowed characters are alphanumeric (`A-Za-z0-9`), period, dash, underscore, colon, or tilde (`.-_:~`)
- must have at least 1 and at most 512 characters
- the specific record key values `.` and `..` are not allowed
- must be a permissible part of repository MST path string (the above constraints satisfy this condition)
- must be permissible to include in a path component of a URI (following RFC-3986, section 3.3).  The above constraints satisfy this condition, by matching the "unreserved" characters allowed in generic URI paths.

Record Keys are case-sensitive.


### Examples

Valid Record Keys:

```
3jui7kd54zh2y
self
example.com
~1.2-3_
dHJ1ZQ
pre:fix
_
```

Invalid Record Keys:

```
alpha/beta
.
..
#extra
@handle
any space
any+space
number[3]
number(3)
"quote"
dHJ1ZQ==
```

### Usage and Implementation Guidelines

Implementations should not rely on global uniqueness of TIDs, and should not trust TID timestamps as actual record creation timestamps. Record Keys are "user-controlled data" and may be arbitrarily selected by hostile accounts.

Most software processing repositories and records should be agnostic to the Record Key type and values, and usually treat them as simple strings. For example, relying on TID keys to decode as `base32` into a unique `uint64` makes it tempting to rely on this for use as a database key, but doing so is not resilient to key format changes and is discouraged.

Note that in the context of a repository, the same Record Key value may be used under multiple collections. The tuple of `(did, rkey)` is not unique; the tuple `(did, collection, rkey)` is unique.

As a best practice, keep key paths to under 80 characters in virtually all situations.

The colon character was de-facto allowed in the reference implementation in 2023, and the spec was updated to allow this character in February 2024.

Note that "most" DIDs work as record keys, but that the full DID W3C specification allows DIDs with additional characters. This means that DIDs could be used as record keys at time of writing, and this will work with current "blessed" DID methods (note that `did:web` is restricted in atproto to only full domains, not paths or port specification), but that this could break in the future with different DID methods or DID features (like `did:web` paths) allowed.

While Record Keys are case-sensitive, it is a recommended practice to use all-lower-case Record Keys to avoid confusion and maximize possible re-use in case-insensitive contexts.

### Possible Future Changes

The constraints on Record Key syntax may be relaxed in the future to allow non-ASCII Unicode characters. Record keys will always be valid Unicode, never relaxed to allow arbitrary byte-strings.

Additional Record Key types may be defined.

The maximum length may be tweaked.

The `%` character is reserved for possible use with URL encoding, but note that such encoding is not currently supported.

Additional constraints on the generic syntax may be added. For example, requiring at least one alphanumeric character.


---
atproto/src/app/[locale]/specs/record-key/ko.mdx
---
export const metadata = {
  title: '레코드 키',
  description:
    'AT 프로토콜 레포지토리 내 컬렉션의 개별 레코드를 식별하는 식별자',
}

# 레코드 키

**레코드 키**(때로는 `rkey`로 축약됨)는 atproto 레포지토리 내 동일 컬렉션의 개별 레코드를 명명하고 참조하는 데 사용됩니다. 이 값은 AT URI의 한 부분이 되며, 레포지토리의 MST(머클 서명 트리) 경로에도 포함됩니다. {{ className: 'lead' }}

여러 가지 레코드 키 명명 방식이 지원됩니다. 모든 레코드 Lexicon 스키마는 레코드 컬렉션의 필요와 의미에 따라 사용해야 하는 레코드 키 타입을 지정합니다. {{ className: 'lead' }}

### 레코드 키 타입: `tid`

가장 일반적인 레코드 명명 방식으로, [TID ("timestamp identifier") 문법](/specs/tid)을 사용합니다. 예를 들어 `3jzfcijpj2z2a`와 같이 표현됩니다. TID는 보통 레코드 생성 시점의 로컬 시계를 기반으로 생성되며, 동일 컬렉션 내에서 중복 또는 재사용되지 않도록 추가 메커니즘이 적용됩니다.

TID 레코드 키를 사용하는 경우 레코드의 원래 생성 시점을 유추할 수 있지만, 이러한 키는 최종 사용자가 지정할 수 있어 임의의 값이 될 수 있으므로 절대 신뢰해서는 안 됩니다.

동일한 TID가 동일 레포지토리의 다른 컬렉션의 레코드에도 사용될 수 있습니다. 이는 두 레코드 간의 관계(예: "사이드카" 확장 레코드)를 나타내는 경우가 많습니다.

TID 방식의 초기 동기는 레코드의 느슨한 시간 순서를 제공하여 레포지토리 데이터 구조(MST)의 저장 효율성을 높이기 위함이었습니다.

### 레코드 키 타입: `nsid`

레코드 키가 유효한 [NSID](/specs/nsid)여야 하는 경우에 사용합니다.

### 레코드 키 타입: `literal:<value>`

컬렉션 내에 단 하나의 레코드만 존재해야 하며, 고정되고 잘 알려진 레코드 키가 필요한 경우 사용합니다.

가장 일반적인 값은 `self`이며, Lexicon 스키마에서는 `literal:self`로 지정됩니다.

### 레코드 키 타입: `any`

전체 레코드 키 스키마 요구사항(아래 참조)을 충족하는 임의의 문자열이 허용됩니다. 가장 유연한 타입의 레코드 키입니다.

이 방식은 도메인 이름, 정수 또는 (변환된) AT URI 등 이름에 의미를 부여하기 위해 사용될 수 있습니다. 이를 통해 중복 제거 및 알려진 URI 조회가 가능합니다.

### 레코드 키 문법

Lexicon 문자열 타입: `record-key`

타입에 관계없이, 레코드 키는 다음 기본 문법 제약을 충족해야 합니다:

- ASCII 문자 중 제한된 일부만 사용 — 허용 문자는 영문 대소문자 및 숫자(`A-Za-z0-9`), 마침표, 대시, 밑줄, 콜론, 물결표(`.-_:~`)
- 최소 1자, 최대 512자
- 특정 값 `.` 와 `..` 는 허용되지 않음
- 레포지토리 MST 경로 문자열의 일부로 사용할 수 있어야 함 (위의 제약 조건을 충족)
- URI의 경로 구성 요소에 포함될 수 있어야 함 (RFC-3986, 섹션 3.3 참조). 위의 제약 조건은 URI 경로의 "unreserved" 문자와 일치함

레코드 키는 대소문자를 구분합니다.

### 예제

**유효한 레코드 키:**

```
3jui7kd54zh2y
self
example.com
~1.2-3_
dHJ1ZQ
pre:fix
_
```

**유효하지 않은 레코드 키:**

```
alpha/beta
.
..
#extra
@handle
any space
any+space
number[3]
number(3)
"quote"
dHJ1ZQ==
```

### 사용 및 구현 가이드라인

구현체는 TID의 전역 고유성에 의존해서는 안 되며, TID에 내포된 타임스탬프를 실제 레코드 생성 시간으로 신뢰해서는 안 됩니다. 레코드 키는 "사용자 제어 데이터"이므로, 악의적인 계정에 의해 임의로 선택될 수 있습니다.

대부분의 레포지토리 및 레코드 처리를 위한 소프트웨어는 레코드 키 타입과 값을 단순 문자열로 취급해야 하며, 특별한 의미를 부여하지 않아야 합니다. 예를 들어, TID 키를 `base32`로 디코딩하여 고유 `uint64` 값으로 사용하는 것을 데이터베이스 키로 활용하는 것은 키 포맷 변경에 취약하므로 권장되지 않습니다.

레포지토리 내에서 동일한 레코드 키 값이 여러 컬렉션에서 사용될 수 있음을 유의하세요. `(did, rkey)` 조합은 고유하지 않고, `(did, collection, rkey)` 조합이 유일합니다.

최선의 관례로, 대부분의 상황에서 키 경로 길이는 80자 미만으로 유지하는 것이 좋습니다.

콜론(`:`) 문자는 2023년 기준 참조 구현에서 사실상 허용되었으며, 2024년 2월에 스펙이 업데이트되어 공식적으로 허용되었습니다.

대부분의 DID는 레코드 키로 사용할 수 있으나, DID W3C 전체 스펙은 추가 문자를 허용하므로, 현재는 작동하더라도 향후 DID 메서드나 기능(예: 경로 포함 `did:web`)이 변경되면 문제가 발생할 수 있습니다.

레코드 키는 대소문자를 구분하지만, 혼동을 피하고 대소문자 구분이 없는 환경에서도 재사용 가능하도록 모두 소문자로 사용하는 것이 권장됩니다.

### 향후 변경 가능 사항

- 레코드 키 문법에 대한 제약은 향후 비 ASCII 유니코드 문자 허용으로 완화될 수 있습니다. 단, 레코드 키는 항상 유효한 유니코드여야 하며, 임의의 바이트 문자열은 허용되지 않습니다.
- 추가적인 레코드 키 타입이 정의될 수 있습니다.
- 최대 길이가 조정될 수 있습니다.
- `%` 문자는 URL 인코딩과 관련하여 예약되어 있으나, 현재 인코딩은 지원되지 않습니다.
- 전체 문법에 대한 추가 제약(예: 최소 한 개 이상의 영문자 또는 숫자 포함)이 추가될 수 있습니다.


---
atproto/src/app/[locale]/specs/record-key/page.tsx
---
export const metadata = {
  title: 'Record Key',
  description: 'Identifier for individual records in a collection',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/at-uri-scheme/en.mdx
---
export const metadata = {
  title: 'AT URI scheme (at://)',
  description:
    'A URI scheme for addressing ATP repository data.',
}

# AT URI Scheme (at://)

The AT URI scheme (`at://`) makes it easy to reference individual records in a specific repository, identified by either DID or handle. AT URIs can also be used to reference a collection within a repository, or an entire repository (aka, an identity). {{ className: 'lead' }}

Both of these AT URIs reference the same record in the same repository; one uses the account's DID, and one uses the account's handle.

- `at://did:plc:44ybard66vv44zksje25o7dz/app.bsky.feed.post/3jwdwj2ctlk26`
- `at://bnewbold.bsky.team/app.bsky.feed.post/3jwdwj2ctlk26`

<Note>
**Caveats for Handle-based AT URIs**

AT URIs referencing handles are not durable.

If a user changes their handle, any AT URIs using that handle will become invalid and could potentially point to a record in another repo if the handle is reused.

AT URIs are not content-addressed, so the _contents_ of the record they refer to may also change over time.
</Note>

### Structure

The full, general structure of an AT URI is:

```text
"at://" AUTHORITY [ PATH ] [ "?" QUERY ] [ "#" FRAGMENT ]
```

The **authority** part of the URI can be either a handle or a DID, indicating the identity associated with the repository. Note that a handle can refer to different DIDs (and thus different repositories) over time. See discussion below about strong references, and in "Usage and Implementation".

In current atproto Lexicon use, the **query** and **fragment** parts are not yet supported, and only a fixed pattern of paths are allowed:

```text
"at://" AUTHORITY [ "/" COLLECTION [ "/" RKEY ] ]
```

The **authority** section is required, must be normalized, and if a DID must be one of the "blessed" DID methods. The optional **collection** part of the path must be a normalized [NSID](./nsid). The optional **rkey** part of the path must be a valid [Record Key](./record-key).

An AT URI pointing to a specific record in a repository is not a *strong* reference, in that it is not content-addressed. The record may change or be removed over time, or the DID itself may be deleted or unavailable. For `did:web`, control of the DID (and thus repository) may change over time. For AT URIs with a handle in the authority section, the handle-to-DID mapping can also change.

A major semantic difference between AT URIs and common URL formats like `https://`, `ftp://`, or `wss://` is that the "authority" part of an AT URI does not indicate a network location for the indicated resource. Even when a handle is in the authority part, the hostname is only used for identity lookup, and is often not the ultimate host for repository content (aka, the handle hostname is often not the PDS host).

### Generic URI Compliance

AT URIs meet the generic syntax for Universal Resource Identifiers, as defined in IETF [RFC-3986](https://www.rfc-editor.org/rfc/rfc3986). They utilize some generic URI features outlined in that document, though not all. As a summary of generic URI parts and features:

- Authority part, preceded by double slash: supported
- Empty authority part: not supported
- Userinfo: not currently supported, but reserved for future use. a lone `@` character preceding a handle is not valid (eg, `at://@handle.example.com` is not valid)
- Host and port separation: not supported. syntax conflicts with DID in authority part
- Path part: supported, optional
- Query: supported in general syntax, not currently used
- Fragment: supported in general syntax, not currently used
- Relative references: not yet supported
- Normalization rules: supported in general syntax, not currently used

AT URIs are not compliant with the WHATWG URL Standard ([https://url.spec.whatwg.org/](https://url.spec.whatwg.org/)). Un-encoded colon characters in DIDs in the authority part of the URI are disallowed by that standard. Note that it is possible to un-ambigiously differentiate a DID in the authority section from a `host:port` pair. DIDs always have at least two colons, always begin with `did:`, and the DID method can not contain digits.

### Full AT URI Syntax

The full syntax for AT URIs is flexible to a variety of future use cases, including future extensions to the path structure, query parameters, and a fragment part. The full syntax rules are:

- The overall URI is restricted to a subset of ASCII characters
- For reference below, the set of unreserved characters, as defined in [RFC-3986](https://www.rfc-editor.org/rfc/rfc3986), includes alphanumeric (`A-Za-z0-9`), period, hyphen, underscore, and tilde (`.-_~`)
- Maximum overall length is 8 kilobytes (which may be shortened in the future)
- Hex-encoding of characters is permitted (but in practice not necessary)
- The URI scheme is `at`, and an authority part preceded with double slashes is always required, so the URI always starts `at://`
- An authority section is required and must be non-empty. the authority can be either an atproto Handle, or a DID meeting the restrictions for use with atproto. note that the authority part can *not* be interpreted as a host:port pair, because of the use of colon characters (`:`) in DIDs. Colons and unreserved characters should not be escaped in DIDs, but other reserved characters (including `#`, `/`, `$`, `&`, `@`) must be escaped.
    - Note that none of the current "blessed" DID methods for atproto allow these characters in DID identifiers
- An optional path section may follow the authority. The path may contain multiple segments separated by a single slash (`/`). Generic URI path normalization rules may be used.
- An optional query part is allowed, following generic URI syntax restrictions
- An optional fragment part is allowed, using JSON Path syntax


### Restricted AT URI Syntax

A restricted sub-set of valid AT URIs are currently used in Lexicons for the `at-uri` type. Query parameters and fragments are not currently used. Trailing slashes are not allowed, including a trailing slash after the authority with no other path. The URI should be in normalized form (see "Normalization" section), with all of the individual sub-identifiers also normalized.

```text
AT-URI        = "at://" AUTHORITY [ "/" COLLECTION [ "/" RKEY ] ]

AUTHORITY     = HANDLE | DID
COLLECTION    = NSID
RKEY          = RECORD-KEY
```


### Normalization

Particularly when included in atproto records, strict normalization should be followed to ensure that the representation is reproducible and can be used with simple string equality checks.

- No unnecessary hex-encoding in any part of the URI
- Any hex-encoding hex characters must be upper-case
- URI schema is lowercase
- Authority as handle: lowercased
- Authority as DID: in normalized form, and no duplicate hex-encoding. For example, if the DID is already hex-encoded, don't re-encode the percent signs.
- No trailing slashes in path part
- No duplicate slashes or "dot" sections in path part (`/./` or `/abc/../` for example)
- NSID in path: domain authority part lowercased
- Record Key is case-sensitive and not normalized
- Query and fragment parts should not be included when referencing repositories or records in Lexicon records

Refer to [RFC-3986](https://www.rfc-editor.org/rfc/rfc3986) for generic rules to normalize paths and remove `..` / `.` relative references.


### Examples

Valid AT URIs (both general and Lexicon syntax):

```text
at://foo.com/com.example.foo/123
```

Valid general AT URI syntax, invalid in current Lexicon:

```text
at://foo.com/example/123     // invalid NSID
at://computer                // not a valid DID or handle
at://example.com:3000        // not a valid DID or handle
```

Invalid AT URI (in both contexts)

```text
at://foo.com/                // trailing slash
at://user:pass@foo.com       // userinfo not currently supported
```


### Usage and Implementation Guidelines

Generic URI and URL parsing libraries can sometimes be used with AT URIs, but not always. A key requirement is the ability to work with the authority (or origin) part of the URI as a simple string, without being parsed in to userinfo, host, and port sub-parts. Specifically: the Python 3 `urllib` module (from the standard library) works; the Javascript `url-parse` package works; the Golang `net/url` package does not work; and most of the popular Rust URL parsing crates do not work.

When referencing records, especially from other repositories, best practice is to use a DID in the authority part, not a handle. For application display, a handle can be used as a more human-readable alternative. In HTML, it is permissible to *display* the handle version of an AT-URI and *link* (`href`) to the DID version.

When a *strong* reference to another record is required, best practice is to use a CID hash in addition to the AT URI.

In Lexicons (APIs, records, and other contexts), sometimes a specific variant of an AT URI is required, beyond the general purpose `at-uri` string format. For example, references to records from inside records usually require a DID in the authority section, and the URI must include the collection and rkey path segments. URIs not meeting these criteria will fail to validate.

Do not confuse the JSON Path fragment syntax with the Lexicon reference syntax. They both use `#`-based fragments to reference other fields in JSON documents, but, for example, JSON Path syntax starts with a slash (`#/key`).


### Possible Future Changes

The maximum length constraint may change.

Relative references may be supported in Lexicons in `at-uri` fields. For example, one record referencing other records in the same repository could use `../<collection>/<rkey>` relative path syntax.


---
atproto/src/app/[locale]/specs/at-uri-scheme/ko.mdx
---
export const metadata = {
  title: 'AT URI 스키마 (at://)',
  description: 'ATP 저장소 데이터를 주소 지정하기 위한 URI 스키마입니다.',
}

# AT URI 스키마 (at://)

AT URI 스키마(`at://`)은 특정 저장소의 개별 레코드를 참조할 때, 해당 저장소를 DID 또는 핸들로 식별하여 쉽게 참조할 수 있도록 설계되었습니다. 또한 AT URI는 저장소 내의 컬렉션이나 전체 저장소(즉, 정체성)를 참조하는 데에도 사용할 수 있습니다.

두 개의 AT URI는 동일한 저장소 내의 동일한 레코드를 참조합니다; 하나는 계정의 DID를 사용하고, 다른 하나는 계정의 핸들을 사용합니다.

- `at://did:plc:44ybard66vv44zksje25o7dz/app.bsky.feed.post/3jwdwj2ctlk26`
- `at://bnewbold.bsky.team/app.bsky.feed.post/3jwdwj2ctlk26`

<Note>
**핸들 기반 AT URI에 대한 주의사항**

핸들을 참조하는 AT URI는 내구성이 없습니다.

사용자가 핸들을 변경하면, 해당 핸들을 사용하는 모든 AT URI는 유효하지 않게 되며, 핸들이 재사용될 경우 다른 저장소의 레코드를 참조할 위험이 있습니다.

AT URI는 콘텐츠 기반 주소 지정 방식이 아니므로, 참조하는 레코드의 _내용_ 또한 시간이 지나면서 변경될 수 있습니다.
</Note>

### 구조

AT URI의 전체 일반 구조는 다음과 같습니다:

```text
"at://" AUTHORITY [ PATH ] [ "?" QUERY ] [ "#" FRAGMENT ]
```

**authority** 부분은 저장소와 연결된 정체성을 나타내며, DID 또는 핸들을 사용할 수 있습니다. 단, 핸들은 시간이 지나면서 다른 DID(즉, 다른 저장소)를 참조할 수 있으므로 주의해야 합니다. (자세한 내용은 아래의 "사용 및 구현 가이드라인"과 "강력한 참조" 항목을 참조하세요.)

현재 atproto Lexicon 사용에서는 **query**와 **fragment** 부분이 지원되지 않으며, 경로에 허용되는 패턴은 다음과 같이 고정되어 있습니다:

```text
"at://" AUTHORITY [ "/" COLLECTION [ "/" RKEY ] ]
```

**authority** 부분은 필수이며 정규화되어야 합니다. 만약 DID가 사용된다면, atproto에서 허용된 "blessed" DID 메서드 중 하나여야 합니다. 선택적인 **collection** 부분은 정규화된 [NSID](./nsid)여야 하며, 선택적인 **rkey** 부분은 유효한 [Record Key](./record-key)여야 합니다.

특정 저장소 내의 레코드를 참조하는 AT URI는 *강력한* 참조가 아닙니다. 즉, 콘텐츠 기반 주소 방식이 아니므로 해당 레코드는 시간이 지나면서 변경되거나 삭제될 수 있으며, DID 자체가 삭제되거나 접근이 어려워질 수 있습니다. `did:web`의 경우, DID의 통제(즉, 저장소에 대한 통제)가 시간이 지나면서 변경될 수 있습니다. 또한 authority 부분에 핸들이 포함된 경우, 핸들과 DID 간의 매핑도 변경될 수 있습니다.

AT URI와 `https://`, `ftp://`, `wss://` 등 일반적인 URL 형식 간의 주요 차이점은, AT URI의 "authority" 부분이 지정된 리소스의 네트워크 위치를 나타내지 않는다는 점입니다. 핸들이 authority에 있더라도, 해당 호스트명은 단지 정체성 조회에 사용되며, 실제 저장소 콘텐츠의 호스트(예: PDS 호스트)와 다를 수 있습니다.

### 일반 URI 준수

AT URI는 IETF [RFC-3986](https://www.rfc-editor.org/rfc/rfc3986)에서 정의한 범용 리소스 식별자(URI)의 일반 문법을 따릅니다. 이 문서에서 설명하는 일부 일반 URI 기능을 활용하지만, 모든 기능을 사용하지는 않습니다. 일반 URI의 구성 요소 및 특징은 다음과 같습니다:

- Authority 부분 (double slash 앞에 위치): 지원됨
- 빈 authority 부분: 지원되지 않음
- Userinfo: 현재 지원되지 않으나 향후 사용을 위해 예약됨. 단독 `@` 문자가 핸들 앞에 오는 경우는 유효하지 않음 (예: `at://@handle.example.com` 은 유효하지 않음)
- 호스트와 포트 구분: 지원되지 않음. 이는 DID에 사용된 콜론(`:`)과 문법 충돌이 발생하기 때문입니다.
- Path 부분: 지원되며 선택적임
- Query: 일반 문법에 따라 지원되지만, 현재 사용되지 않음
- Fragment: 일반 문법에 따라 지원되지만, 현재 사용되지 않음
- 상대 참조: 아직 지원되지 않음
- 정규화 규칙: 일반 문법에 따라 지원되나, 현재 사용되지 않음

AT URI는 WHATWG URL 표준([https://url.spec.whatwg.org/](https://url.spec.whatwg.org/))을 준수하지 않습니다. Authority 부분에 포함된 DID의 콜론 문자가 인코딩되지 않은 경우, 해당 표준에서 허용되지 않습니다. DID는 항상 적어도 두 개의 콜론을 포함하며, 항상 `did:`로 시작하고, DID 메서드에는 숫자가 포함될 수 없으므로 명확하게 구분할 수 있습니다.

### 전체 AT URI 문법

전체 AT URI 문법은 향후 경로 구조, 쿼리 매개변수, 그리고 fragment 부분의 확장을 포함한 다양한 사용 사례에 유연하게 대응할 수 있도록 설계되었습니다. 전체 문법 규칙은 다음과 같습니다:

- 전체 URI는 제한된 ASCII 문자 집합으로 구성됩니다.
- 아래에서 언급하는 "unreserved" 문자 집합은 [RFC-3986](https://www.rfc-editor.org/rfc/rfc3986)에서 정의된 영숫자(`A-Za-z0-9`), 마침표, 하이픈, 언더스코어, 틸데(`.-_~`)를 포함합니다.
- 전체 길이 제한은 최대 8킬로바이트(향후 변경될 수 있음)
- 문자의 16진수 인코딩은 허용되나, 실제로는 필요하지 않습니다.
- URI 스키마은 `at`이며, 항상 double slash(`//`)와 함께 authority 부분이 필수입니다. 즉, URI는 항상 `at://`로 시작해야 합니다.
- Authority 부분은 필수이며 비어있을 수 없습니다. Authority는 atproto 핸들이거나 atproto와 함께 사용하기 위한 제한 조건을 충족하는 DID여야 합니다. Authority 부분은 호스트:포트 쌍으로 해석될 수 없으며, 이는 DID에 콜론(`:`)이 사용되기 때문입니다. DID 내의 콜론과 unreserved 문자는 인코딩되지 않아야 하며, 다른 예약 문자는(예: `#`, `/`, `$`, `&`, `@`) 인코딩되어야 합니다.
    - 현재 atproto에서 허용하는 "blessed" DID 메서드는 이러한 예약 문자가 DID 식별자에 포함되지 않음을 보장합니다.
- 선택적인 경로 부분은 authority 다음에 올 수 있으며, 경로는 단일 슬래시(`/`)로 구분된 여러 세그먼트를 포함할 수 있습니다. 일반 URI 경로 정규화 규칙을 사용할 수 있습니다.
- 선택적인 쿼리 부분은 일반 URI 문법 제한을 따릅니다.
- 선택적인 fragment 부분은 JSON Path 문법을 따릅니다.

### 제한된 AT URI 문법

Lexicon에서 `at-uri` 타입으로 사용되는 AT URI의 하위 집합은 다음과 같습니다. 현재 쿼리 매개변수와 fragment는 사용되지 않으며, 끝에 슬래시가 허용되지 않습니다 (authority 뒤에 추가적인 경로가 없을 때의 슬래시 포함 불가). URI는 정규화된 형태여야 하며, 모든 하위 식별자들도 정규화되어야 합니다.

```text
AT-URI        = "at://" AUTHORITY [ "/" COLLECTION [ "/" RKEY ] ]

AUTHORITY     = HANDLE | DID
COLLECTION    = NSID
RKEY          = RECORD-KEY
```

### 정규화

특히 atproto 레코드에 포함될 때, 재현 가능하고 단순 문자열 비교가 가능하도록 엄격한 정규화 규칙을 따라야 합니다.

- URI의 모든 부분에서 불필요한 16진수 인코딩을 사용하지 않습니다.
- 인코딩된 16진수 문자는 대문자로 표기합니다.
- URI 스키마은 소문자로 표기합니다.
- 핸들이 authority에 사용된 경우, 소문자로 표기합니다.
- DID가 authority에 사용된 경우, 정규화된 형태여야 하며 중복된 16진수 인코딩을 피해야 합니다. (이미 16진수 인코딩되어 있다면 퍼센트 기호를 다시 인코딩하지 않습니다.)
- 경로 부분 끝에 슬래시를 사용하지 않습니다.
- 경로 부분 내 중복 슬래시나 불필요한 "dot" 섹션(`/./` 또는 `/abc/../` 등)을 제거합니다.
- 경로에 포함된 NSID의 도메인 부분은 소문자로 표기합니다.
- 레코드 키는 대소문자를 구분하며 정규화하지 않습니다.
- Lexicon 레코드에서 저장소나 레코드를 참조할 때는 쿼리와 fragment 부분을 포함하지 않습니다.

자세한 경로 정규화 및 `..`/`.` 상대 참조 제거는 [RFC-3986](https://www.rfc-editor.org/rfc/rfc3986)을 참고하세요.

### 예시

유효한 AT URI (일반 및 Lexicon 문법 모두):

```text
at://foo.com/com.example.foo/123
```

일반 AT URI 문법에서는 유효하지만, 현재 Lexicon에서는 유효하지 않은 예:

```text
at://foo.com/example/123     // 유효하지 않은 NSID
at://computer                // 유효한 DID 또는 핸들이 아님
at://example.com:3000        // 유효한 DID 또는 핸들이 아님
```

두 문맥 모두에서 유효하지 않은 AT URI:

```text
at://foo.com/                // 끝에 슬래시 있음
at://user:pass@foo.com       // userinfo는 현재 지원되지 않음
```

### 사용 및 구현 가이드라인

일반 URI 및 URL 파싱 라이브러리는 때때로 AT URI에 사용할 수 있으나, 항상 적절히 동작하는 것은 아닙니다. 핵심 요구사항은 URI의 authority(또는 origin) 부분을 userinfo, host, port로 분리하지 않고 단순 문자열로 다룰 수 있어야 한다는 점입니다. 구체적으로:

- Python 3의 `urllib` 모듈(표준 라이브러리)은 사용 가능합니다.
- Javascript의 `url-parse` 패키지는 사용 가능합니다.
- Golang의 `net/url` 패키지는 작동하지 않습니다.
- 대부분의 인기 있는 Rust URL 파싱 크레이트들은 작동하지 않습니다.

특히 다른 저장소의 레코드를 참조할 때는, 권장사항으로 authority 부분에 DID를 사용하는 것입니다. 어플리케이션 화면에서는 보다 읽기 쉬운 핸들을 사용할 수 있습니다. HTML에서는 핸들 버전의 AT URI를 *표시*하고, 실제 링크(`href`)는 DID 버전을 사용할 수 있습니다.

레코드에 대한 *강력한* 참조가 필요한 경우, AT URI와 함께 CID 해시를 사용하는 것이 권장됩니다.

Lexicon(레코드, API 등)에서는 때때로 일반 `at-uri` 문자열 형식 외에 특정 변형의 AT URI가 요구됩니다. 예를 들어, 레코드 내부에서 다른 레코드를 참조할 때는 authority 부분에 DID가 있어야 하며, URI는 컬렉션과 rkey 경로 세그먼트를 포함해야 합니다. 이러한 조건을 충족하지 않는 URI는 검증에 실패합니다.

JSON Path의 fragment 문법과 Lexicon 참조 문법을 혼동하지 마세요. 두 문법 모두 `#` 기호를 사용하여 JSON 문서 내 다른 필드를 참조하지만, 예를 들어 JSON Path 문법은 슬래시(`/`)로 시작합니다 (예: `#/key`).

### 향후 변경 가능 사항

최대 길이 제한은 변경될 수 있습니다.

상대 참조는 Lexicon의 `at-uri` 필드에서 지원될 수 있습니다. 예를 들어, 동일한 저장소 내의 다른 레코드를 참조할 때 `../<collection>/<rkey>`와 같은 상대 경로 문법을 사용할 수 있습니다.


---
atproto/src/app/[locale]/specs/at-uri-scheme/page.tsx
---
export const metadata = {
  title: 'AT URI scheme (at://)',
  description: 'A URI scheme for addressing ATP repository data.',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/data-model/en.mdx
---
export const metadata = {
  title: 'Data Model',
  description:
    'Consistent data encoding for records and messages.',
}

# Data Model

Records and messages in atproto are stored, transmitted, encoded, and authenticated in a consistent way. The core "data model" supports both binary (CBOR) and textual (JSON) representations. {{ className: 'lead' }}

When data needs to be authenticated (signed), referenced (linked by content hash), or stored efficiently, it is encoded in Concise Binary Object Representation (CBOR). CBOR is an IETF standard roughly based on JSON. The specific normalized subset of CBOR used in the atproto data model is called **DAG-CBOR**. All DAG-CBOR data is valid CBOR, and can be read with any CBOR library. Writing or strictly verifying DAG-CBOR with the correct normalization rules sometimes requires additional configuration or a special CBOR implementation. {{ className: 'lead' }}

The schema definition language for atproto is [Lexicon](/specs/lexicon). Other lower-level data structures, like [repository](/specs/repository) internals, are not specified with Lexicons, but use the same data model and encodings.

Distinct pieces of data are called **nodes,** and when encoded in binary (DAG-CBOR) result in a **block.** A node may have internal nested structure (maps or lists). Nodes may reference each other by string URLs or URIs, just like with regular JSON on the web. They can also reference each other strongly by hash, referred a **link.** A set of linked nodes can form higher-level data structures like [Merkle Trees](https://en.wikipedia.org/wiki/Merkle_tree) or [Directed Acyclical Graphs (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph). Links can also refer to arbitrary binary data (blobs).

Unlike URLs, hash references (links) do not encode a specific network location where the content can be found. The location and access mechanism must be inferred by protocol-level context. Hash references do have the property of being "self-certifying", meaning that returned data can be verified against the link hash. This makes it possible to redistribute content and trust copies even if coming from an untrusted party.

Links are encoded as [Content Identifiers](https://docs.ipfs.tech/concepts/content-addressing/#identifier-formats) (CIDs), which have both binary and string representations. CIDs include a metadata code which indicates whether it links to a node (DAG-CBOR) or arbitrary binary data. Some additional constraints on the use of CIDs in atproto are described below.

In atproto, object nodes often include a string field `$type` that specifies their Lexicon schema. Data is mostly self-describing and can be processed in schema-agnostic ways (including decoding and re-encoding), but can not be fully validated without the schema on-hand or known ahead of time.

## Relationship With IPLD

The data model is inspired by [Interplanetary Linked Data (IPLD)](https://ipld.io/docs/data-model/), a specification for hash-linked data structures from the IPFS ecosystem.

IPLD specifies a normalized JSON encoding called **DAG-JSON,** but atproto uses a different set of conventions when encoding JSON data. The atproto JSON encoding is not designed to be byte-determinisitic, and the CBOR representation is used when data needs to be cryptographically signed or hashed.

The IPLD Schema language is not used.

## Data Types

| Lexicon Type  | IPLD Type | JSON                 | CBOR                    | Note                    |
| ---           | ---       | ---                  | ---                     | ---                     |
| `null`        | null      | Null                 | Special Value (major 7) |                         |
| `boolean`     | boolean   | Boolean              | Special Value (major 7) |                         |
| `integer`     | integer   | Number               | Integer (majors 0,1)    | signed, 64-bit          |
| `string`      | string    | String               | UTF-8 String (major 3)  | Unicode, UTF-8          |
| -             | float     | Number               | Special (major 7)       | not allowed in atproto  |
| `bytes`       | bytes     | `$bytes` Object      | Byte String (major 2)   |                         |
| `cid-link`    | link      | `$link` Object       | CID (tag 42)            | CID                     |
| `array`       | list      | Array                | Array (major 4)         |                         |
| `object`      | map       | Object               | Map (major 5)           | keys are always strings |
| `blob`        | -         | `$type: blob` Object | `$type: blob` Map       |                         |

`blob` is for references to files, such as images. It includes basic metadata like MIME Type and size (in bytes).

As a best practice to ensure Javascript compatibility with default types, `integer` should be limited to 53 bits of precision. Note that JSON numbers can have an arbitrary number of digits, but `integer` is limited to 64 bits even ignoring Javascript.

Lexicons can include additional validation constraints on individual fields. For example, integers can have maximum and minimum values. Data can not be validated against these additional constraints without access to the relevant Lexicon schema, but there is a concept of validating free-form JSON or CBOR against the atproto data model in an abstract sense. For example, a JSON object with a nested `$bytes` object with a boolean instead of a base64-encoded string might be valid JSON, but can never be valid under the atproto data model.

Lexicon string fields can have additional `format` type information associated with them for validation, but as with other validation constraints this information is not available without the Lexicon itself.

Data field names starting with `$` are reserved for use by the data model or protocol itself, in both JSON and CBOR representations. For example, the `$bytes` key name (used in CBOR and JSON), the `$link` key (used for JSON CID Links), or `$type` (used to indicate record type). Implementations should ignore unknown `$` fields (to allow protocol evolution). Applications, extensions, and integrations should not use or unilaterally define new `$` fields, to prevent conflicts as the protocol evolves.

### Nullable and False-y

In the atproto data model there is a semantic difference between explicitly setting an map field to `null` and not including the field at all. Both JSON and CBOR have the same distinction.

Null or missing fields are also distinct from "false-y" value like `false` (for booleans), `0` (for integers), empty lists, or empty objects.

### Why No Floats?

CBOR and JSON both natively support floating point numbers, so why does atproto go out of the way to disallow them?

The IPLD specification describes [some of the complexities and sharp edges](https://ipld.io/docs/data-model/kinds/#float-kind) when working with floats in a content-addressable world. In short, de-serializing in to machine-native format, then later re-encoding, is not always consistent. This is definitely true for special values and corner-cases, but can even be true with "normal" float values on less-common architectures.

It may be possible to come up with rules to ensure reliable round-trip encoding of floats in the future, but for now we disallow floats.

If you have a use-case where integers can not be substituted for floats, we recommend encoding the floats as strings or even bytes. This provides a safe default round-trip representation.

## `blob` Type

References to "blobs" (arbitrary files) have a consistent format in atproto, and can be detected and processed without access to any specific Lexicon. That is, it is possible to parse nodes and extract any blob references without knowing the schema.

Blob nodes are maps with following fields:

- `$type` (string, required): fixed value `blob`. Note that this is not a valid NSID.
- `ref` (link, required): CID reference to blob, with multicodec type `raw`. In JSON, encoded as a `$link` object as usual
- `mimeType` (string, required, not empty): content type of blob. `application/octet-stream` if not known
- `size` (integer, required, positive, non-zero): length of blob in bytes

There is also a deprecated legacy blob format, with some records in the wild still containing blob references in this format:

- `cid` (string, required): a CID in *string* format, not *link* format
- `mimeType` (string, required, not empty): same as `mimeType` above

Note that the legacy format has no `$type` and can only be parsed for known Lexicons. Implementations should not throw errors when encountering the old format, but should never write them, and it is acceptable to only partially support them.

## JSON Representation

atproto uses its own conventions for JSON, instead of using DAG-JSON directly. The main motivation was to have more idiomatic and human-readable representations for `link` and `bytes` in HTTP APIs. The DAG-JSON specification itself mentions that it is primarily oriented toward debugging and development environments, and we found that the use of `/` as a field key was confusing to developers.

Normalizations like key sorting are also not required or enforced when using JSON in atproto: only DAG-CBOR is used as a byte-reproducible representation.

The encoding for most of the core and compound types is straight forward, with only `link` and `bytes` needing special treatment.

### `link`

The JSON encoding for link is an object with the single key `$link` and the string-encoded CID as a value.

For example, a node with a single field `"exampleLink"` with type `link` would encode in JSON like:

```
{
  "exampleLink": {
    "$link": "bafyreidfayvfuwqa7qlnopdjiqrxzs6blmoeu4rujcjtnci5beludirz2a"
  }
}

```

For comparison, this is very similar to the DAG-JSON encoding, but substitutes `$link` as the key name instead of `/` (single-character, forward slash).

### `bytes`

The JSON encoding for bytes is an object with the single key `$bytes` and string value with the base64-encoded bytes. The base64 scheme is the one specified in [RFC-4648, section 4](https://datatracker.ietf.org/doc/html/rfc4648#section-4), frequently referred to as simple "base64". This scheme is not URL-safe, and `=` padding is optional.

For example, a node with a single field `"exampleBytes"` with type `bytes` would be represented in JSON like:

```
{
  "exampleBytes": {
    "$bytes": "nFERjvLLiw9qm45JrqH9QTzyC2Lu1Xb4ne6+sBrCzI0"
  }
}

```

For comparison, the DAG-JSON encoding has two nested objects, with outer key `/` (single-character, forward slash), inner key `bytes`, and the same base64 encoding.

## Link and CID Formats

The [IPFS CID specification](https://github.com/multiformats/cid) is very flexible. It supports a wide variety of hash types, a field indicating the "type" of content being linked to, and various string encoding options. These features are valuable to allow evolution over time, but to maximize interoperability among implementations, only a specific "blessed" set of CID types are allowed.

The blessed formats for CIDs in atproto are:

- CIDv1
- multibase: binary serialization within DAG-CBOR `link` fields, and `base32` for string encoding
- multicodec: `dag-cbor` (0x71) for links to data objects, and `raw` (0x55) for links to blobs
- multihash: `sha-256` with 256 bits (0x12) is preferred

The use of SHA-256 is a stable requirement in some contexts, such as the repository MST nodes. In other contexts, like referencing media blobs, there will likely be a set of "blessed" hash types which evolve over time. A balance needs to be struck between protocol flexibility on the one hand (to adopt improved hashes and remove weak ones), and ensuring broad and consistent interoperability throughout an ecosystem of protocol implementations.

There are several ways to include a CID hash reference in an atproto object:

- `link` field type (Lexicon type `cid-link`). In DAG-CBOR encodes as a binary CID (multibase type 0x00) in a bytestring with CBOR tag 42. In JSON, encodes as `$link` object (see above)
- `string` field type, with Lexicon string format `cid`. In DAG-CBOR and JSON, encodes as a simple string
- `string` field type, with Lexicon string format `uri`, with URI scheme `ipld://`

## Usage and Implementation Guidelines

When working with the deprecated/legacy "blob" format, it is recommend to store in the same internal representation as regular "blob" references, but to set the `size` to zero or a negative value. This field should be checked when re-serializing to ensure proper round-trip behavior and avoid ever encoding a zero or negative `size` value in the normal object format.

Best practices for validating and limiting the size and structure of generic atproto data are described in a [Data Validation guide](/guides/data-validation), which is not formally part of this specification.

## Security and Privacy Considerations

There are a number of resource-consumption attacks possible when parsing untrusted CBOR content. It is recommended to use a library that automatically protects against huge allocations, deep nesting, invalid references, etc. This is particularly important for libraries implemented in languages without strong memory safety, such as C and C++. Note that high-level languages frequently wrap parsers written in lower-level languages.

## Possible Future Changes

Floats may be supported in one form or another.

The legacy "blob" format may be entirely removed, if all known records and repositories can be rewritten.

Additional hash types are likely to be included in the set of "blessed" CID configurations.


---
atproto/src/app/[locale]/specs/data-model/ko.mdx
---
export const metadata = {
  title: '데이터 모델',
  description: '레코드와 메시지를 위한 일관된 데이터 인코딩 방식입니다.',
}

# 데이터 모델

atproto에서 레코드와 메시지는 일관된 방식으로 저장, 전송, 인코딩, 인증됩니다. 핵심 "데이터 모델"은 이진(CBOR)과 텍스트(JSON) 표현 방식을 모두 지원합니다. {{ className: 'lead' }}

데이터가 인증(서명)되거나, 참조(내용 해시로 연결)되거나, 효율적으로 저장되어야 할 때, 데이터는 **Concise Binary Object Representation (CBOR)** 으로 인코딩됩니다. CBOR은 JSON을 기반으로 한 IETF 표준입니다. atproto 데이터 모델에서 사용되는 CBOR의 특정 표준화된 하위 집합은 **DAG-CBOR** 라고 합니다. 모든 DAG-CBOR 데이터는 유효한 CBOR이며, 모든 CBOR 라이브러리로 읽을 수 있습니다. 올바른 정규화 규칙을 사용하여 DAG-CBOR를 작성하거나 엄격하게 검증하려면 추가 구성이나 특별한 CBOR 구현이 필요할 수 있습니다. {{ className: 'lead' }}

atproto의 스키마 정의 언어는 [Lexicon](/specs/lexicon)입니다. [데이터 레포지토리](/specs/repository) 내부와 같은 다른 하위 수준의 데이터 구조는 Lexicon으로 지정되지는 않지만, 동일한 데이터 모델과 인코딩 방식을 사용합니다.

서로 구분되는 데이터 조각들은 **노드**라고 하며, 이진(DAG-CBOR)으로 인코딩될 경우 **블록**이 됩니다. 하나의 노드는 내부에 중첩된 구조(맵 또는 리스트)를 포함할 수 있습니다. 노드는 일반적인 JSON과 마찬가지로 문자열 URL이나 URI로 서로를 참조할 수 있습니다. 또한 **링크**라고 하는 해시를 통해 강하게 서로를 참조할 수도 있습니다. 링크된 노드들의 집합은 [머클 트리](https://en.wikipedia.org/wiki/Merkle_tree) 또는 [DAG (Directed Acyclical Graph)](https://en.wikipedia.org/wiki/Directed_acyclic_graph)와 같은 상위 수준의 데이터 구조를 형성할 수 있습니다. 링크는 임의의 이진 데이터(블롭)도 참조할 수 있습니다.

URL과 달리, 해시 참조(링크)는 콘텐츠를 찾을 수 있는 특정 네트워크 위치를 인코딩하지 않습니다. 위치와 접근 메커니즘은 프로토콜 수준의 문맥에 의해 유추되어야 합니다. 해시 참조는 "자기 인증(self-certifying)" 특성을 가지므로, 반환된 데이터를 링크 해시와 대조하여 검증할 수 있습니다. 이를 통해 신뢰할 수 없는 당사자로부터 온 복사본이라도 콘텐츠를 재배포하고 신뢰할 수 있게 됩니다.

링크는 [Content Identifier](https://docs.ipfs.tech/concepts/content-addressing/#identifier-formats) (CID)로 인코딩되며, 이는 이진 및 문자열 표현을 모두 지원합니다. CID는 링크 대상이 노드(DAG-CBOR)인지 임의의 이진 데이터인지를 나타내는 메타데이터 코드를 포함합니다. atproto에서 CID 사용에 대한 몇 가지 추가 제약 조건이 아래에 설명되어 있습니다.

atproto에서는 객체 노드에 종종 Lexicon 스키마를 지정하는 문자열 필드 `$type`이 포함됩니다. 데이터는 대부분 자체 기술(self-describing)되어 스키마에 독립적으로 처리(디코딩 및 재인코딩)할 수 있지만, 스키마가 없으면 완전하게 검증할 수는 없습니다.

## IPLD와의 관계

데이터 모델은 IPFS 생태계의 해시로 연결된 데이터 구조에 대한 명세인 [Interplanetary Linked Data (IPLD)](https://ipld.io/docs/data-model/)에서 영감을 받았습니다.

IPLD는 **DAG-JSON**이라는 정규화된 JSON 인코딩 방식을 지정하지만, atproto는 JSON 데이터를 인코딩할 때 다른 규칙들을 사용합니다. atproto JSON 인코딩은 바이트 단위 결정적(byte-reproducible)이지 않으며, 데이터가 암호학적으로 서명되거나 해시되어야 할 때는 CBOR 표현을 사용합니다.

IPLD 스키마 언어는 사용되지 않습니다.

## 데이터 타입

| Lexicon Type  | IPLD Type | JSON                 | CBOR                    | Note                    |
| ---           | ---       | ---                  | ---                     | ---                     |
| `null`        | null      | Null                 | Special Value (major 7) |                         |
| `boolean`     | boolean   | Boolean              | Special Value (major 7) |                         |
| `integer`     | integer   | Number               | Integer (majors 0,1)    | signed, 64-bit          |
| `string`      | string    | String               | UTF-8 String (major 3)  | Unicode, UTF-8          |
| -             | float     | Number               | Special (major 7)       | not allowed in atproto  |
| `bytes`       | bytes     | `$bytes` Object      | Byte String (major 2)   |                         |
| `cid-link`    | link      | `$link` Object       | CID (tag 42)            | CID                     |
| `array`       | list      | Array                | Array (major 4)         |                         |
| `object`      | map       | Object               | Map (major 5)           | keys are always strings |
| `blob`        | -         | `$type: blob` Object | `$type: blob` Map       |                         |

`blob`은 이미지와 같은 파일 참조를 위한 타입입니다.

자바스크립트 기본 타입과의 호환성을 보장하기 위한 모범 사례로, `integer`는 53비트 정밀도로 제한하는 것이 좋습니다. JSON 숫자는 임의의 자릿수를 가질 수 있으나, `integer`는 자바스크립트를 무시하더라도 64비트로 제한됩니다.

Lexicon은 개별 필드에 대해 추가적인 검증 제약 조건을 포함할 수 있습니다. 예를 들어, 정수에 대해 최대값과 최소값을 지정할 수 있습니다. 해당 Lexicon 스키마 없이 데이터가 atproto 데이터 모델에 대해 추상적으로 검증될 수는 있지만, 예를 들어 내부에 `$bytes` 객체가 있고 그 값이 base64 인코딩된 문자열 대신 boolean인 JSON 객체는 유효한 JSON일 수 있으나 atproto 데이터 모델에서는 절대 유효하지 않습니다.

Lexicon 문자열 필드는 추가적인 `format` 타입 정보를 가질 수 있으나, 다른 검증 제약 조건과 마찬가지로 Lexicon 없이는 이 정보에 접근할 수 없습니다.

`$`로 시작하는 데이터 필드 이름은 JSON과 CBOR 표현 모두에서 데이터 모델이나 프로토콜 자체에서 사용하는 예약어입니다. 예를 들어, CBOR와 JSON 모두에서 사용되는 `$bytes` 키, JSON CID 링크를 위한 `$link` 키, 또는 레코드 타입을 나타내는 `$type` 등이 이에 해당합니다. 구현체는 프로토콜 발전을 위해 알 수 없는 `$` 필드는 무시해야 합니다. 응용 프로그램, 확장, 통합에서는 프로토콜 발전으로 인한 충돌을 막기 위해 새로운 `$` 필드를 임의로 사용하거나 정의해서는 안 됩니다.

### Nullable과 False-y

atproto 데이터 모델에서는 맵 필드를 명시적으로 `null`로 설정하는 것과 필드를 아예 포함하지 않는 것 사이에 의미상의 차이가 있습니다. JSON과 CBOR 모두 동일한 구분을 유지합니다.

null 또는 누락된 필드는 `false`(불리언), `0`(정수), 빈 리스트 또는 빈 객체와 같은 "false-y" 값과도 구분됩니다.

### 왜 부동소수점은 없는가?

CBOR와 JSON은 원래 부동소수점 숫자를 지원하지만, atproto에서는 왜 이를 일부러 허용하지 않을까요?

IPLD 명세에서는 콘텐츠 주소 지정 환경에서 부동소수점 숫자를 다루면서 발생하는 [복잡성과 문제점들](https://ipld.io/docs/data-model/kinds/#float-kind)에 대해 설명하고 있습니다. 요약하면, 부동소수점을 기계 기본 형식으로 역직렬화한 후 나중에 재인코딩할 때 항상 일관되지 않을 수 있습니다. 이는 특수 값이나 극한의 경우뿐만 아니라 덜 일반적인 아키텍처에서도 "일반" 부동소수점 값에 대해 적용됩니다.

미래에 부동소수점의 안정적인 왕복 인코딩을 보장하는 규칙을 마련할 수도 있겠으나, 현재로서는 부동소수점을 허용하지 않습니다.

만약 부동소수점을 정수로 대체할 수 없는 사용 사례가 있다면, 부동소수점을 문자열이나 바이트로 인코딩하는 것을 권장합니다. 이는 안전한 기본 왕복 표현을 제공합니다.

## `blob` 타입

"블롭"에 대한 참조(임의 파일)는 atproto에서 일관된 형식을 가지며, 특정 Lexicon 없이도 탐지하고 처리할 수 있습니다. 즉, 노드를 파싱하여 블롭 참조를 추출할 수 있습니다.

블롭 노드는 다음 필드를 가진 맵입니다:

- `$type` (문자열, 필수): 고정값 `blob`. 이 값은 유효한 NSID가 아닙니다.
- `ref` (링크, 필수): 멀티코덱 타입이 `raw`인 블롭에 대한 CID 참조. JSON에서는 일반적인 `$link` 객체로 인코딩됩니다.
- `mimeType` (문자열, 필수, 빈 문자열 불가): 블롭의 콘텐츠 타입. 알 수 없는 경우 `application/octet-stream`
- `size` (정수, 필수, 양수, 0이 아님): 블롭의 바이트 단위 크기

또한, 아직 일부 레코드에 남아있는 사용 사례를 위해 사용되는 더 이상 권장되지 않는 구 버전의 블롭 포맷이 있습니다:

- `cid` (문자열, 필수): *문자열* 형식의 CID. *링크* 형식이 아닙니다.
- `mimeType` (문자열, 필수, 빈 문자열 불가): 위의 `mimeType`과 동일

구 버전 포맷은 `$type`이 없으며, 알려진 Lexicon에 대해서만 파싱될 수 있습니다. 구현체는 구 버전 포맷을 만났을 때 에러를 발생시키지 않아야 하지만, 이를 새로 작성해서는 안 되며, 부분적인 지원만 허용됩니다.

## JSON 표현

atproto는 DAG-JSON을 직접 사용하는 대신 자체적인 JSON 규칙을 사용합니다. 주요 동기는 HTTP API에서 `link`와 `bytes`를 더 관용적이고 사람이 읽기 쉬운 형식으로 표현하기 위함입니다. DAG-JSON 명세 자체도 주로 디버깅과 개발 환경에 초점을 맞추고 있으며, `/`를 필드 키로 사용하는 것이 개발자들에게 혼란을 주는 것으로 판단되었습니다.

JSON 사용 시 키 정렬과 같은 정규화는 요구되거나 강제되지 않으며, 오직 DAG-CBOR만이 바이트 재현성을 보장합니다.

대부분의 기본 및 복합 타입에 대한 인코딩은 간단하지만, `link`와 `bytes`만 특별한 처리가 필요합니다.

### `link`

JSON에서의 링크 인코딩은 단일 키 `$link`와 문자열로 인코딩된 CID 값을 가지는 객체입니다.

예를 들어, 타입이 `link`인 `"exampleLink"` 필드를 가진 노드는 JSON에서 다음과 같이 인코딩됩니다:

```
{
  "exampleLink": {
    "$link": "bafyreidfayvfuwqa7qlnopdjiqrxzs6blmoeu4rujcjtnci5beludirz2a"
  }
}
```

비교를 위해, 이는 DAG-JSON 인코딩과 매우 유사하지만, 외부 키로 단일 문자 `/` 대신 `$link`를 사용합니다.

### `bytes`

JSON에서의 bytes 인코딩은 단일 키 `$bytes`와 base64로 인코딩된 문자열 값을 가지는 객체입니다. 사용되는 base64 방식은 [RFC-4648, section 4](https://datatracker.ietf.org/doc/html/rfc4648#section-4)에 명시된 것으로, 일반적으로 "base64"라고 불립니다. 이 방식은 URL 안전하지 않으며, `=` 패딩은 선택적입니다.

예를 들어, 타입이 `bytes`인 `"exampleBytes"` 필드를 가진 노드는 JSON에서 다음과 같이 표현됩니다:

```
{
  "exampleBytes": {
    "$bytes": "nFERjvLLiw9qm45JrqH9QTzyC2Lu1Xb4ne6+sBrCzI0"
  }
}
```

비교를 위해, DAG-JSON 인코딩은 외부에 단일 문자 `/`, 내부에 `bytes` 키를 중첩하여 사용하는 방식과 동일한 base64 인코딩을 사용합니다.

## 링크와 CID 형식

[IPFS CID 명세](https://github.com/multiformats/cid)는 매우 유연합니다. 다양한 해시 타입, 링크되는 콘텐츠의 "타입"을 나타내는 필드, 그리고 다양한 문자열 인코딩 옵션을 지원합니다. 이러한 기능들은 시간이 지나면서 발전하는데 유용하지만, 구현 간 상호 운용성을 극대화하기 위해서는 "인증된(blessed)" CID 타입 집합만을 허용합니다.

atproto에서 인증된 CID 형식은 다음과 같습니다:

- CIDv1
- multibase: DAG-CBOR `link` 필드 내의 이진 직렬화와 문자열 인코딩 시 `base32`
- multicodec: 데이터 객체에 대한 링크는 `dag-cbor` (0x71), 블롭에 대한 링크는 `raw` (0x55)
- multihash: SHA-256 (256비트, 0x12)을 선호

SHA-256의 사용은 레포지토리 MST 노드와 같이 일부 상황에서 안정적인 요구사항입니다. 미디어 블롭 참조와 같이 다른 상황에서는 시간이 지나며 발전하는 "인증된" 해시 타입 집합이 있을 수 있습니다. 프로토콜 구현체 전반에 걸친 상호 운용성을 보장하기 위해, 프로토콜 유연성과 약한 해시 제거 사이에서 균형을 유지해야 합니다.

atproto 객체에서 CID 해시 참조를 포함시키는 방법은 여러 가지가 있습니다:

- `link` 필드 타입 (Lexicon 타입 `cid-link`): DAG-CBOR에서는 CBOR 태그 42가 있는 바이트 문자열로 이진 CID를 인코딩합니다. JSON에서는 위에 설명한 `$link` 객체로 인코딩됩니다.
- Lexicon의 string 필드 타입으로 `cid` 형식을 사용하는 경우: DAG-CBOR와 JSON 모두 단순 문자열로 인코딩됩니다.
- Lexicon의 string 필드 타입으로 `uri` 형식을 사용하며, URI 스킴 `ipld://`를 사용하는 경우

## 사용 및 구현 지침

더 이상 권장되지 않는 구 버전의 "blob" 포맷을 사용할 때는, 일반 "blob" 참조와 동일한 내부 표현을 사용하되 `size` 필드를 0 또는 음수로 설정하는 것이 좋습니다. 이 값은 재인코딩 시 올바른 왕복 동작을 보장하고 일반 객체 형식에서 0 또는 음수 `size` 값이 인코딩되지 않도록 하기 위해 검사되어야 합니다.

## 보안 및 개인정보 보호 고려사항

신뢰할 수 없는 CBOR 콘텐츠를 파싱할 때 다양한 자원 소비 공격이 가능하므로, 거대한 할당, 깊은 중첩, 잘못된 참조 등을 자동으로 방어하는 라이브러리의 사용을 권장합니다. 이는 C나 C++와 같이 강력한 메모리 안전성이 없는 언어로 구현된 라이브러리에서 특히 중요합니다. 고수준 언어의 경우, 내부적으로 하위 수준 언어로 작성된 파서를 감쌀 수 있습니다.

## 향후 변경 가능 사항

- 미래에는 부동소수점이 어떤 형태로든 지원될 수 있습니다.
- 모든 알려진 레코드와 레포지토리가 재작성될 수 있다면, 구 버전의 "blob" 포맷은 완전히 제거될 수 있습니다.
- "인증된" CID 구성 집합에 추가적인 해시 타입이 포함될 가능성이 높습니다.

---
atproto/src/app/[locale]/specs/data-model/page.tsx
---
export const metadata = {
  title: 'Data Model',
  description: 'Consistent data encoding for records and messages.',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/atp/en.mdx
---
export const metadata = {
  title: 'AT Protocol',
  description:
    'Specification for the Authenticated Transfer Protocol (AT Protocol)',
}

# AT Protocol

The Authenticated Transfer Protocol (AT Protocol or atproto) is a generic federated protocol for building open social media applications. Some recurring themes and features are: {{ className: 'lead' }}

- Self-authenticating data and identity, allowing seamless account migrations and redistribution of content {{ className: 'lead' }}
- Design for "big world" use cases, scaling to billions of accounts {{ className: 'lead' }}
- Delegated authority over application-layer schemas and aggregation infrastructure {{ className: 'lead' }}
- Re-use of existing data models from the dweb protocol family and network primitives from the web platform {{ className: 'lead' }}

## Protocol Structure

**Identity:** account control is rooted in stable [DID](/specs/did) identifiers, which can be rapidly resolved to determine the current service provider location and [Cryptographic keys](/specs/cryptography) associated with the account. [Handles](/specs/handle) provide a more human-recognizable and mutable identifier for accounts.

**Data:** public content is stored in content-addressed and cryptographically verifiable [Repositories](/specs/repository). Data records and network messages all conform to a unified [Data Model](/specs/data-model) (with [CBOR](https://en.wikipedia.org/wiki/CBOR) and JSON representations). [Labels](/specs/label) are a separate lightweight form of metadata, individually signed and distributed outside repositories.

**Network:** HTTP client-server and server-server [APIs](/specs/xrpc) are described with Lexicons, as are WebSocket [Event Streams](/specs/event-stream). Individual records can be referenced across the network by [AT URI](/specs/at-uri-scheme). A [Personal Data Server (PDS)](/specs/account) acts as an account's trusted agent in the network, routes client network requests, and hosts repositories. A relay crawls many repositories and outputs a unified event [Firehose](/specs/sync).

**Application:** APIs and record schemas for applications built on atproto are specified in [Lexicons](/specs/lexicon), which are referenced by [Namespaced Identifiers](/specs/nsid) (NSIDs). Application-specific aggregations (such as search) are provided by an Application View (App View) service. Clients can include mobile apps, desktop software, or web interfaces.

The AT Protocol itself does not specify common social media conventions like follows or avatars, leaving these to application-level Lexicons. The `com.atproto.*` Lexicons provide common APIs for things like account signup and login. These could be considered part of AT Protocol itself, though they can also be extended or replaced over time as needed. Bluesky is a microblogging social app built on top of AT Protocol, with lexicons under the `app.bsky.*` namespace.

While atproto borrows several formats and specifications from the IPFS ecosystem (such as [IPLD](https://ipld.io/) and [CID](https://github.com/multiformats/cid)), atproto data does not need to be stored in the IPFS network, and the atproto reference implementation does not use the IPFS network at all.


## Protocol Extension and Applications

AT Protocol was designed from the beginning to balance stability and interoperation against flexibility for third-party application development.

The core protocol extension mechanism is development of new Lexicons under independent namespaces. Lexicons can declare new repository record schemas (stored in collections by NSID), new HTTP API endpoints, and new event stream endpoints and message types. It is also expected that new applications might require new network aggregation services ("AppViews") and client apps (eg, mobile apps or web interfaces).

It is expected that third parties will reuse Lexicons and record data across namespaces. For example, new applications are welcome to build on top of the social graph records specified in the `app.bsky.*` Lexicons, as long as they comply with the schemas controlled by the `bsky.app` authority.

Governance structures for individual Lexicon namespaces are flexible. They could be developed and maintained by volunteer communities, corporations, consortia, academic researchers, funded non-profits, etc.

## What Is Missing?

These specifications cover most details as implemented in Bluesky's reference implementation. A few important pieces have not been finalized, both in that reference implementation and in these specifications.

**Moderation Primitives:** The `com.atproto.admin.*` routes for handling moderation reports and doing infrastructure-level take-downs is specified in Lexicons but should also be described in more detail.

## Future Work

Smaller changes are described in individual specification documents, but a few large changes span the entire protocol.

**Non-Public Content:** mechanisms for private group and one-to-one communication will be an entire second phase of protocol development. This encompasses primitives like "private accounts", direct messages, encrypted data, and more. We recommend against simply "bolting on" encryption or private content using the existing protocol primitives.

**Protocol Governance and Formal Standards Process:** The current development focus is to demonstrate all the core protocol features via the reference implementation, including open federation. After that milestone, the intent is to stabilize the lower-level protocol and submit the specification for independent review and revision through a standards body such as the IETF or the W3C.



---
atproto/src/app/[locale]/specs/atp/ko.mdx
---
export const metadata = {
  title: 'AT Protocol',
  description: 'Authenticated Transfer Protocol (AT Protocol)의 명세',
}

# AT Protocol

Authenticated Transfer Protocol (AT Protocol 또는 atproto)은 개방형 소셜 미디어 애플리케이션 구축을 위한 범용 연합 프로토콜입니다. 다음은 이 프로토콜의 반복적으로 등장하는 주제와 특징들입니다. {{ className: 'lead' }}

- 사용자 데이터와 신원의 자체 인증: 원활한 계정 이전 및 콘텐츠 재배포를 가능하게 합니다. {{ className: 'lead' }}
- "빅 월드" 사용 사례에 맞춘 설계: 수십억 계정에 확장할 수 있습니다. {{ className: 'lead' }}
- 애플리케이션 계층 스키마와 집계 인프라에 대한 위임된 권한 부여. {{ className: 'lead' }}
- dweb 프로토콜 계열의 기존 데이터 모델과 웹 플랫폼의 네트워크 원시 요소 재사용. {{ className: 'lead' }}

## 프로토콜 구조

**신원:** 계정 제어는 안정적인 [DID](/specs/did) 식별자에 기반하며, 이를 통해 현재 서비스 제공 위치와 계정에 연관된 [암호화 키](/specs/cryptography)를 신속하게 확인할 수 있습니다. [핸들](/specs/handle)은 계정을 보다 인간 친화적이고 변경 가능한 식별자로 제공합니다.

**데이터:** 공개 콘텐츠는 내용 기반 주소 지정 및 암호학적으로 검증 가능한 [레포지토리](/specs/repository)에 저장됩니다. 데이터 레코드와 네트워크 메시지는 모두 통합된 [데이터 모델](/specs/data-model)을 따르며, [CBOR](https://en.wikipedia.org/wiki/CBOR)와 JSON 형식을 지원합니다. [라벨](/specs/label)은 개별 서명이 포함된 경량 메타데이터로, 레포지토리 외부에서 배포됩니다.

**네트워크:** HTTP 클라이언트-서버 및 서버-서버 [API](/specs/xrpc)는 Lexicon을 사용하여 기술되며, WebSocket [이벤트 스트림](/specs/event-stream)도 동일한 방식으로 정의됩니다. 개별 레코드는 네트워크 상에서 [AT URI](/specs/at-uri-scheme)를 통해 참조할 수 있습니다. [개인 데이터 서버(PDS)](/specs/account)는 계정의 신뢰할 수 있는 대리인 역할을 하며, 클라이언트의 네트워크 요청을 라우팅하고 레포지토리를 호스팅합니다. 또한, 릴레이는 여러 레포지토리를 크롤링하여 통합된 이벤트 [Firehose](/specs/sync)를 제공합니다.

**애플리케이션:** atproto 위에서 구축된 애플리케이션의 API와 레코드 스키마는 [Lexicon](/specs/lexicon)으로 지정되며, [Namespaced Identifiers](/specs/nsid) (NSID)를 통해 참조됩니다. 애플리케이션별 집계(예: 검색)는 애플리케이션 뷰(App View) 서비스를 통해 제공됩니다. 클라이언트에는 모바일 앱, 데스크톱 소프트웨어, 웹 인터페이스 등이 포함됩니다.

AT Protocol 자체는 팔로우나 아바타와 같은 일반적인 소셜 미디어 관례를 규정하지 않으며, 이는 애플리케이션 수준의 Lexicon에 위임됩니다. `com.atproto.*` Lexicon은 계정 가입, 로그인 등 일반 API를 제공하며, 이는 AT Protocol의 일부로 간주될 수 있으나 필요에 따라 확장 또는 대체될 수 있습니다. Bluesky는 `app.bsky.*` 네임스페이스 하에 Lexicon을 사용하여 구현된 AT Protocol 기반의 마이크로블로깅 소셜 애플리케이션입니다.

또한, atproto는 IPFS 생태계의 여러 포맷과 명세(IPLD, CID 등)를 차용하지만, atproto 데이터가 반드시 IPFS 네트워크에 저장될 필요는 없으며, 참조 구현 역시 IPFS 네트워크를 사용하지 않습니다.

## 프로토콜 확장 및 애플리케이션

AT Protocol은 제3자 애플리케이션 개발에 유연성을 부여하면서도 안정성과 상호 운용성을 유지하도록 설계되었습니다.

핵심 프로토콜 확장 메커니즘은 독립적인 네임스페이스 하에서 새로운 Lexicon을 개발하는 것입니다. Lexicon은 새로운 레포지토리 레코드 스키마(NSID를 통해 저장됨), 새로운 HTTP API 엔드포인트, 그리고 새로운 이벤트 스트림 엔드포인트와 메시지 유형을 선언할 수 있습니다. 또한, 새로운 애플리케이션은 새로운 네트워크 집계 서비스(예: AppView) 및 클라이언트 애플리케이션(모바일 앱이나 웹 인터페이스 등)을 필요로 할 수 있습니다.

제3자들은 네임스페이스 간에 Lexicon과 레코드 데이터를 재사용할 수 있으며, 예를 들어 새로운 애플리케이션은 `bsky.app` 권한이 관리하는 스키마를 준수하는 한 `app.bsky.*` Lexicon에 명시된 소셜 그래프 레코드를 활용할 수 있습니다.

개별 Lexicon 네임스페이스의 거버넌스 구조는 자원봉사 커뮤니티, 기업, 컨소시엄, 학계 연구진, 후원 비영리 단체 등 다양한 주체에 의해 관리될 수 있습니다.

## 누락된 부분

이 명세서는 Bluesky의 참조 구현에서 구현된 대부분의 세부 사항을 다룹니다. 그러나 해당 참조 구현과 이 명세서 모두에서 아직 최종 확정되지 않은 몇 가지 중요한 원시 요소가 남아 있습니다.

**모더레이션 원시 요소:** 모더레이션 보고 처리 및 인프라 수준의 콘텐츠 삭제를 위한 `com.atproto.admin.*` 경로는 Lexicon에 명시되어 있으나, 이에 대한 추가적인 설명이 필요합니다.

**Lexicon 해상도:** 주어진 타입 이름(NSID)에 대해 Lexicon 스키마 정의 파일을 자동으로 조회하고 가져오는 방법이 요구됩니다.

## 향후 작업

개별 명세 문서에서 다루는 소규모 변경 사항 외에도, 전체 프로토콜에 걸친 몇 가지 대규모 변경 사항이 계획되어 있습니다.

**비공개 콘텐츠:** 개인 그룹 및 일대일 통신을 위한 메커니즘은 프로토콜 개발의 두 번째 단계로 진행될 예정입니다. 여기에는 "비공개 계정", 다이렉트 메시지, 암호화된 데이터 등의 원시 요소가 포함됩니다. 기존 프로토콜 원시 요소에 단순히 암호화나 비공개 콘텐츠 기능을 추가하는 방식은 권장되지 않습니다.

**프로토콜 거버넌스 및 정식 표준 프로세스:** 현재의 개발 초점은 개방형 연합을 포함한 모든 핵심 프로토콜 기능을 참조 구현을 통해 입증하는 것입니다. 그 후, 하위 프로토콜을 안정화하고 IETF 또는 W3C와 같은 표준화 기구를 통해 명세를 독립적으로 검토 및 수정할 계획입니다.


---
atproto/src/app/[locale]/specs/atp/page.tsx
---
export const metadata = {
  title: 'AT Protocol',
  description:
    'Specification for the Authenticated Transfer Protocol (AT Protocol)',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/tid/en.mdx
---
export const metadata = {
  title: 'Timestamp Identifiers (TIDs)',
  description:
    'A compact timestamp-based identifier for revisions and records.',
}

# Timestamp Identifiers (TIDs)

A TID ("timestamp identifier") is a compact string identifier based on an integer timestamp. They are sortable, appropriate for use in web URLs, and useful as a "logical clock" in networked systems. TIDs are currently used in atproto as record keys and for repository commit "revision" numbers.

Note: There are similarities to ["snowflake identifiers"](https://en.wikipedia.org/wiki/Snowflake_ID). In the decentralized context of atproto, the global uniqueness of TIDs can not be guaranteed, and an antagonistic repo controller could trivially create records re-using known TIDs.

## TID Structure

The high-level semantics of a TID are:

- 64-bit integer
- big-endian byte ordering
- encoded as `base32-sortable`. That is, encoded with characters `234567abcdefghijklmnopqrstuvwxyz`
- no special padding characters (like `=`) are used, but all digits are always encoded, so length is always 13 ASCII characters. The TID corresponding to integer zero is `2222222222222`.

The layout of the 64-bit integer is:

- The top bit is always 0
- The next 53 bits represent microseconds since the UNIX epoch. 53 bits is chosen as the maximum safe integer precision in a 64-bit floating point number, as used by Javascript.
- The final 10 bits are a random "clock identifier."

TID generators should generate a random clock identifier number, chosen to avoid collisions as much as possible (for example, between multiple worker instances of a PDS service cluster). A local clock can be used to generate the timestamp itself. Care should be taken to ensure the TID output stream is monotonically increasing and never repeats, even if multiple TIDs are generated in the same microsecond, or during "clock smear" or clock synchronization incidents. If the local clock has only millisecond precision, the timestamp should be padded. (You can do this by multiplying by 1000.)


## TID Syntax

Lexicon string type: `tid`

TID string syntax parsing rules:

- length is always 13 ASCII characters
- uses base32-sortable character set, meaning `234567abcdefghijklmnopqrstuvwxyz`
- the first character must be one of `234567abcdefghij`

Early versions of the TID syntax allowed hyphens, but they are no longer allowed and should be rejected when parsing.

A reference regex for TID is:

```
/^[234567abcdefghij][234567abcdefghijklmnopqrstuvwxyz]{12}$/
```

### Examples

Syntactically valid TIDs:

```
3jzfcijpj2z2a
7777777777777
3zzzzzzzzzzzz
2222222222222
```

Invalid TIDs:

```
# not base32
3jzfcijpj2z21
0000000000000

# case-sensitive
3JZFCIJPJ2Z2A

# too long/short
3jzfcijpj2z2aa
3jzfcijpj2z2
222

# legacy dash syntax *not* supported (TTTT-TTT-TTTT-CC)
3jzf-cij-pj2z-2a

# high bit can't be set
zzzzzzzzzzzzz
kjzfcijpj2z2a
```


---
atproto/src/app/[locale]/specs/tid/ko.mdx
---
export const metadata = {
  title: 'Timestamp Identifiers (TIDs)',
  description: '수정 및 레코드를 위한 간결한 타임스탬프 기반 식별자입니다.',
}

# Timestamp Identifiers (TIDs)

TID("Timestamp Identifiers")는 정수 타임스탬프를 기반으로 한 간결한 문자열 식별자입니다.  
이 식별자는 정렬이 가능하며 웹 URL에 적합하고, 네트워크 시스템 내에서 "논리적 시계" 역할을 수행할 수 있습니다.  
현재 TID는 atproto에서 레코드 키와 저장소 커밋의 "리비전" 번호로 사용되고 있습니다.

**참고:** TID는 ["스노우플레이크 식별자"](https://en.wikipedia.org/wiki/Snowflake_ID)와 유사한 점이 있습니다.  
그러나 atproto의 탈중앙화 맥락에서는 TID의 전역적 유일성이 보장되지 않으며, 악의적인 저장소 관리자가 알려진 TID를 재사용하는 레코드를 쉽게 생성할 수 있습니다.

## TID 구조

TID의 높은 수준의 의미는 다음과 같습니다:

- 64비트 정수
- 빅 엔디언 바이트 순서
- `base32-sortable` 인코딩: 즉, `234567abcdefghijklmnopqrstuvwxyz` 문자셋을 사용하여 인코딩합니다.
- 특별한 패딩 문자(예: `=`)를 사용하지 않으며, 모든 자릿수가 항상 인코딩되므로 길이는 항상 13개의 ASCII 문자입니다.  
  정수 0에 해당하는 TID는 `2222222222222`입니다.

64비트 정수의 배치는 다음과 같습니다:

- 최상위 비트는 항상 0입니다.
- 다음 53비트는 UNIX 에포크(1970년 1월 1일) 이후의 마이크로초를 나타냅니다.  
  53비트를 선택한 이유는 자바스크립트에서 사용하는 64비트 부동소수점 수의 최대 안전 정수 정밀도 때문입니다.
- 마지막 10비트는 임의의 "클록 식별자"입니다.

TID 생성기는 가능한 충돌을 피하기 위해 임의의 클록 식별자 번호를 생성해야 합니다 (예: 여러 PDS 서비스 클러스터의 작업자 인스턴스 간).  
타임스탬프는 로컬 클록을 사용하여 생성할 수 있습니다.  
동일한 마이크로초에 여러 TID가 생성되거나, "클록 스미어" 또는 클록 동기화 이슈가 발생하는 경우에도 TID 출력 스트림이 단조 증가하며 절대 반복되지 않도록 주의해야 합니다.  
만약 로컬 클록이 밀리초 정밀도만 제공한다면, 타임스탬프를 패딩해야 합니다. (예: 값을 1000으로 곱하여 마이크로초 단위로 변환할 수 있습니다.)

## TID 구문

Lexicon 문자열 타입: `tid`

TID 문자열 구문 파싱 규칙:

- 길이는 항상 13개의 ASCII 문자여야 합니다.
- `234567abcdefghijklmnopqrstuvwxyz` 문자셋, 즉 base32-sortable 문자셋을 사용합니다.
- 첫 번째 문자는 반드시 `234567abcdefghij` 중 하나여야 합니다.

초기 버전의 TID 구문에서는 하이픈을 허용했으나, 현재는 허용되지 않으며 파싱 시 거부해야 합니다.

TID에 대한 참고 정규식은 다음과 같습니다:

```
/^[234567abcdefghij][234567abcdefghijklmnopqrstuvwxyz]{12}$/
```

### 예시

#### 구문적으로 유효한 TID:

```
3jzfcijpj2z2a
7777777777777
3zzzzzzzzzzzz
2222222222222
```

#### 유효하지 않은 TID:

```
# base32 문자가 아님
3jzfcijpj2z21
0000000000000

# 대소문자 구분 (대문자 사용됨)
3JZFCIJPJ2Z2A

# 길이가 너무 길거나 짧음
3jzfcijpj2z2aa
3jzfcijpj2z2
222

# 더 이상 지원되지 않는 레거시 하이픈 구문 (TTTT-TTT-TTTT-CC)
3jzf-cij-pj2z-2a

# 최상위 비트가 1인 경우
zzzzzzzzzzzzz
kjzfcijpj2z2a


---
atproto/src/app/[locale]/specs/tid/page.tsx
---
export const metadata = {
  title: 'Timestamp Identifiers (TIDs)',
  description:
    'A compact timestamp-based identifier for revisions and records.',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/cryptography/en.mdx
---
export const metadata = {
  title: 'Cryptography',
  description:
    'Cryptographic systems, curves, and key types used in AT Protocol',
}

# Cryptography

Two elliptic curves are currently supported throughout the protocol, and implementations are expected to fully support both: {{ className: 'lead' }}

- `p256` elliptic curve: aka "NIST P-256", aka `secp256r1` (note the `r`), aka `prime256v1`
    - This curve *is* included in the WebCrypto API. It is commonly supported by personal device hardware (Trusted Platform Modules (TPMs) and mobile Secure Enclaves), and by cloud Hardware Security Modules (HSMs)
- `k256` elliptic curve: aka `secp256k1` (note the `k`)
    - This curve *is not* included in the WebCrypto API. It is used in Bitcoin and other cryptocurrencies, and as a result is broadly supported by personal secret key management technologies. It is also supported by cloud HSMs.

Because of the subtle visual distinction when the full curve names are written out, we often refer to them as `p256` or `k256`.

The atproto reference implementation from Bluesky supports both curves in all contexts, and creates `k256`  key pairs by default.

Key points for both systems have loss-less "compressed" representations, which are useful when sharing the public keys. This is usually supported natively for `k256`, but sometimes requires extra methods or jumping through hoops for `p256`. You can read more about this at: [02, 03 or 04? So What Are Compressed and Uncompressed Public Keys?](https://medium.com/asecuritysite-when-bob-met-alice/02-03-or-04-so-what-are-compressed-and-uncompressed-public-keys-6abcb57efeb6).

A common pattern when signing data in atproto is to encode the data in DAG-CBOR, hash the CBOR bytes with SHA-256, yielding raw bytes (not a hex-encoded string), and then sign the hash bytes.


## ECDSA Signature Malleability

Some ECDSA signatures can be transformed to yield a new distinct but still-valid signature. This does not require access to the private signing key or the data that was signed. The scope of attacks possible using this property is limited, but it is an unexpected property.

For `k256` specifically, the distinction is between "low-S" and "high-S" signatures, as discussed in [Bitcoin BIP-0062](https://github.com/bitcoin/bips/blob/master/bip-0062.mediawiki).

In atproto, use of the "low-S" signature variant is required for both `p256` and `k256` curves.

In atproto, signatures should always be verified using the verification routines provided by the cryptographic library, never by comparing signature values as raw bytes.


## Public Key Encoding

When encoding public keys as strings, the preferred representation uses multibase (with `base58btc` specifically) and a multicode prefix to indicate the specific key type. By embedding metadata about the type of key in the encoding itself, they can be parsed unambiguously. The process for encoding a public key in this format is:

- Encode the public key curve "point" as bytes. Be sure to use the smaller "compact" or "compressed" representation. This is usually easy for `k256`, but might require a special argument or configuration for `p256` keys
- Prepend the appropriate curve multicodec value, as varint-encoded bytes, in front of the key bytes:
    - `p256` (compressed, 33 byte key length): `p256-pub`, code 0x1200, varint-encoded bytes: [0x80, 0x24]
    - `k256` (compressed, 33 byte key length): `secp256k1-pub`, code 0xE7, varint bytes: [0xE7, 0x01]
- Encode the combined bytes with with `base58btc`, and prefix with a `z` character, yielding a multibase-encoded string

The decoding process is the same in reverse, using the identified curve type as context.

To encode a key as a `did:key` identifier, use the above multibase encoding, and add the ASCII prefix `did:key:`. This identifier is used as an internal implementation detail in the DID PLC method.

Note that there is a variant legacy multibase encoding described in the [atproto DID specification document](/specs/did), which does not include a multicodec type value, and uses uncompressed byte encoding of keys. This format is deprecated.

### Encoded Examples

A P-256 public key, encoded in multibase (with multicodec), and as `did:key`:

```
zDnaembgSGUhZULN2Caob4HLJPaxBh92N7rtH21TErzqf8HQo
did:key:zDnaembgSGUhZULN2Caob4HLJPaxBh92N7rtH21TErzqf8HQo
```

A K-256 public key, encoded in multibase (with multicodec), and as `did:key`:

```
zQ3shqwJEJyMBsBXCWyCBpUBMqxcon9oHB7mCvx4sSpMdLJwc
did:key:zQ3shqwJEJyMBsBXCWyCBpUBMqxcon9oHB7mCvx4sSpMdLJwc
```

## Usage and Implementation Guidelines

There is no specific recommended byte or string encoding for private keys across the atproto ecosystem. Sometimes simple hex encoding is used, sometimes multibase with or without multicodec type information.


## Possible Future Changes

The set of supported cryptographic systems is expected to evolve slowly. There are significant interoperability and implementation advantages to having as few systems as possible at any point in time.


---
atproto/src/app/[locale]/specs/cryptography/ko.mdx
---
export const metadata = {
  title: '암호학',
  description:
    'AT Protocol에서 사용되는 암호 시스템, 곡선 및 키 유형',
}

# 암호학

현재 프로토콜 전반에서 두 가지 타원 곡선이 지원되며, 구현체는 둘 모두를 완벽하게 지원해야 합니다: {{ className: 'lead' }}

- `p256` 타원 곡선: 별칭 "NIST P-256", 별칭 `secp256r1` (여기서 `r`에 주의), 별칭 `prime256v1`
    - 이 곡선은 *WebCrypto API에 포함되어 있습니다*. 일반적으로 개인 기기 하드웨어(신뢰할 수 있는 플랫폼 모듈(TPM) 및 모바일 시큐어 인클레이브)와 클라우드 하드웨어 보안 모듈(HSM)에 의해 지원됩니다.
- `k256` 타원 곡선: 별칭 `secp256k1` (여기서 `k`에 주의)
    - 이 곡선은 *WebCrypto API에 포함되어 있지 않습니다*. Bitcoin 및 기타 암호화폐에서 사용되며, 그 결과 개인 비밀 키 관리 기술에 폭넓게 지원됩니다. 클라우드 HSM에서도 지원됩니다.

전체 곡선 이름을 모두 작성할 때 미묘한 시각적 차이가 있기 때문에, 우리는 이를 종종 `p256` 또는 `k256`이라고 부릅니다.

Bluesky의 atproto 참조 구현은 모든 문맥에서 두 곡선을 모두 지원하며, 기본적으로 `k256` 키 쌍을 생성합니다.

두 시스템의 키 포인트는 손실 없이 압축된(compressed) 표현을 가지며, 이는 공개 키를 공유할 때 유용합니다. 이는 보통 `k256`에서는 기본적으로 지원되지만, `p256`의 경우 추가 메서드나 복잡한 설정이 필요할 수 있습니다. 이에 대해서는 [02, 03 or 04? So What Are Compressed and Uncompressed Public Keys?](https://medium.com/asecuritysite-when-bob-met-alice/02-03-or-04-so-what-are-compressed-and-uncompressed-public-keys-6abcb57efeb6)에서 자세히 읽어보실 수 있습니다.

atproto에서 데이터를 서명할 때의 일반적인 패턴은 데이터를 DAG-CBOR로 인코딩하고, 해당 CBOR 바이트를 SHA-256으로 해시하여 원시 바이트(헥스 인코딩 문자열이 아님)를 생성한 다음, 이 해시 바이트에 서명하는 것입니다.


## ECDSA 서명 변조 가능성

일부 ECDSA 서명은 변환되어 새로운 구분되지만 여전히 유효한 서명을 만들어낼 수 있습니다. 이는 서명에 사용된 개인 키나 데이터에 접근할 필요 없이 발생합니다. 이 속성을 이용한 공격의 범위는 제한적이지만, 예상치 못한 특성입니다.

특히 `k256`의 경우, 이는 [Bitcoin BIP-0062](https://github.com/bitcoin/bips/blob/master/bip-0062.mediawiki)에서 논의된 바와 같이 "low-S"와 "high-S" 서명 간의 구분에 해당합니다.

atproto에서는 `p256`과 `k256` 곡선 모두에 대해 "low-S" 서명 변형의 사용이 요구됩니다.

atproto에서는 서명을 원시 바이트 값으로 단순 비교하는 대신, 항상 암호화 라이브러리에서 제공하는 검증 루틴을 사용하여 서명을 검증해야 합니다.


## 공개 키 인코딩

공개 키를 문자열로 인코딩할 때, 선호되는 표현 방식은 multibase(특히 `base58btc` 사용)와 특정 키 유형을 나타내기 위한 multicode 접두사를 사용하는 것입니다. 인코딩 자체에 키 유형에 대한 메타데이터를 포함함으로써, 이를 명확하게 파싱할 수 있습니다. 이 형식으로 공개 키를 인코딩하는 과정은 다음과 같습니다:

- 공개 키 곡선의 "포인트"를 바이트로 인코딩합니다. 반드시 더 작은 "compact" 또는 "compressed" 표현 방식을 사용하세요. 이는 보통 `k256`에서는 간단하지만, `p256` 키의 경우 특별한 인자나 설정이 필요할 수 있습니다.
- 키 바이트 앞에 적절한 곡선 multicodec 값을 varint 인코딩된 바이트 형태로 붙입니다:
    - `p256` (압축된, 33바이트 키 길이): `p256-pub`, 코드 0x1200, varint 인코딩 바이트: [0x80, 0x24]
    - `k256` (압축된, 33바이트 키 길이): `secp256k1-pub`, 코드 0xE7, varint 인코딩 바이트: [0xE7, 0x01]
- 결합된 바이트를 `base58btc`로 인코딩하고, `z` 문자를 접두사로 추가하여 multibase 인코딩 문자열을 생성합니다.

디코딩 과정은 식별된 곡선 유형을 문맥으로 사용하여 반대로 진행됩니다.

키를 `did:key` 식별자로 인코딩하려면, 위의 multibase 인코딩을 사용하고 ASCII 접두사 `did:key:`를 추가합니다. 이 식별자는 DID PLC 메서드의 내부 구현 세부사항으로 사용됩니다.

[atproto DID 사양 문서](/specs/did)에서 설명된 변형된 레거시 multibase 인코딩은 multicodec 유형 값을 포함하지 않고 키의 압축되지 않은 바이트 인코딩을 사용합니다. 이 형식은 더 이상 권장되지 않습니다.

### 인코딩 예시

아래는 multicodec을 포함한 multibase 인코딩 및 `did:key` 형식으로 인코딩된 P-256 공개 키 예시입니다:

```
zDnaembgSGUhZULN2Caob4HLJPaxBh92N7rtH21TErzqf8HQo
did:key:zDnaembgSGUhZULN2Caob4HLJPaxBh92N7rtH21TErzqf8HQo
```

아래는 multicodec을 포함한 multibase 인코딩 및 `did:key` 형식으로 인코딩된 K-256 공개 키 예시입니다:

```
zQ3shqwJEJyMBsBXCWyCBpUBMqxcon9oHB7mCvx4sSpMdLJwc
did:key:zQ3shqwJEJyMBsBXCWyCBpUBMqxcon9oHB7mCvx4sSpMdLJwc
```


## 사용 및 구현 가이드라인

atproto 생태계 전반에서 개인 키에 대해 특정 추천 바이트 또는 문자열 인코딩 방식은 없습니다. 때로는 단순 헥스 인코딩이 사용되며, 때로는 multicodec 유형 정보가 포함되거나 생략된 multibase가 사용됩니다.


## 향후 변경 가능성

지원되는 암호 시스템 집합은 점진적으로 발전할 것으로 예상됩니다. 가능한 한 적은 시스템을 유지하는 것은 상호 운용성과 구현 측면에서 상당한 이점을 제공합니다.


---
atproto/src/app/[locale]/specs/cryptography/page.tsx
---
export const metadata = {
  title: 'Cryptography',
  description:
    'Cryptographic systems, curves, and key types used in AT Protocol',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/oauth/en.mdx
---
export const metadata = {
  title: 'OAuth',
  description:
    'OAuth for Client/Server Authentication and Authorization',
}

# OAuth

<Note>
The OAuth profile for atproto is new and may be revised based on feedback from the development community and ongoing standards work. Read more about the rollout in the [OAuth Roadmap](https://github.com/bluesky-social/atproto/discussions/2656).
</Note>

<Note>
This specification is authoritative, but is not an implementation guide and does not provide much background or context around design decisions. The earlier [design proposal](https://github.com/bluesky-social/proposals/tree/main/0004-oauth) is not authoritative but provides more context and examples. SDK documentation and the [client implementation guide](https://docs.bsky.app/docs/advanced-guides/oauth-client) are more approachable for developers.
</Note>

OAuth is the primary mechanism in atproto for clients to make authorized requests to PDS instances. Most user-facing software is expected to use OAuth, including "front-end" clients like mobile apps, rich browser apps, or native desktop apps, as well as "back-end" clients like web services.

See the [HTTP API specification](./xrpc) for other forms of auth in atproto, including legacy HTTP client sessions/tokens, and inter-service auth.

OAuth is a constantly evolving framework of standards and best practices, standardized by the IETF. atproto uses a specific "profile" of OAuth which mandates a particular combination of OAuth standards, as described in this document.

At a high level, we start with the "OAuth 2.1" ([`draft-ietf-oauth-v2-1`](https://datatracker.ietf.org/doc/draft-ietf-oauth-v2-1/)) evolution of OAuth 2.0, which means:

- only the "authorization code" OAuth 2.0 grant type is supported, not "implicit" or other grant types
- mandatory Proof Key for Code Exchange (PKCE, [RFC 7636](https://datatracker.ietf.org/doc/html/rfc7636))
- security best practices ([Best Current Practice for OAuth 2.0 Security](https://datatracker.ietf.org/doc/html/rfc9700) and [`draft-ietf-oauth-browser-based-apps`](https://datatracker.ietf.org/doc/html/draft-ietf-oauth-browser-based-apps)) are required

Unlike a centralized app platform, in atproto there are many independent server implementations, so server discovery and client registration are automated using a combination of public auth server metadata and public client metadata. The `client_id` is a fully-qualified web URL pointing to the public client metadata (JSON document). There is no `client_secret` shared between servers and clients. When initiating a login with a handle or DID, an atproto-specific identity resolution step is required to discover the account’s PDS network location.

In OAuth terminology, an atproto Personal Data Server (PDS) is a "Resource Server" to which authorized HTTP requests are made using access tokens. Sometimes the PDS is also the "Authorization Server" - which services OAuth authorization flows and token requests - while in other situations a separate "entryway" service acts as the Authorization Server for multiple PDS instances. Clients from a metadata file from the PDS to discover the Authorization Server network location.

DPoP (with mandatory server issued nonces) is required to bind auth tokens to specific client software instances (eg, end devices or browser sessions). Pushed Authentication Requests (PAR) are used to streamline the authorization request flow. "Confidential" clients use JWTs signed with a secret key to authenticate the client software to Authorization Servers when making authorization requests.

Automated client registration using client metadata is one of the more novel aspects of OAuth in atproto. As of August 2024, client metadata is still an Internet Draft ([`draft-parecki-oauth-client-id-metadata-document`](https://datatracker.ietf.org/doc/draft-parecki-oauth-client-id-metadata-document/)); it should not be confused with the existing "Dynamic Client Registration" standard ([RFC 7591](https://datatracker.ietf.org/doc/html/rfc7591)). We are hopeful other open protocols will adopt similar automated registration flows in the future, but there may not be general OAuth ecosystem support for some time.

OAuth 2.0 is traditionally an authorization (`authz`) system, not an authentication (`authn`) system, meaning that it is not always a solution for pure account authentication use cases, such as "Signup/Login with XYZ" identity integrations. OpenID Connect (OIDC), which builds on top of OAuth 2.0, is usually the recommended standard for identity authentication. Unfortunately, the current version of OIDC does not enable authentication of atproto identities in a secure and generic way. The atproto profile of OAuth includes a (mandatory) mechanism for account authentication during the authorization flow and can be used for atproto identity authentication use cases.

## Clients

This section describes requirements for OAuth clients, which are enforced by Authorization Servers.

OAuth client software is identified by a globally unique `client_id`. Distinct variants of client software may have distinct `client_id` values; for example the browser app and Android (mobile OS) variants of the same software might have different `client_id` values. As required by the [`draft-parecki-oauth-client-id-metadata-document`](https://datatracker.ietf.org/doc/draft-parecki-oauth-client-id-metadata-document) specification draft, the `client_id` must be a fully-qualified web URL from which the client-metadata JSON document can be fetched. For example, `https://app.example.com/client-metadata.json`. Some more about the `client_id`:

- it must be a well-formed URL, following the W3C URL specification
- the schema must be `https://`, and there must not be a port number included. Note that there is a special exception for `http://localhost` `client_id` values for development, see details below
- the path does not need to include `client-metadata.json`, but it is helpful convention

Authorization Servers which support both the atproto OAuth profile and other forms of OAuth should take care to prevent `client_id` value collisions. For example, `client_id` values for clients which are not auto-registered should never have the prefix `https://` or `http://`.

### Types of Clients

All atproto OAuth clients need to meet a core set of standards and requirements, but there are a few variations in capabilities (such as session lifetime) depending on the security properties of the client itself.

As described in the OAuth 2.0 specification ([RFC 6749](https://datatracker.ietf.org/doc/html/rfc6749)), every client is one of two broad types:

- **confidential clients** are clients which can authenticate themselves to Authorization Servers using a cryptographic signing key. When a client is authenticated through a public key, refresh tokens are bound to the key that was used when initially issued. Note that this form of client authentication is distinct from DPoP: the client authentication key is common to all client sessions (although it can be rotated). This usually means that there is a web service controlled by the client which holds the key. Because they are authenticated and can revoke tokens in a security incident (by removing keys associated with compromised accounts from their advertised JWKS), confidential clients may be trusted with longer session and token lifetimes.
- **public clients** do not authenticate using a client signing key, either because they don’t have a server-side component (the client software all runs on end-user devices), or they simply chose not to implement it.

It is acceptable for a web service to act as a public client, and conversely it is possible for mobile apps and browser apps to coordinate with a token-mediating backend service and for the combination to form a confidential client. Mobile apps and browser apps can also adopt a "backend-for-frontend" (BFF) architecture with a web service backend acting as the OAuth client. This document will use the "public" vs "confidential" client terminology for clarity.

The environment a client runs in also impacts the type of redirect (callback) URLs it uses during the Authorization Flow:

- **web clients** include web services and browser apps. Redirect URLs are regular web URLs which open in a browser.
- **native clients** include some mobile and desktop native clients. Redirect URLs may use platform-specific app callback schemes to open in the app itself.

Authorization Servers may maintain a set of "trusted" clients, identified by `client_id`. Because any client could use unverified client metadata to impersonate a better-known app or brand, Authorization Servers should not display such metadata to end users in the Authorization Interface by default. Trusted clients can have additional metadata shown, such as a readable name (`client_name`), project URI (`client_uri`, which may have a different domain/origin than `client_id`) and logo (`logo_uri`). See the "Security Considerations" section for more details.

Clients which are only using atproto OAuth for account authentication (without authorization to access PDS resources) should request minimal scopes (see "Scopes" section), but still need to implement most of the authorization flow. In particular, it is critical that they check the `sub` field in a token response to verify the account identity (this is an atproto-specific detail).

### Client ID Metadata Document

<Note>
The Client ID Metadata Document specification ([`draft-parecki-oauth-client-id-metadata-document`](https://datatracker.ietf.org/doc/draft-parecki-oauth-client-id-metadata-document/) is still a draft and may evolve over time. Our intention is to evolve and align with subsequent drafts and any final standard, while minimizing disruption and breakage with existing implementations.
</Note>

Clients must publish a "client metadata" JSON file on the public web. This will be fetched dynamically by Authorization Servers as part of the authorization request (PAR) and at other times during the session lifecycle. The response HTTP status must be 200 (not another 2xx or a redirect), with a JSON object body with the correct `Content-Type` (`application/json`).

Authorization Servers need to fetch client metadata documents from the public web. They should use a hardened HTTP client for these requests (see "OAuth Security Considerations"). Servers may cache client metadata responses, optionally respecting HTTP caching headers (within limits). Minimum and maximum cache TTLs are not currently specified, but should be chosen to ensure that auth token requests predicated on stale confidential client authentication keys (`jwks` or `jwks_uris`) are rejected in a timely manner.

The following fields are relevant for all client types:

- `client_id` (string, required): the `client_id`. Must exactly match the full URL used to fetch the client metadata file itself
- `application_type` (string, optional): must be one of `web` or `native`, with `web` as the default if not specified. Note that this is field specified by OpenID/OIDC, which we are borrowing. Used by the Authorization Server to enforce the relevant "best current practices".
- `grant_types` (array of strings, required): `authorization_code` must always be included. `refresh_token` is optional, but must be included if the client will make token refresh requests.
- `scope` (string, sub-strings space-separated, required): all scope values which *might* be requested by this client are declared here. The `atproto` scope is required, so must be included here. See "Scopes" section.
- `response_types` (array of strings, required): `code` must be included.
- `redirect_uris` (array of strings, required): at least one redirect URI is required. See Authorization Request Fields section for rules about redirect URIs, which also apply here.
- `token_endpoint_auth_method` (string, optional): confidential clients must set this to  `private_key_jwt`.
- `token_endpoint_auth_signing_alg` (string, optional): `none` is never allowed here. The current recommended and most-supported algorithm is `ES256`, but this may evolve over time. Authorization Servers will compare this against their supported algorithms.
- `dpop_bound_access_tokens` (boolean, required): DPoP is mandatory for all clients, so this must be present and `true`
- `jwks` (object with array of JWKs, optional): confidential clients must supply at least one public key in JWK format for use with JWT client authentication. Either this field or the `jwks_uri` field must be provided for confidential clients, but not both.
- `jwks_uri` (string, optional): URL pointing to a JWKS JSON object. See `jwks` above for details.

These fields are optional but recommended:

- `client_name` (string, optional): human-readable name of the client
- `client_uri` (string, optional): not to be confused with `client_id`, this is a homepage URL for the client. If provided, the `client_uri` must have the same hostname as `client_id`.
- `logo_uri` (string, optional): URL to client logo. Only `https:` URIs are allowed.
- `tos_uri` (string, optional): URL to human-readable terms of service (ToS) for the client. Only `https:` URIs are allowed.
- `policy_uri` (string, optional): URL to human-readable privacy policy for the client. Only `https:` URIs are allowed.

See "OAuth Security Considerations" below for when `client_name`, `client_uri`, and `logo_uri` will or will not be displayed to end users.

Additional optional client metadata fields are enumerated with [IANA](https://www.iana.org/assignments/oauth-parameters/oauth-parameters.xhtml#client-metadata). Note that these are shared with the "Dynamic Client Registration" standard, which is not used directly by the atproto OAuth profile.

### Localhost Client Development

When working with a development environment (Authorization Server and Client), it may be difficult for developers to publish in-progress client metadata at a public URL so that authorization servers can access it. This may even be true for development environments using a containerized Authorization Server and local DNS, because of SSRF protections against local IP ranges.

To make development workflows easier, a special exception is made for clients with `client_id` having origin `http://localhost` (with no port number specified). Authorization Servers are encouraged to support this exception - including in production environments - but it is optional.

In a localhost `client_id` scenario, the Authorization Server should verify that the scheme is `http`, and that the hostname is exactly `localhost` with no port specified. IP addresses (`127.0.0.1`, etc) are not supported. The path parameter must be empty (`/`).

In the Authorization Request, the `redirect_uri` must match one of those supplied (or a default). Path components must match, but port numbers are not matched.

Some metadata fields can be configured via query parameter in the `client_id` URL (with appropriate URL encoding):

- `redirect_uri` (string, multiple query parameters allowed, optional): allows declaring a local redirect/callback URL, with path component matched but port numbers ignored. The default values (if none are supplied) are `http://127.0.0.1/` and `http://[::1]/`.
- `scope` (string with space-separated values, single query parameter allowed, optional): the set of scopes which might be requested by the client. Default is `atproto`.

The other parameters in the virtual client metadata document will be:

- `client_id` (string): the exact `client_id` (URL) used to generate the virtual document
- `client_name` (string): a value chosen by the Authorization Server (e.g. "Development client")
- `response_types` (array of strings): must include `code`
- `grant_types` (array of strings): `authorization_code` and `refresh_token`
- `token_endpoint_auth_method`: `none`
- `application_type`: `native`
- `dpop_bound_access_tokens`: `true`

Note that this works as a public client, not a confidential client.

## Identity Authentication

As mentioned in the introduction, OAuth 2.0 generally provides only Authorization (`authz`), and additional standards like OpenID/OIDC are used for Authentication (`authn`). The atproto profile of OAuth requires authentication of account identity and supports the use case of simple identity authentication without additional resource access authorization.

In atproto, account identity is anchored in the account DID, which is the permanent, globally unique, publicly resolvable identifier for the account. The DID resolves to a DID document which indicates the current PDS host location for the account. That PDS (combined with an optional entryway) is the authorization authority and the OAuth Authorization Server for the account. When speaking to any Authorization Server, it is critical (mandatory) for clients to confirm that it is actually the authoritative server for the account in question, which means independently resolving the account identity (by DID) and confirming that the Authorization Server matches. It is also critical (mandatory) to confirm at the end of an authorization flow that the Authorization Server actually authorized the expected account. The reason this is necessary is to confirm that the Authorization Server is authoritative for the account in question. Otherwise a malicious server could authenticate arbitrary accounts (DIDs) to the client.

Clients can start an auth flow in one of two ways:

- starting with a public account identifier, provided by the user: handle or DID
- starting with a server hostname, provided by the user: PDS or entryway, mapping to either Resource Server and/or Authorization Server

One use case for starting with a server instead of an account identifier is when the user does not remember their full account handle or only knows their account email.  Another is for authentication when a user’s handle is broken. The user will still need to know their hosting provider in these situation.

When starting with an account identifier, the client must resolve the atproto identity to a DID document. If starting with a handle, it is critical (mandatory) to bidirectionally verify the handle by checking that the DID document claims the handle (see atproto Handle specification). All handle resolution techniques and all atproto-blessed DID methods must be supported to ensure interoperability with all accounts.

In some client environments, it may be difficult to resolve all identity types. For example, handle resolution may involve DNS TXT queries, which are not directly supported from browser apps. Client implementations might use alternative techniques (such as DNS-over-HTTP) or could make use of a supporting web service to resolve identities.

Because authorization flows are security-critical, any caching of identity resolution should choose cache lifetimes carefully. Cache lifetimes of less than 10 minutes are recommended for auth flows specifically.

The resolved DID should be bound to the overall auth session and should be used as the primary account identifier within client app code. Handles (when verified) are acceptable to display in user interfaces, but may change over time and need to be re-verified periodically. When passing an account identifier through to the Authorization Server as part of the Authorization Request in the `login_hint`, it is recommended to use the exact account identifier supplied by the user (handle or DID) to ensure any sign-in flow is consistent (users might not recognize their own account DID).

At the end of the auth flow, when the client does an initial token fetch, the Authorization Server must return the account DID in the `sub` field of the JSON response body. If the entire auth flow started with an account identifier, it is critical for the client to verify that this DID matches the expected DID bound to the session earlier; the linkage from account to Authorization Server will already have been verified in this situation.

If the auth flow instead starts with a server (hostname or URL), the client will first attempt to fetch Resource Server metadata (and resolve to Authorization Server if found) and then attempt to fetch Authorization Server metadata. See "Authorization Server" section for server metadata fetching. If either is successful, the client will end up with an identified Authorization Server. The Authorization Request flow will proceed without a `login_hint` or account identifier being bound to the session, but the Authorization Server `issuer` will be bound to the session.

After the auth flow continues and an initial token request succeeds, the client will parse the account identifier from the `sub` field in the token response. At this point, the client still cannot trust that it has actually authenticated the indicated account. It is critical for the client to resolve the identity (DID document), extract the declared PDS host, confirm that the PDS (Resource Server) resolves to the Authorization Server bound to the session by fetching the Resource Server metadata, and fetch the Authorization Server metadata to confirm that the `issuer` field matches the Authorization Server origin (see [`draft-ietf-oauth-v2-1` section 7.3.1](https://datatracker.ietf.org/doc/html/draft-ietf-oauth-v2-1-11#section-7.13.1) regarding this last point).

To reiterate, it is critical for all clients - including those only interested in atproto Identity Authentication - to go through the entire Authorization flow and to verify that the account identifier (DID) in the `sub` field of the token response is consistent with the Authorization Server hostname/origin (`issuer`).

## Authorization Scopes

OAuth scopes allow more granular control over the resources and actions a client is granted access to.

The special `atproto` scope is required for all atproto OAuth sessions. The semantics are somewhat similar to the `openid` scope: inclusion of it confirms that the client is using the atproto profile of OAuth and will comply with all the requirements laid out in this specification. No access to any atproto-specific PDS resources will be granted without this scope included.

Authorization Servers may support other profiles of OAuth if client does not include the `atproto` scope. For example, an Authorization Server might function as both an atproto PDS/entryway, and support other protocols/standards at the same time.

Use of the atproto OAuth profile, as indicated by the `atproto` scope, means that the Authorization Server will return the atproto account DID as an account identifier in the `sub` field of token requests. Authorization Servers must return `atproto` in `scopes_supported` in their metadata document, so that clients know they support the atproto OAuth profile. A client may include only the `atproto` scope if they only need account authentication - for example a "Login with atproto" use case. Unlike OpenID, profile metadata in atproto is generally public, so an additional authorization scope for fetching profile metadata is not needed.

The OAuth 2.0 specification does not require Authorization Servers to return the granted scopes in the token responses unless the scope that was granted is different from what the client requested. In the atproto OAuth profile, servers must always return the granted scopes in the token response. Clients should reject token responses if they don't contain a `scope` field, or if the `scope` field does not contain `atproto`.

The intention is to support flexible scopes based on Lexicon namespaces (NSIDs) so that clients can be given access only to the specific content and API endpoints they need access to. Until the design of that scope system is ready, the atproto profile of OAuth defines two transitional scopes which align with the permissions granted under the original "session token" auth system:

- `transition:generic`: broad PDS account permissions, equivalent to the previous "App Password" authorization level.
    - write (create/update/delete) any repository record type
    - upload blobs (media files)
    - read and write any personal preferences
    - API endpoints and service proxying for most Lexicon endpoints, to any service provider (identified by DID)
    - ability to generate service auth tokens for the specific API endpoints the client has access to
    - no account management actions: change handle, change email, delete or deactivate account, migrate account
    - no access to DMs (the `chat.bsky.*` Lexicons), specifically
- `transition:chat.bsky`: equivalent to adding the "DM Access" toggle for "App Passwords"
    - API endpoints and service proxying for the `chat.bsky` Lexicons specifically
    - ability to generate service auth tokens for the `chat.bsky` Lexicons
    - this scope depends on and does not function without the  `transition:generic` scope
- `transition:email`: access to the account email address
    - email address (and confirmation status) gets included in response to `com.atproto.server.getSession` endpoint

## Authorization Requests

This section details standards and requirements specific to Authorization Requests.

PKCE and PAR are required for all client types and Authorization Servers. Confidential clients authenticate themselves using JWT client assertions.

### Request Fields

A summary of fields relevant to authorization requests with the atproto OAuth profile:

- `client_id` (string, required): identifies the client software. See "Clients" section above for details.
- `response_type` (string, required): must be `code`
- `code_challenge` (string, required): the PKCE challenge value. See "PKCE" section.
- `code_challenge_method` (string, required): which code challenge method is used, for example `S256`. See "PKCE" section.
- `state` (string, required): random token used to verify the authorization request against the response. See below.
- `redirect_uri` (string, required): must match against URIs declared in client metadata and have a format consistent with the `application_type` declared in the client metadata. See below.
- `scope` (string with space-separated values, required): must be a subset of the scopes declared in client metadata. Must include `atproto`. See "Scopes" section.
- `client_assertion_type` (string, optional): used by confidential clients to describe the client authentication mechanism. See "Confidential Client" section.
- `client_assertion` (string, optional): only used for confidential clients, for client authentication. See "Confidential Client" section.
- `login_hint` (string, optional): account identifier to be used for login. See "Authorization Interface" section.

The `client_secret` value, used in many other OAuth profiles, should not be included.

The `state` parameter in client authorization requests is mandatory. Clients should use randomly-generated tokens for this parameter and not have collisions or reuse tokens across any combination of device, account, or session. Authorization Servers should reject duplicate state parameters, but are not currently required to track state values across accounts or sessions. The `state` parameter is effectively used to verify the `issuer` later, and it is important that the parameter can not be forged or guessed by an untrusted party.

For web clients, the `redirect_uri` is a HTTPS URL which will be redirected in the browser to return users to the application at the end of the Authorization flow. The URL may include a port number, but not if it is the default port number. The `redirect_uri` must match one of the URIs declared in the client metadata and the Authorization Server must verify this condition.

There is a special exception for the localhost development workflow to use `http://127.0.0.1` or `http://[::1]` URLs, with matching rules described in the "Localhost Client Development" section. These clients use web URLs, but have `application_type` set to `native` in the generated client metadata.

For native clients, the `redirect_uri` may use a custom URI scheme to have the operating system redirect the user back to the app, instead of a web browser. Native clients are also allowed to use an HTTPS URL. Any custom scheme must match the `client_id` hostname in reverse-domain order. The URI scheme must be followed by a single colon (`:`) then a single forward slash (`/`) and then a URI path component. For example, an app with `client_id`  [`https://app.example.com/client-metadata.json`](https://app.example.com/client-metadata.json) could have a `redirect_uri` of `com.example.app:/callback`.

Native clients are also allowed to use an HTTPS URL. In this case, the URL origin must be the same as the `client_id`. One example use-case is "Apple Universal Links".

Clients may include additional optional authorization request parameters - and servers may process them - but they are not required to. Refer to other OAuth standards and the [IANA OAuth parameter registry](https://www.iana.org/assignments/oauth-parameters/oauth-parameters.xhtml).

### Proof Key for Code Exchange (PKCE)

PKCE is mandatory for all Authorization Requests. Clients must generate new, unique, random challenges for every authorization request. Authorization Servers must prevent reuse of `code_challenge` values across sessions (at least within some reasonable time frame, such as a 24 hour period).

The `S256` challenge method must be supported by all clients and Authorization Servers; see [RFC 7636](https://datatracker.ietf.org/doc/html/rfc7636) for details. The `plain` method is not allowed. Additional methods may in theory be supported if both client and server support them.

Authorization Servers should reject reuse of a `code` value, and revoke any outstanding sessions and tokens associated with the earlier use of the `code` value.

### Pushed Authorization Requests (PAR)

Authorization Servers must support PAR and clients of all types must use PAR for Authorization Requests.

Authorization Servers must set `require_pushed_authorization_requests` to `true` in their server metadata document and include a valid URL in `pushed_authorization_request_endpoint`. See [RFC 9207](https://datatracker.ietf.org/doc/html/rfc9207) for requirements on this URL.

Clients make an HTTPS POST request to the `pushed_authorization_request_endpoint` URL, with the request parameters in the form-encoded request body. They receive a `request_uri` (not to be confused with `redirect_uri`) in the JSON response object. When they redirect the user to the authorization endpoint (`authorization_endpoint`), they omit most of the request parameters they already sent and include this `request_uri` along with `client_id` as query parameters instead.

<Note>
PAR is a relatively new and less-supported standard, and the requirement to use PAR may be relaxed if it is found to be too onerous a requirement for client implementations. In that case, Authorization Servers would be required to support both PAR and non-PAR requests with PAR being optional for clients.
</Note>

### Confidential Client Authentication

Confidential clients authenticate themselves during the Authorization Request using a JWT client assertion. Authorization Servers may grant confidential clients longer token/session lifetimes. See "Tokens" section for more context.

The client assertion type to use is `urn:ietf:params:oauth:client-assertion-type:jwt-bearer`, as described in "JSON Web Token (JWT) Profile for OAuth 2.0 Client Authentication and Authorization Grants" ([RFC 7523](https://datatracker.ietf.org/doc/html/rfc7523)). Clients and Authorization Servers currently must support the `ES256` cryptographic system. The set of recommended systems/algorithms is expected to evolve over time.

Additional requirements:

- confidential clients must publish one or more client authentication keys (public key) in the client metadata. This can be either direct JWK format as JSON in the `jwks` field, or as a separate JWKS JSON object on the web linked by a `jwks_uri` URL. A `jwks_uri` URL must be a valid fully qualified URL with `https://` scheme.
- confidential clients should periodically rotate client keys, adding new keys to the JWKS set and using then for new sessions, then removing old keys once they are no longer associated with any active auth sessions
- confidential clients must include `token_endpoint_auth_method` as `private_key_jwt` in their client metadata document
- confidential clients are expected to immediately remove client authentication keys from their client metadata if the key has been leaked or compromised
- Authorization Servers must bind active auth sessions for confidential clients to the client authentication key used at the start of the session. They do so by ensuring that the key's identifier (`kid`), signature algorithm (`alg`) and thumbprint of the signing key (`jkt`) of the client attestation JWT stay the same for the entire life time of the session. The server must revoke the session and reject further token refreshes if the client authentication key becomes absent from the client metadata. This means the Authorization Server must periodically re-fetch client metadata (and related keyset).
- The client assertion JWT must include an `iat` claim (issued at) timestamp. Authorization Servers should not reject client assertion JWTs generated less than a minute ago.
- The client assertion JWT must include an `jti` claim (JWT ID) with a unique value to prevent replays. Authorization Servers must ensure uniqueness of `jti` values over the full token validity time period.
- The `aud` claim (audience) of the client assertion JWT must be the Authorization Server's `issuer`.

## Tokens and Session Lifetime

Access tokens are used to authorize client requests to the account's PDS ("Resource Server"). From the standpoint of the client they are opaque, but they are often signed JWTs including an expiration time. Depending on the PDS implementation, it may or may not be possible to revoke individual access tokens in the event of a compromise, so they must be restricted to a relatively short lifetime.

Refresh tokens are used to request new tokens (of both types) from the Authorization Server (PDS or entryway). They are also opaque from the standpoint of clients. Auth sessions can be revoked - invalidating the refresh tokens - so they may have a longer lifetime. In the atproto OAuth profile, refresh tokens are generally single-use, with the "new" refresh token replacing that used in the token request. This means client implementations may need locking primitives to prevent concurrent token refresh requests.

To request refresh tokens, the client must declare `refresh_token` as a grant type in their client metadata.

Tokens are always bound to a unique session DPoP key. Tokens must not be shared or reused across client devices. They must also be uniquely bound to the client software (`client_id`). The overall session ends when the access and refresh tokens can no longer be used.

The specific lifetime of sessions, access tokens, and refresh tokens is up to the Authorization Server implementation and may depend on security assessments of client type and reputation.

Some guidelines and requirements:

- access token lifetimes should be less than 30 minutes in all situations. If the server cannot revoke individual access tokens then the maximum is 15 minutes, and 5 minutes is recommended.
- for "untrusted" public clients, overall session lifetime should be limited to 7 days, and the lifetime of individual refresh tokens should be limited to 24 hours
- for confidential clients, the overall session lifetime may be unlimited. Individual refresh tokens should have a lifetime limited to 180 days
- confidential clients must use the same client authentication key and assertion method for refresh token requests that they did for the initial authentication request

## Demonstrating Proof of Possession (DPoP)

The atproto OAuth profile mandates use of DPoP for all client types when making auth token requests to the Authorization Server and when making authorized requests to the Resource Server. See [RFC 9449](https://datatracker.ietf.org/doc/html/rfc9449) for details.

Clients must initiate DPoP in the initial authorization request (PAR).

Server-provided DPoP nonces are mandatory. The Resource Server and Authorization Server may share nonces (especially if they are the same server) or they may have separate nonces. Clients should track the DPoP nonce per account session and per server. Servers must rotate nonces periodically, with a maximum lifetime of 5 minutes. Servers may use the same nonce across all client sessions and across multiple requests at any point in time. Servers should accept recently-stale (old) nonces to make rotation smoother for clients with multiple concurrent request in-flight. Clients should be resilient to unexpected nonce updates in the form of HTTP 400 errors and should retry those failed requests. Clients must reject responses missing a `DPoP-Nonce` header (case insensitive), if the request included DPoP.

Clients must generate and sign a unique DPoP token (JWT) for every request. Each DPoP request JWT must have a unique (randomly generated) `jti` nonce. Servers should prevent token replays by tracking `jti` nonces and rejecting re-use. They can restrict their client-generated `jti` nonce history to the server-generated DPoP nonce so that they do not need to track an endlessly growing set of nonces.

The `ES256` (NIST "P-256") cryptographic algorithm must be supported by all clients and servers for DPoP JWT signing. The set of algorithms recommended for use is expected to evolve over time. Clients and Servers may implement additional algorithms and declare them in metadata documents to facilitate cryptographic evolution and negotiation.

## Authorization Servers

To enable browser apps, Authorization Servers must support HTTP CORS requests/headers on relevant endpoints, including server metadata, auth requests (PAR), and token requests.

### Server Metadata

Both Resource Servers (PDS instances) and Authorization Servers (PDS or entryway) need to publish metadata files at well-known HTTPS endpoints.

Resource Server (PDS) metadata must comply with the "OAuth 2.0 Protected Resource Metadata" ([`draft-ietf-oauth-resource-metadata`](https://datatracker.ietf.org/doc/draft-ietf-oauth-resource-metadata/)) draft specification. A summary of requirements:

- the URL path is `/.well-known/oauth-protected-resource`
- response must be an HTTP 200 (not 2xx or redirect), and must be a valid JSON object with content type `application/json`
- must contain an `authorization_servers` array of strings, with a single element, which is a fully-qualified URL

The Authorization Server URL may be the same origin as the Resource Server (PDS), or might point to a separate server (e.g. entryway). The URL must be a simple origin URL: `https` scheme, no credentials (user:password), no path, no query string or fragment. A port number is allowed, but a default port (443 for HTTPS) must not be included.

The Authorization Server also publishes metadata, complying with the "OAuth 2.0 Authorization Server Metadata" ([RFC 8414](https://datatracker.ietf.org/doc/html/rfc8414)) standard. A summary of requirements:

- the URL path is `/.well-known/oauth-authorization-server`
- response must be an HTTP 200 (not 2xx or redirect), and must be a valid JSON object with content type `application/json`
- `issuer` (string, required): the "origin" URL of the Authorization Server. Must be a valid URL, with `https` scheme. A port number is allowed (if that matches the origin), but the default port (443 for HTTPS) must not be specified. There must be no path segments. Must match the origin of the URL used to fetch the metadata document itself.
- `authorization_endpoint` (string, required): endpoint URL for authorization redirects
- `token_endpoint` (string, required): endpoint URL for token requests
- `response_types_supported` (array of strings, required): must include `code`
- `grant_types_supported` (array of strings, required): must include `authorization_code` and `refresh_token` (refresh tokens must be supported)
- `code_challenge_methods_supported` (array of strings, required): must include `S256` (see "PKCE" section)
- `token_endpoint_auth_methods_supported` (array of strings, required): must include both `none` (public clients) and `private_key_jwt` (confidential clients)
- `token_endpoint_auth_signing_alg_values_supported` (array of strings, required): must not include `none`. Must include `ES256` for now. Cryptographic algorithm suites are expected to evolve over time.
- `scopes_supported` (array of strings, required): must include `atproto`. If supporting the transitional grants, they should be included here as well. See "Scopes" section.
- `authorization_response_iss_parameter_supported` (boolean): must be `true`
- `require_pushed_authorization_requests` (boolean): must be `true`. See "PAR" section.
- `pushed_authorization_request_endpoint` (string, required): must be the PAR endpoint URL. See "PAR" section.
- `dpop_signing_alg_values_supported` (array of strings, required): currently must include `ES256`. See "DPoP" section.
- `require_request_uri_registration` (boolean, optional): default is `true`; does not need to be set explicitly, but must not be `false`
- `client_id_metadata_document_supported` (boolean, required): must be `true`. See "Client ID Metadata" section.

The `issuer` ("origin") is the overall identifier for the Authorization Server.


### Authorization Interface

The Authorization Server (PDS/entryway) must implement a web interface for users to authenticate with the server, approve (or reject) Authorization Requests from clients, and manage active sessions. This is called the "Authorization Interface".

Server implementations can chose their own technology for user authentication and account recovery: secure cookies, email, various two-factor authentication, passkeys, external identity providers (including upstream OpenID/OIDC), etc. Servers may also support multiple concurrent auth sessions with users.

When a client redirects to the Authorization Server’s authorization URL (the declared `authorization_endpoint`), the server first needs to authenticate the user. If there is no active auth session, the user may be prompted to log in. If a `login_hint` was provided in the Authorization Request, that can be used to pre-populate the login form. If there are multiple active auth sessions, the user could be prompted to select one from a list, or the `login_hint` could be used to auto-select. If there is a single active session, the interface can move to the approval view, possibly with the option to login as a different account. If a `login_hint` was supplied, the Authorization Server should only allow the user to authenticate with that account. Otherwise the overall authorization flow will fail when the client verifies the account identity (`sub` field).

The authorization approval prompt should identify the client app and describe the scope of authorization that has been requested.

The amount of client metadata that should be displayed may depend on whether the client is "trusted" by the Authorization Server; see the "Client" and "Security Concerns" sections. The full `client_id` URL should be displayed by default.

See the "Scopes" section for a description of scope options.

If a client is a confidential client and the user has already approved the same scopes for the same client in the past, the Authorization Server may allow "silent sign-in" by auto-approving the request. Authorization Servers can set their own policies for this flow: it may require explicit user configuration, or the client may be required to be "trusted".

Authorization Servers should separately implement a web interface which allows authenticated users to view active OAuth sessions and delete them.

## Summary of Authorization Flow

This is a high-level description of what an atproto OAuth authorization flow looks like. It assumes the user already has an atproto account.

The client starts by asking for the user’s account identifier (handle or DID), or for a PDS/entryway hostname. See "Identity Authentication" section for details.

For an account identifier, the client resolves the identity to a DID document, extracts the declared PDS URL, then fetches the Resource Server and Authorization Server locations. If starting with a server hostname, the client resolves that hostname to an Authorization Server. In either case, Authorization Server metadata is fetched and verified against requirements for atproto OAuth (see "Authorization Server" section).

The client next makes a Pushed Authorization Request via HTTP POST request. See "Authorization Request" section; some notable details include:

- a randomly generated `state` token is required, and will be used to reference this authorization request with the subsequent response callback
- PKCE is required, so a secret value is generated and stored, and a derived challenge is included in the request
- `scope` values are requested here, and must include `atproto`
- for confidential clients, a `client_assertion` is included, with type `jwt-bearer`, signed using the secret client authentication key
- the client generates a new DPoP key for the user/device/session and uses it starting with the PAR request
- if the auth flow started with an account identifier, the client should pass that starting identifier via the `login_hint` field
- atproto uses PAR, so the request will be sent as an HTTP POST request to the Authorization Server

The Authorization Server will receive the PAR request and use the `client_id` URL to resolve the client metadata document. The server validates the request and client metadata, then stores information about the session, including binding a DPoP key to the session. The server returns a `request_uri` token to the client, including a DPoP nonce via HTTP header.

The client receives the `request_uri` and prepares to redirect the user. At this point, the client usually needs to persist information about the session to some type of secure storage, so it can be read back after the redirect returns. This might be a database (for a web service backend) or web platform storage like IndexedDB (for a browser app). The client then redirects the user via browser to the Authorization Server’s auth endpoint, including the `request_uri` as a URL parameter. In any case, clients must not store session data directly in the `state` request param.

The Authorization Server uses the `request_uri` to look up the earlier Authorization Request parameters, authenticates the user (which might include sign-in or account selection), and prompts the user with the Authorization Interface. The user might refine any granular requested scopes, then approves or rejects the request. The Authorization Server redirects the user back to the `redirect_uri`, which might be a web callback URL, or a native app URI (for native clients).

The client uses URL query parameters (`state` and `iss`) to look up and verify session information. Using the `code` query parameter, the client then makes an initial token request to the Authorization Server’s token endpoint. The client completes the PKCE flow by including the earlier value in the `code_verifier` field. Confidential clients need to include a client assertion JWT in the token request; see the "Confidential Client" section. The Authorization Server validates the request and returns a set of tokens, as well as a `sub` field indicating the account identifier (DID) for this session, and the `scope` that is covered by the issued access token.

At this point it is critical (mandatory) for all clients to verify that the account identified by the `sub` field is consistent with the Authorization Server "issuer" (present in the `iss` query string), either by validating against the originally-supplied account DID, or by resolving the accounts DID to confirm the PDS is consistent with the Authorization Server. See "Identity Authentication" section. The Authorization always returns the scopes approved for the session in the `scopes` field (even if they are the same as the request, as an atproto OAuth profile requirement), which may reflect partial authorization by the user. Clients must reject the session if the response does not include `atproto` in the returned scopes.

Authentication-only clients can end the flow here.

Using the access token, clients are now able to make authorized requests to the PDS ("Resource Server"). They must use DPoP for all such requests, along with the access token. Tokens (both access and refresh) will need to be periodically "refreshed" by subsequent request to the Authorization Server token endpoint. These also require DPoP. See "Tokens and Session Lifetime" section for details.

## Security Considerations

There are a number of situations where HTTP URLs provided by external parties are fetched by both clients and providers (servers). Care must be taken to prevent harmful fetches due to maliciously crafted URLs, including hostnames which resolve to private or internal IP addresses. The general term for this class of security issue is Server-Side Request Forgery (SSRF). There is also a class of denial-of-service attacks involving HTTP requests to malicious servers, such as huge response bodies, TCP-level slow-loris attacks, etc. We strongly recommend using "hardened" HTTP client implementations/configurations to mitigate these attacks.

Any party can create a client and client metadata file with any contents at any time. Even the hostname in the `client_id` cannot be entirely trusted to represent the overall client: an untrusted user may have been able to upload the client metadata file to an arbitrary URL on the host. In particular, the `client_uri`, `client_name`, and `logo_uri` fields are not verified and could be used by a malicious actor to impersonate a legitimate client. It is strongly recommended for Authorization Servers to not display these fields to end users during the auth flow for unknown clients. Service operators may maintain a list of "trusted" `client_id` values and display the extra metadata for those apps only.

## Possible Future Changes

Client metadata requests (by the authorization server) might fail for any number of reasons: transient network disruptions, the client server being down for regular maintenance, etc. It seems brittle for the Authorization Server to immediately revoke access to active client sessions in this scenario. Maybe there should be an explicit grace period?

The requirement that resource server metadata only have a single URL reference to an authorization server might be relaxed.

The details around session and token lifetimes might change with further security review.


---
atproto/src/app/[locale]/specs/oauth/ko.mdx
---
export const metadata = {
  title: 'OAuth',
  description:
    '클라이언트/서버 인증 및 권한 부여를 위한 OAuth',
}

# OAuth

<Note>
atproto를 위한 OAuth 프로파일은 새로 도입되었으며, 개발 커뮤니티의 피드백과 진행 중인 표준 작업에 따라 수정될 수 있습니다. 자세한 내용은 [OAuth 로드맵](https://github.com/bluesky-social/atproto/discussions/2656)을 참조하세요.
</Note>

<Note>
이 명세는 권위 있는 표준이지만 구현 가이드는 아니며, 설계 결정의 배경이나 맥락에 대한 설명은 많지 않습니다. 초기 [설계 제안](https://github.com/bluesky-social/proposals/tree/main/0004-oauth)은 권위 있는 문서는 아니지만 보다 많은 배경과 예시를 제공합니다. 개발자에게는 SDK 문서나 [OAuth 클라이언트 구현 가이드](https://docs.bsky.app/docs/advanced-guides/oauth-client)가 더 이해하기 쉬울 수 있습니다.
</Note>

OAuth는 atproto에서 클라이언트가 PDS 인스턴스에 대해 권한 있는 요청을 수행하기 위한 주요 메커니즘입니다. 대부분의 사용자 대상 소프트웨어는 OAuth를 사용하도록 설계되어 있으며, 여기에는 모바일 앱, 리치 브라우저 앱, 네이티브 데스크톱 앱과 같은 "프론트엔드" 클라이언트뿐 아니라 웹 서비스와 같은 "백엔드" 클라이언트도 포함됩니다.

다른 인증 방식에 대해서는 atproto의 [HTTP API 명세](./xrpc)를 참고하세요. 여기에는 기존의 HTTP 클라이언트 세션/토큰 및 서비스 간 인증 방식이 포함되어 있습니다.

OAuth는 IETF에 의해 표준화된 표준 및 모범 사례의 지속적으로 발전하는 프레임워크입니다. atproto는 이 문서에서 설명하는 특정 OAuth 표준 조합을 의무화하는 OAuth의 "프로파일"을 사용합니다.

대략적으로 atproto는 다음과 같이 "OAuth 2.1"([`draft-ietf-oauth-v2-1`](https://datatracker.ietf.org/doc/draft-ietf-oauth-v2-1/))의 진화를 시작점으로 합니다. 즉:

- OAuth 2.0의 "authorization code" 그랜트 타입만 지원하며, "implicit"나 다른 그랜트 타입은 지원하지 않습니다.
- Proof Key for Code Exchange(PKCE, [RFC 7636](https://datatracker.ietf.org/doc/html/rfc7636))가 의무화됩니다.
- 보안 모범 사례([`draft-ietf-oauth-security-topics`](https://datatracker.ietf.org/doc/html/draft-ietf-oauth-security-topics) 및 [`draft-ietf-oauth-browser-based-apps`](https://datatracker.ietf.org/doc/html/draft-ietf-oauth-browser-based-apps))가 필수입니다.

중앙 집중형 앱 플랫폼과 달리, atproto에서는 독립적인 서버 구현체가 다수 존재하므로, 서버 검색과 클라이언트 등록은 공개 인증 서버 메타데이터와 공개 클라이언트 메타데이터의 조합을 통해 자동화됩니다. `client_id`는 클라이언트 메타데이터(JSON 문서)가 위치한 완전한 웹 URL입니다. 서버와 클라이언트 간에 공유되는 `client_secret`은 존재하지 않습니다. 핸들 또는 DID로 로그인할 때, 계정의 PDS 네트워크 위치를 발견하기 위해 atproto 전용 신원 확인 단계가 필요합니다.

OAuth 용어에서 atproto Personal Data Server(PDS)는 액세스 토큰을 사용하여 권한 있는 HTTP 요청이 이루어지는 "리소스 서버"입니다. 때때로 PDS는 OAuth 인증 흐름과 토큰 요청을 처리하는 "인증 서버" 역할도 수행하며, 다른 상황에서는 별도의 "입구(Entryway)" 서비스가 여러 PDS 인스턴스에 대한 인증 서버 역할을 합니다. 클라이언트는 PDS의 메타데이터 파일을 통해 인증 서버의 네트워크 위치를 자동으로 발견합니다.

DPoP(서버가 발급하는 nonce 포함)는 인증 토큰을 특정 클라이언트 소프트웨어 인스턴스(예: 단말기나 브라우저 세션)에 바인딩하기 위해 필수입니다. Pushed Authentication Requests(PAR)는 인증 요청 흐름을 간소화하기 위해 사용됩니다. "Confidential" 클라이언트는 JWT를 사용해 클라이언트 소프트웨어를 인증 서버에 인증하며, 이때 비밀 키로 서명합니다.

자동화된 클라이언트 등록은 atproto OAuth의 새로운 측면 중 하나입니다. 2024년 8월 기준으로, 클라이언트 메타데이터는 여전히 Internet Draft([`draft-parecki-oauth-client-id-metadata-document`](https://datatracker.ietf.org/doc/draft-parecki-oauth-client-id-metadata-document/)) 상태이며, 기존의 "Dynamic Client Registration" 표준([RFC 7591](https://datatracker.ietf.org/doc/html/rfc7591))과 혼동해서는 안 됩니다. 향후 다른 개방형 프로토콜에서도 유사한 자동 등록 흐름을 채택할 수 있기를 바라지만, 당분간 일반 OAuth 생태계에서는 지원이 미흡할 수 있습니다.

OAuth 2.0은 전통적으로 권한 부여(`authz`) 시스템이지 인증(`authn`) 시스템이 아니므로, "XYZ로 로그인/가입"과 같은 순수한 계정 인증 용도에 항상 적합한 솔루션은 아닙니다. OpenID Connect(OIDC)는 OAuth 2.0 위에 구축되어 보통 신원 인증을 위해 권장됩니다. 불행히도, 현재 버전의 OIDC는 atproto 신원을 안전하고 일반적인 방식으로 인증할 수 있는 기능을 제공하지 않습니다. atproto OAuth 프로파일은 인증 흐름 중 계정 인증을 위한 (의무적인) 메커니즘을 포함하며, atproto 신원 인증 용도로 사용할 수 있습니다.

## 클라이언트

이 섹션에서는 인증 서버가 강제하는 OAuth 클라이언트에 대한 요구사항을 설명합니다.

OAuth 클라이언트 소프트웨어는 전 세계적으로 유일한 `client_id`로 식별됩니다. 동일한 소프트웨어의 다른 변형(예: 브라우저 앱과 Android(모바일 OS) 버전)은 서로 다른 `client_id` 값을 가질 수 있습니다. [`draft-parecki-oauth-client-id-metadata-document`](https://datatracker.ietf.org/doc/draft-parecki-oauth-client-id-metadata-document) 명세 초안에 따라, `client_id`는 클라이언트 메타데이터 JSON 문서를 가져올 수 있는 완전한 웹 URL이어야 합니다. 예를 들어, `https://app.example.com/client-metadata.json`과 같이 사용합니다. 추가 설명은 아래를 참조하세요:

- URL은 W3C URL 명세를 따르는 올바른 형식이어야 합니다.
- 스킴은 반드시 `https://`여야 하며, 포트 번호가 포함되어서는 안 됩니다. 단, 개발 환경에서는 `http://localhost`에 대한 특별한 예외가 있으니 아래 세부사항을 참고하세요.
- 경로에 반드시 `client-metadata.json`이 포함될 필요는 없으나, 포함하는 것이 관례입니다.

atproto OAuth 프로파일과 다른 형태의 OAuth를 모두 지원하는 인증 서버는 `client_id` 값 충돌을 방지해야 합니다. 예를 들어, 자동 등록되지 않은 클라이언트의 `client_id` 값은 절대로 `https://` 또는 `http://` 접두어를 가져서는 안 됩니다.

### 클라이언트 유형

모든 atproto OAuth 클라이언트는 기본적인 표준과 요구사항을 충족해야 하지만, 클라이언트의 보안 특성(예: 세션 수명)에 따라 약간의 차이가 있을 수 있습니다.

OAuth 2.0 명세([RFC 6749](https://datatracker.ietf.org/doc/html/rfc6749))에 따라, 모든 클라이언트는 두 가지 유형 중 하나입니다:

- **Confidential 클라이언트**는 암호화 서명 키를 사용해 인증 서버에 자신을 인증할 수 있는 클라이언트입니다. 이를 통해 리프레시 토큰을 특정 클라이언트에 바인딩할 수 있습니다. 이 클라이언트 인증 방식은 DPoP와는 별개이며, 클라이언트 인증 키는 모든 클라이언트 세션에서 공유됩니다(키 교체가 가능함). 일반적으로 이는 클라이언트가 관리하는 서버 측 웹 서비스가 키를 보유하는 경우입니다. 보안 사고 시 토큰을 취소할 수 있으므로, Confidential 클라이언트는 더 긴 세션 및 토큰 수명을 가질 수 있습니다.
- **Public 클라이언트**는 클라이언트 서명 키를 사용해 자신을 인증하지 않는 클라이언트입니다. 이는 클라이언트 소프트웨어가 전적으로 사용자 단말에서 실행되거나, 단순히 구현하지 않기로 선택한 경우에 해당합니다.

웹 서비스가 Public 클라이언트 역할을 하는 것은 허용되며, 반대로 모바일 앱이나 브라우저 앱이 토큰 중개 백엔드 서비스와 협력하여 Confidential 클라이언트를 구성할 수도 있습니다. 모바일 및 브라우저 앱은 "backend-for-frontend"(BFF) 아키텍처를 채택할 수 있습니다. 이 문서에서는 명확성을 위해 "public"과 "confidential"이라는 용어를 사용합니다.

클라이언트가 실행되는 환경은 인증 흐름 중 사용하는 리디렉션(콜백) URL 유형에 영향을 줍니다:

- **웹 클라이언트**: 웹 서비스와 브라우저 앱을 포함합니다. 리디렉션 URL은 브라우저에서 열리는 일반적인 웹 URL입니다.
- **네이티브 클라이언트**: 일부 모바일 및 데스크톱 네이티브 클라이언트를 포함합니다. 리디렉션 URL은 앱 자체를 여는 플랫폼 전용 콜백 스킴을 사용할 수 있습니다.

인증 서버는 `client_id`로 식별되는 "신뢰할 수 있는" 클라이언트 집합을 유지할 수 있습니다. 어떤 클라이언트라도 검증되지 않은 클라이언트 메타데이터를 사용하여 더 잘 알려진 앱이나 브랜드를 가장할 수 있으므로, 인증 서버는 기본적으로 이러한 메타데이터를 사용자에게 표시해서는 안 됩니다. 신뢰할 수 있는 클라이언트의 경우, 읽기 쉬운 이름(`client_name`), 프로젝트 URI(`client_uri`; `client_id`와 다른 도메인/출처일 수 있음) 및 로고(`logo_uri`)와 같은 추가 메타데이터를 표시할 수 있습니다. (자세한 내용은 "보안 고려사항" 섹션을 참조하세요.)

단순히 atproto 계정 인증(리소스 접근 권한 부여 없이)을 위해 atproto OAuth를 사용하는 클라이언트는 최소한의 스코프만 요청해야 합니다("Scopes" 섹션 참조). 그럼에도 불구하고 대부분의 인증 흐름은 구현해야 합니다. 특히, 토큰 응답의 `sub` 필드를 확인하여 계정 신원을 검증하는 것이 매우 중요합니다(이는 atproto 고유 사항입니다).

### 클라이언트 ID 메타데이터 문서

<Note>
클라이언트 ID 메타데이터 문서 명세([`draft-parecki-oauth-client-id-metadata-document`](https://datatracker.ietf.org/doc/draft-parecki-oauth-client-id-metadata-document/))는 여전히 초안 상태이며 시간이 지남에 따라 발전할 수 있습니다. 우리의 의도는 후속 초안 및 최종 표준과 정렬하면서도 기존 구현에 미치는 혼란을 최소화하는 것입니다.
</Note>

클라이언트는 공개 웹에 "클라이언트 메타데이터" JSON 파일을 게시해야 합니다. 이 파일은 인증 요청(PAR)이나 세션 수명 주기 중에 인증 서버에 의해 동적으로 가져와집니다. HTTP 응답 상태 코드는 반드시 200이어야 하며(다른 2xx나 리디렉션이 아님), JSON 객체 본문과 `Content-Type` 헤더는 `application/json`이어야 합니다.

인증 서버는 공개 웹에서 클라이언트 메타데이터 문서를 가져와야 합니다. 이때는 SSRF(서버 사이드 요청 위조) 공격을 방지하기 위해 강화된 HTTP 클라이언트를 사용해야 합니다("OAuth 보안 고려사항" 참조). 서버는 HTTP 캐싱 헤더를 고려하여 클라이언트 메타데이터 응답을 캐시할 수 있습니다(단, 제한 내에서). 최소 및 최대 캐시 TTL은 현재 명세에 없으나, 인증 토큰 요청 시 기한이 지난 Confidential 클라이언트 인증 키(`jwks` 또는 `jwks_uris`)로 인한 요청이 신속하게 거부되도록 적절하게 설정해야 합니다.

모든 클라이언트 유형에 대해 관련된 필드는 다음과 같습니다:

- `client_id` (문자열, 필수): `client_id`. 클라이언트 메타데이터 파일을 가져오기 위해 사용한 URL과 정확히 일치해야 합니다.
- `application_type` (문자열, 선택): `web` 또는 `native` 중 하나여야 하며, 명시되지 않은 경우 기본값은 `web`입니다. 이는 OpenID/OIDC에서 사용하는 필드이며, 인증 서버가 관련 "최신 모범 사례"를 적용하는 데 사용됩니다.
- `grant_types` (문자열 배열, 필수): `authorization_code`는 항상 포함되어야 합니다. `refresh_token`은 선택 사항이지만, 클라이언트가 토큰 재발급 요청을 할 경우 포함되어야 합니다.
- `scope` (문자열, 공백으로 구분된 부분 문자열, 필수): 클라이언트가 요청할 수 있는 모든 스코프 값을 여기에 선언합니다. `atproto` 스코프는 필수이며, 반드시 포함되어야 합니다. (자세한 내용은 "Scopes" 섹션 참조)
- `response_types` (문자열 배열, 필수): `code`가 포함되어야 합니다.
- `redirect_uris` (문자열 배열, 필수): 적어도 하나의 리디렉션 URI가 필요합니다. 리디렉션 URI에 관한 규칙은 인증 요청 필드 섹션에 나와 있습니다.
- `token_endpoint_auth_method` (문자열, 선택): Confidential 클라이언트는 이를 `private_key_jwt`로 설정해야 합니다.
- `token_endpoint_auth_signing_alg` (문자열, 선택): 여기서 `none`은 허용되지 않습니다. 현재 권장되고 가장 널리 지원되는 알고리즘은 `ES256`이지만, 향후 변경될 수 있습니다. 인증 서버는 이를 지원하는 알고리즘과 비교합니다.
- `dpop_bound_access_tokens` (불린, 필수): 모든 클라이언트에 대해 DPoP은 필수이므로, 반드시 `true`여야 합니다.
- `jwks` (JWK 배열을 포함하는 객체, 선택): Confidential 클라이언트는 JWT 클라이언트 인증에 사용할 공개 키를 최소 하나 이상 제공해야 합니다. 이 필드나 `jwks_uri` 필드 중 하나는 반드시 제공되어야 하며, 둘 다 동시에 제공해서는 안 됩니다.
- `jwks_uri` (문자열, 선택): JWKS JSON 객체가 있는 URL입니다. 위의 `jwks`와 관련된 세부사항을 참고하세요.

다음 필드들은 선택 사항이지만 권장됩니다:

- `client_name` (문자열, 선택): 클라이언트의 사람이 읽을 수 있는 이름
- `client_uri` (문자열, 선택): `client_id`와 혼동해서는 안 되며, 클라이언트의 홈페이지 URL입니다. 제공되는 경우, `client_uri`는 `client_id`와 동일한 호스트네임을 가져야 합니다.
- `logo_uri` (문자열, 선택): 클라이언트 로고에 대한 URL. 오직 `https:` URI만 허용됩니다.
- `tos_uri` (문자열, 선택): 클라이언트의 이용 약관(ToS)에 대한 사람이 읽을 수 있는 URL. 오직 `https:` URI만 허용됩니다.
- `policy_uri` (문자열, 선택): 클라이언트의 개인정보 처리방침에 대한 사람이 읽을 수 있는 URL. 오직 `https:` URI만 허용됩니다.

"OAuth 보안 고려사항" 섹션에서 언제 `client_name`, `client_uri` 및 `logo_uri`가 사용자에게 표시되거나 표시되지 않아야 하는지에 대한 내용을 참조하세요.

추가적인 선택적 클라이언트 메타데이터 필드는 [IANA](https://www.iana.org/assignments/oauth-parameters/oauth-parameters.xhtml#client-metadata)에 나와 있습니다. 이 필드들은 atproto OAuth 프로파일에서 직접 사용하지 않는 "Dynamic Client Registration" 표준(RFC 7591)과 공유됩니다.

### 로컬호스트 클라이언트 개발

개발 환경(인증 서버와 클라이언트)에서 작업할 때, 개발 중인 클라이언트 메타데이터를 공개 URL에 게시하여 인증 서버가 접근할 수 있게 하는 것이 어려울 수 있습니다. 이는 컨테이너화된 인증 서버와 로컬 DNS 환경에서도, 내부 IP 대역에 대한 SSRF 보호로 인해 발생할 수 있습니다.

개발 워크플로를 쉽게 하기 위해, origin이 `http://localhost`인 `client_id`에 대해 특별한 예외가 적용됩니다. 인증 서버는 이 예외를 지원하는 것이 권장되며(실제 운영 환경에서도 지원 가능), 단 이는 선택 사항입니다.

로컬호스트 `client_id`의 경우, 인증 서버는 스킴이 `http`이고, 호스트명이 정확히 `localhost`이며 포트가 지정되어 있지 않은지 확인해야 합니다. IP 주소(예: `127.0.0.1` 등)는 지원되지 않습니다. 경로는 빈 값이어야 합니다 (`/`).

인증 요청에서, `redirect_uri`는 제공된(또는 기본) URI 중 하나와 일치해야 합니다. 경로 구성 요소는 일치해야 하나 포트 번호는 일치하지 않아도 됩니다.

특정 메타데이터 필드는 `client_id` URL의 쿼리 파라미터를 통해 구성할 수 있습니다(적절히 URL 인코딩):

- `redirect_uri` (문자열, 선택, 여러 쿼리 파라미터 허용): 로컬 리디렉션/콜백 URL을 선언할 수 있으며, 경로 구성 요소는 매치되지만 포트 번호는 무시됩니다. 값이 제공되지 않으면 기본값은 `http://127.0.0.1/`와 `http://[::1]/`입니다.
- `scope` (공백으로 구분된 값이 포함된 문자열, 선택, 단일 쿼리 파라미터): 클라이언트가 요청할 수 있는 스코프 집합. 기본값은 `atproto`입니다.

나머지 가상 클라이언트 메타데이터 문서의 내용은 다음과 같습니다:

- `client_id` (문자열): 가상 문서를 생성하는 데 사용된 정확한 `client_id` (URL)
- `client_name` (문자열): 인증 서버에서 선택한 값(예: "Development client")
- `response_types` (문자열 배열): 반드시 `code`를 포함해야 합니다.
- `grant_types` (문자열 배열): `authorization_code`와 `refresh_token`을 포함합니다.
- `token_endpoint_auth_method`: `none`
- `application_type`: `native`
- `dpop_bound_access_tokens`: `true`

이는 Confidential 클라이언트가 아닌 Public 클라이언트로 작동함을 유의하세요.

## 신원 인증

앞서 언급한 바와 같이, OAuth 2.0은 전통적으로 권한 부여(`authz`)만을 제공하며, 신원 인증(`authn`)은 OpenID/OIDC와 같은 추가 표준에 의존합니다. atproto OAuth 프로파일은 계정 신원의 인증을 요구하며, 추가 리소스 접근 권한 부여 없이 단순한 신원 인증 사용 사례를 지원합니다.

atproto에서 계정 신원은 해당 계정의 영구적이며 전 세계적으로 유일한 공개 확인 식별자인 DID에 기반합니다. DID는 계정의 현재 PDS 호스트 위치를 나타내는 DID 문서로 해결됩니다. 해당 PDS(및 선택적 입구)는 인증 권한 기관이자 OAuth 인증 서버 역할을 수행합니다. 어떤 인증 서버와 통신할 때든, 클라이언트는 반드시 해당 인증 서버가 문제의 계정에 대해 권한이 있음을 독립적으로 확인해야 합니다. 즉, 계정을 DID로 해결하고 해당 인증 서버가 일치하는지 확인해야 합니다.

클라이언트는 다음 두 가지 방식 중 하나로 인증 흐름을 시작할 수 있습니다:

- 사용자가 제공한 공개 계정 식별자(핸들 또는 DID)로 시작
- 사용자가 제공한 서버 호스트네임(PDS 또는 입구)으로 시작하여, 해당 서버가 리소스 서버 및/또는 인증 서버 역할을 하는지 확인

계정 식별자로 시작할 경우, 클라이언트는 DID 문서로 계정을 해결해야 하며, 핸들인 경우 DID 문서가 해당 핸들을 주장하는지 양방향 검증하는 것이 필수입니다(atproto 핸들 명세 참조). 모든 핸들 해결 기법과 atproto에서 승인된 DID 메서드를 지원해야 모든 계정과의 상호 운용성이 보장됩니다.

일부 클라이언트 환경에서는 모든 신원 유형을 해결하기 어려울 수 있습니다. 예를 들어, 핸들 해결은 브라우저 앱에서 직접 지원되지 않는 DNS TXT 조회를 포함할 수 있습니다. 클라이언트 구현체는 DNS-over-HTTP와 같은 대체 기법을 사용하거나 신원 해결을 지원하는 웹 서비스를 활용할 수 있습니다.

인증 흐름은 보안상 매우 중요하므로, 신원 해결 결과의 캐싱 기간은 주의 깊게 설정되어야 합니다. 인증 흐름의 경우 캐시 수명은 10분 미만이어야 합니다.

해결된 DID는 전체 인증 세션에 바인딩되어 클라이언트 앱 코드 내에서 기본 계정 식별자로 사용되어야 합니다. (검증된) 핸들은 UI에 표시할 수 있으나, 시간이 지남에 따라 변경될 수 있으므로 주기적인 재검증이 필요합니다. 인증 요청의 `login_hint`로 계정 식별자를 전달할 경우, 클라이언트는 사용자가 제공한 원래의 계정 식별자(핸들이나 DID)를 사용하는 것이 권장됩니다.

인증 흐름의 마지막 단계에서, 클라이언트가 초기 토큰 요청을 할 때, 인증 서버는 JSON 응답 본문의 `sub` 필드에 계정 DID를 반환해야 합니다. 만약 인증 흐름이 계정 식별자로 시작되었다면, 클라이언트는 이 DID가 세션 초기에 바인딩된 예상 DID와 일치하는지 반드시 검증해야 합니다.

반대로 서버(호스트네임 또는 URL)로 시작한 경우, 클라이언트는 먼저 리소스 서버 메타데이터를 가져와(필요 시 인증 서버로 해결) 처리한 다음, 인증 서버 메타데이터를 가져옵니다. (자세한 내용은 "인증 서버" 섹션 참조) 두 경우 모두, 최종적으로 클라이언트는 식별된 인증 서버와 함께 작업하게 됩니다. 이때 인증 요청에는 `login_hint`나 계정 식별자가 세션에 바인딩되지 않으나, 인증 서버의 `issuer`가 세션에 바인딩됩니다.

인증 흐름이 진행되어 초기 토큰 요청이 성공하면, 클라이언트는 토큰 응답의 `sub` 필드에서 계정 식별자를 파싱합니다. 이 시점에서도 클라이언트는 해당 계정이 실제로 인증되었음을 신뢰할 수 없습니다. 클라이언트는 반드시 신원을 다시 해결(DID 문서 확인)하고, 선언된 PDS 호스트를 추출하며, 리소스 서버 메타데이터를 통해 해당 PDS가 세션에 바인딩된 인증 서버와 일치하는지 확인한 후, 인증 서버 메타데이터를 가져와 `issuer` 필드가 일치하는지 확인해야 합니다([`draft-ietf-oauth-v2-1` 섹션 7.3.1](https://datatracker.ietf.org/doc/html/draft-ietf-oauth-v2-1-11#section-7.13.1) 참고).

요약하면, atproto Identity Authentication만을 위한 클라이언트라 하더라도 전체 인증 흐름을 거치고, 토큰 응답의 `sub` 필드에 있는 계정 식별자(DID)가 인증 서버의 호스트명/출처(`issuer`)와 일치하는지 반드시 확인해야 합니다.

## 권한 범위(Scopes)

OAuth 스코프를 사용하면 클라이언트가 부여받는 리소스와 작업에 대해 보다 세분화된 제어가 가능합니다.

특별한 `atproto` 스코프는 모든 atproto OAuth 세션에서 필수입니다. 이 스코프의 의미는 `openid` 스코프와 유사하며, 클라이언트가 atproto OAuth 프로파일을 사용하고 있으며 이 명세의 모든 요구사항을 준수함을 확인합니다. 이 스코프가 포함되지 않으면 atproto 전용 PDS 리소스에 대한 접근 권한이 부여되지 않습니다.

인증 서버는 클라이언트가 `atproto` 스코프를 포함하지 않는 경우, 다른 OAuth 프로파일도 지원할 수 있습니다. 예를 들어, 인증 서버가 atproto PDS/입구로 기능하면서 다른 프로토콜/표준도 동시에 지원할 수 있습니다.

atproto OAuth 프로파일을 사용함으로써(즉, `atproto` 스코프를 포함함으로써) 인증 서버는 토큰 요청 시 계정 식별자로 atproto 계정 DID를 반환합니다. 인증 서버는 메타데이터 문서의 `scopes_supported`에 반드시 `atproto`를 포함해야 하므로, 클라이언트는 해당 서버가 atproto OAuth 프로파일을 지원함을 알 수 있습니다. 클라이언트는 계정 인증 전용(예: "atproto로 로그인" 사용 사례)의 경우 오직 `atproto` 스코프만 요청할 수 있습니다. OpenID와 달리, atproto 프로파일 메타데이터는 일반적으로 공개되므로 프로필 메타데이터를 가져오기 위한 추가 스코프가 필요하지 않습니다.

OAuth 2.0 명세에서는, 클라이언트가 요청한 스코프와 다른 스코프가 부여될 경우에만 토큰 응답에 부여된 스코프를 반환하도록 요구합니다. 하지만 atproto OAuth 프로파일에서는, 인증 서버가 토큰 응답에 항상 부여된 스코프를 반환해야 합니다. 클라이언트는 응답에 `scope` 필드가 없거나, `scope` 필드에 `atproto`가 포함되어 있지 않으면 해당 세션을 거부해야 합니다.

목표는 Lexicon 네임스페이스(NSID)에 기반한 유연한 스코프 지원으로, 클라이언트가 필요한 특정 콘텐츠와 API 엔드포인트에 대해서만 접근 권한을 부여받을 수 있도록 하는 것입니다. 해당 스코프 시스템의 설계가 준비될 때까지, atproto OAuth 프로파일은 기존의 "세션 토큰" 인증 시스템 하에서 부여된 권한과 일치하는 두 가지 과도기 스코프를 정의합니다:

- `transition:generic`: 이전의 "앱 비밀번호" 권한 수준에 해당하는 광범위한 PDS 계정 권한.
    - 모든 저장소 레코드 타입에 대한 생성/수정/삭제 권한
    - 블롭(미디어 파일) 업로드
    - 모든 개인 설정의 읽기 및 쓰기
    - 대부분의 Lexicon 엔드포인트에 대한 API 접근 및 서비스 프록시 기능(서비스 제공자 DID에 의해 식별)
    - 클라이언트가 접근 권한이 있는 특정 API 엔드포인트에 대해 서비스 인증 토큰을 생성할 수 있음
    - 계정 관리 작업(핸들 변경, 이메일 변경, 계정 삭제/비활성화, 계정 마이그레이션) 불가
    - DM(다이렉트 메시지) 접근 불가(`chat.bsky.*` Lexicon 제외)
- `transition:chat.bsky`: "앱 비밀번호"에 DM 접근 토글을 추가한 것과 동일.
    - `chat.bsky` Lexicon에 대한 API 엔드포인트 및 서비스 프록시 기능
    - `chat.bsky` Lexicon에 대해 서비스 인증 토큰 생성 가능
    - 이 스코프는 반드시 `transition:generic` 스코프에 의존하며, 단독으로 동작하지 않습니다.

## 인증 요청

이 섹션에서는 atproto OAuth 프로파일에 대한 인증 요청에 관한 표준 및 요구사항을 상세히 설명합니다.

모든 클라이언트와 인증 서버는 PKCE와 PAR를 의무적으로 사용해야 합니다. Confidential 클라이언트는 JWT 클라이언트 어서션을 사용해 자신을 인증합니다.

### 요청 필드

atproto OAuth 프로파일에 해당하는 인증 요청과 관련된 주요 필드 요약은 다음과 같습니다:

- `client_id` (문자열, 필수): 클라이언트 소프트웨어를 식별합니다. (위 "클라이언트" 섹션 참조)
- `response_type` (문자열, 필수): 반드시 `code`여야 합니다.
- `code_challenge` (문자열, 필수): PKCE 챌린지 값. (아래 "PKCE" 섹션 참조)
- `code_challenge_method` (문자열, 필수): 사용된 코드 챌린지 메서드 예: `S256`. (아래 "PKCE" 섹션 참조)
- `state` (문자열, 필수): 인증 요청과 응답을 검증하기 위해 사용되는 랜덤 토큰.
- `redirect_uri` (문자열, 필수): 클라이언트 메타데이터에 선언된 URI 중 하나와 일치해야 하며, `application_type`에 맞는 형식이어야 합니다.
- `scope` (공백으로 구분된 문자열, 필수): 클라이언트 메타데이터에 선언된 스코프의 부분 집합이어야 하며, 반드시 `atproto`를 포함해야 합니다. (자세한 내용은 "Scopes" 섹션 참조)
- `client_assertion_type` (문자열, 선택): Confidential 클라이언트가 클라이언트 인증 메커니즘을 설명하기 위해 사용.
- `client_assertion` (문자열, 선택): Confidential 클라이언트의 경우 클라이언트 인증을 위해 사용됨.
- `login_hint` (문자열, 선택): 로그인 시 사용할 계정 식별자. ("인증 인터페이스" 섹션 참조)

`client_secret`은 다른 OAuth 프로파일에서 사용되지만 포함되어서는 안 됩니다.

클라이언트 인증 요청에서의 `state` 파라미터는 의무이며, 무작위로 생성된 토큰을 사용해 장치, 계정, 세션 간에 중복이나 재사용이 없어야 합니다. 인증 서버는 중복된 state 파라미터를 거부해야 하나, 계정이나 세션 간의 state 값을 추적할 의무는 없습니다. 이 `state` 파라미터는 나중에 `issuer`를 검증하는 데 사용되므로, 신뢰할 수 없는 당사자가 위조하거나 추측할 수 없어야 합니다.

웹 클라이언트의 경우, `redirect_uri`는 브라우저에서 인증 흐름 종료 후 사용자를 클라이언트로 되돌려 보내기 위한 HTTPS URL입니다. 이 URL은 포트 번호를 포함할 수 있으나, 기본 포트 번호는 포함하지 않아야 합니다. `redirect_uri`는 클라이언트 메타데이터에 선언된 URI 중 하나와 반드시 일치해야 하며, 인증 서버는 이를 검증해야 합니다. URL의 출처(origin)는 `client_id`와 일치해야 합니다.

로컬호스트 개발 워크플로를 위해, "로컬호스트 클라이언트 개발" 섹션에 설명된 규칙에 따라 `http://127.0.0.1` 또는 `http://[::1]` URL 사용에 대한 예외가 존재합니다. 이 클라이언트는 웹 URL을 사용하나, 생성된 클라이언트 메타데이터에서 `application_type`은 `native`로 설정됩니다.

네이티브 클라이언트의 경우, `redirect_uri`는 운영체제에서 앱으로 사용자를 리디렉션할 수 있도록 커스텀 URI 스킴을 사용할 수 있습니다. 커스텀 스킴은 리버스 도메인 순서를 사용하여 `client_id`의 호스트명과 일치해야 합니다. 스킴은 콜론(`:`) 뒤에 단일 슬래시(`/`)와 URI 경로 구성 요소가 따라와야 합니다. 예를 들어, `client_id`가 [`https://app.example.com/client-metadata.json`](https://app.example.com/client-metadata.json)인 앱은 `com.example.app:/callback`과 같은 `redirect_uri`를 가질 수 있습니다.

네이티브 클라이언트는 HTTPS URL을 사용할 수도 있으며, 이 경우 URL의 출처(origin)는 `client_id`와 동일해야 합니다. (예: Apple Universal Links)

클라이언트는 추가적인 선택적 인증 요청 파라미터를 포함할 수 있으며, 서버는 이를 처리할 수 있으나 필수 사항은 아닙니다. 추가 파라미터에 대해서는 다른 OAuth 표준 및 [IANA OAuth 파라미터 레지스트리](https://www.iana.org/assignments/oauth-parameters/oauth-parameters.xhtml)를 참고하세요.

### Proof Key for Code Exchange (PKCE)

PKCE는 모든 인증 요청에 대해 필수입니다. 클라이언트는 매 인증 요청마다 새로운 고유의 무작위 챌린지를 생성해야 합니다. 인증 서버는 (최소 24시간 이내) 세션 간에 `code_challenge` 값의 재사용을 방지해야 합니다.

모든 클라이언트와 인증 서버는 반드시 `S256` 챌린지 메서드를 지원해야 하며, 자세한 내용은 [RFC 7636](https://datatracker.ietf.org/doc/html/rfc7636)를 참조하세요. `plain` 메서드는 허용되지 않습니다. 클라이언트와 서버가 지원할 경우 추가 메서드를 지원할 수도 있습니다.

인증 서버는 `code` 값의 재사용을 거부하고, 이전에 사용된 `code` 값과 관련된 모든 세션과 토큰을 폐기해야 합니다.

### Pushed Authorization Requests (PAR)

인증 서버는 PAR를 반드시 지원해야 하며, 모든 클라이언트는 인증 요청 시 PAR를 사용해야 합니다.

인증 서버는 서버 메타데이터 문서에서 `require_pushed_authorization_requests`를 `true`로 설정하고, `pushed_authorization_request_endpoint`에 유효한 URL을 포함해야 합니다. (자세한 내용은 [RFC 9207](https://datatracker.ietf.org/doc/html/rfc9207) 참고)

클라이언트는 `pushed_authorization_request_endpoint` URL로 HTTPS POST 요청을 하며, 요청 파라미터는 form-encoded request body에 포함합니다. JSON 응답 객체에서 `request_uri`(리디렉션 URI와는 구분됨)를 받게 됩니다. 이후 사용자를 인증 서버의 인증 엔드포인트로 리디렉션할 때, 이전에 전송한 대부분의 요청 파라미터는 생략하고 대신 이 `request_uri`와 `client_id`를 쿼리 파라미터로 포함합니다.

<Note>
PAR는 비교적 새로운 표준으로 지원이 제한적일 수 있으며, 클라이언트에 PAR 사용 요구 사항이 너무 부담스러운 경우 완화될 수 있습니다. 그런 경우, 인증 서버는 PAR와 비PAR 요청을 모두 지원해야 하며, 클라이언트에 대해 PAR은 선택 사항이 됩니다.
</Note>

### Confidential 클라이언트 인증

Confidential 클라이언트는 인증 요청 시 JWT 클라이언트 어서션을 사용해 자신을 인증해야 합니다. 인증 서버는 Confidential 클라이언트에 대해 더 긴 토큰/세션 수명을 부여할 수 있습니다. (자세한 내용은 "토큰 및 세션 수명" 섹션 참조)

사용할 클라이언트 어서션 타입은 `urn:ietf:params:oauth:client-assertion-type:jwt-bearer`이며, 이는 "JSON Web Token (JWT) Profile for OAuth 2.0 Client Authentication and Authorization Grants"([RFC 7523](https://datatracker.ietf.org/doc/html/rfc7523))에 설명되어 있습니다. 현재 모든 클라이언트와 인증 서버는 `ES256` 암호화 시스템을 지원해야 합니다. 권장 암호화 시스템/알고리즘은 향후 발전할 수 있습니다.

추가 요구사항:

- Confidential 클라이언트는 클라이언트 메타데이터에 JWT 클라이언트 인증에 사용할 공개 키(JWK)를 하나 이상 게시해야 합니다. 이는 직접 JSON 형태의 `jwks` 필드이거나, 별도의 JWKS JSON 객체를 가리키는 `jwks_uri`를 통해 제공되어야 합니다. `jwks_uri`는 반드시 `https://` 스킴을 가진 완전한 URL이어야 합니다.
- Confidential 클라이언트는 주기적으로 클라이언트 키를 교체해야 하며, 새 키를 JWKS 집합에 추가한 후 새로운 세션에 사용하고, 더 이상 활성 인증 세션에 사용되지 않는 키는 제거해야 합니다.
- Confidential 클라이언트는 클라이언트 메타데이터 문서에 `token_endpoint_auth_method`를 반드시 `private_key_jwt`로 포함해야 합니다.
- 유출 또는 침해된 경우, Confidential 클라이언트는 즉시 클라이언트 메타데이터에서 인증 키를 제거해야 합니다.
- 인증 서버는 Confidential 클라이언트의 활성 인증 세션을 해당 세션 시작 시 사용한 클라이언트 인증 키에 바인딩해야 합니다. 만약 클라이언트 인증 키가 클라이언트 메타데이터에서 사라지면, 서버는 해당 세션을 폐기하고 이후 토큰 재발급 요청을 거부해야 합니다. 이는 인증 서버가 주기적으로 클라이언트 메타데이터를 다시 가져와야 함을 의미합니다.

## 토큰 및 세션 수명

액세스 토큰은 계정의 PDS("리소스 서버")에 대해 클라이언트 요청을 권한 부여하기 위해 사용됩니다. 클라이언트 입장에서는 토큰이 불투명하지만, 종종 만료 시간이 포함된 서명된 JWT입니다. PDS 구현에 따라 개별 액세스 토큰을 침해 시 폐기할 수 없을 수도 있으므로, 상대적으로 짧은 수명을 가져야 합니다.

리프레시 토큰은 인증 서버(또는 입구)에 새로운 토큰(액세스 토큰 및 리프레시 토큰)을 요청하는 데 사용됩니다. 클라이언트 입장에서는 리프레시 토큰 역시 불투명합니다. 인증 세션은 폐기 가능하며, 리프레시 토큰 또한 단일 사용 형태(요청 시 새로운 리프레시 토큰으로 대체됨)입니다. 이로 인해 클라이언트 구현체는 동시 토큰 재발급 요청을 방지하기 위한 락킹 메커니즘이 필요할 수 있습니다.

리프레시 토큰을 요청하기 위해, 클라이언트는 클라이언트 메타데이터에 `refresh_token` 그랜트 타입을 선언해야 합니다.

토큰은 항상 고유 세션 DPoP 키에 바인딩됩니다. 토큰은 클라이언트 단말 간에 공유되거나 재사용되어서는 안 되며, 클라이언트 소프트웨어(`client_id`)에도 고유하게 바인딩되어야 합니다. 전체 세션은 액세스 토큰과 리프레시 토큰이 더 이상 사용되지 않을 때 종료됩니다.

세션, 액세스 토큰 및 리프레시 토큰의 구체적인 수명은 인증 서버 구현체에 따라 달라지며, 클라이언트 유형 및 평판에 따른 보안 평가에 의존할 수 있습니다.

일부 가이드라인 및 요구사항:

- 액세스 토큰 수명은 모든 경우 30분 미만이어야 합니다. 만약 서버가 개별 액세스 토큰을 폐기할 수 없다면 최대 15분, 권장 수명은 5분입니다.
- 신뢰할 수 없는 Public 클라이언트의 경우, 전체 세션 수명은 7일로 제한되고 개별 리프레시 토큰의 수명은 24시간으로 제한됩니다.
- Confidential 클라이언트의 경우, 전체 세션 수명은 무제한일 수 있으며, 개별 리프레시 토큰은 180일로 제한됩니다.
- Confidential 클라이언트는 초기 인증 요청 시 사용한 것과 동일한 클라이언트 인증 키와 어서션 방법을 리프레시 토큰 요청 시에도 사용해야 합니다.

## DPoP(서명된 증거) 증명

atproto OAuth 프로파일에서는 모든 클라이언트가 인증 서버 및 리소스 서버에 인증 토큰 요청 시 DPoP을 반드시 사용하도록 요구합니다. 자세한 내용은 [RFC 9449](https://datatracker.ietf.org/doc/html/rfc9449)를 참조하세요.

클라이언트는 최초 인증 요청(PAR)에서 DPoP을 시작해야 합니다.

서버가 발급하는 DPoP nonce는 필수입니다. 리소스 서버와 인증 서버는 동일 서버일 경우 동일 nonce를 공유하거나, 별도의 nonce를 사용할 수 있습니다. 클라이언트는 계정 세션 및 서버별로 DPoP nonce를 추적해야 합니다. 서버는 nonce의 수명을 최대 5분으로 제한하며 주기적으로 갱신해야 합니다. 서버는 모든 클라이언트 세션 및 다수의 요청에 대해 동일한 nonce를 사용할 수 있으며, 클라이언트가 여러 동시 요청을 보낼 경우를 위해 최근의 만료된 nonce도 허용해야 합니다. 클라이언트는 DPoP이 포함된 요청에 대해 `DPoP-Nonce` 헤더(대소문자 구분 없음)가 누락된 응답을 받으면 해당 응답을 거부해야 하며, HTTP 400 오류 형태의 예상치 못한 nonce 업데이트에도 재시도해야 합니다.

클라이언트는 모든 요청마다 고유의 DPoP 토큰(JWT)을 생성 및 서명해야 합니다. 각 DPoP 요청 JWT는 고유한(무작위 생성된) `jti` nonce를 포함해야 합니다. 서버는 `jti` nonce 재사용을 방지하기 위해 이를 추적해야 하며, 서버가 관리하는 DPoP nonce 기간 내에 클라이언트가 생성한 `jti` nonce 기록의 범위를 제한할 수 있습니다.

모든 클라이언트와 서버는 DPoP JWT 서명을 위해 반드시 `ES256`(NIST "P-256") 암호화 알고리즘을 지원해야 합니다. 권장 알고리즘 집합은 향후 발전할 수 있으며, 클라이언트와 서버는 추가 알고리즘을 구현하고 메타데이터 문서에 이를 명시할 수 있습니다.

## 인증 서버

브라우저 앱 지원을 위해, 인증 서버는 서버 메타데이터, 인증 요청(PAR), 토큰 요청 등 관련 엔드포인트에서 HTTP CORS 요청/헤더를 반드시 지원해야 합니다.

### 서버 메타데이터

리소스 서버(PDS 인스턴스)와 인증 서버(PDS 또는 입구)는 모두 HTTPS의 well-known 엔드포인트에 메타데이터 파일을 게시해야 합니다.

리소스 서버(PDS) 메타데이터는 "OAuth 2.0 Protected Resource Metadata"([`draft-ietf-oauth-resource-metadata`](https://datatracker.ietf.org/doc/draft-ietf-oauth-resource-metadata/)) 초안 명세를 준수해야 합니다. 주요 요구사항은 다음과 같습니다:

- URL 경로는 `/.well-known/oauth-protected-resource`
- HTTP 응답은 200이어야 하며(2xx나 리디렉션 아님), 유효한 JSON 객체와 `Content-Type`은 `application/json`이어야 합니다.
- 단 하나의 요소를 가지는 문자열 배열 `authorization_servers`를 포함해야 하며, 이는 완전한 URL입니다.

인증 서버 URL은 리소스 서버(PDS)와 동일한 출처일 수 있으며, 또는 별도의 서버(예: 입구)를 가리킬 수 있습니다. 이 URL은 단순 출처 URL이어야 하며: `https` 스킴, 사용자 인증 정보(예: user:password) 없음, 경로 없음, 쿼리 문자열이나 프래그먼트 없음. 포트 번호는 허용되나, 기본 포트(HTTPS의 경우 443)는 포함하면 안 됩니다.

인증 서버는 또한 "OAuth 2.0 Authorization Server Metadata"([RFC 8414](https://datatracker.ietf.org/doc/html/rfc8414)) 표준을 준수하여 메타데이터를 게시합니다. 주요 요구사항은 다음과 같습니다:

- URL 경로는 `/.well-known/oauth-authorization-server`
- HTTP 응답은 200이어야 하며(2xx나 리디렉션 아님), 유효한 JSON 객체와 `Content-Type`은 `application/json`이어야 합니다.
- `issuer` (문자열, 필수): 인증 서버의 "출처" URL. 유효한 URL이어야 하며 `https` 스킴을 사용해야 합니다. 포트 번호가 포함될 수 있으나(출처와 일치하는 경우), 기본 포트(HTTPS의 경우 443)는 지정하면 안 됩니다. 경로 세그먼트가 없어야 하며, 메타데이터 문서를 가져온 URL의 출처와 일치해야 합니다.
- `authorization_endpoint` (문자열, 필수): 인증 리디렉션을 위한 엔드포인트 URL.
- `token_endpoint` (문자열, 필수): 토큰 요청을 위한 엔드포인트 URL.
- `response_types_supported` (문자열 배열, 필수): 반드시 `code`를 포함해야 합니다.
- `grant_types_supported` (문자열 배열, 필수): 반드시 `authorization_code`와 `refresh_token`(리프레시 토큰 지원 포함)을 포함해야 합니다.
- `code_challenge_methods_supported` (문자열 배열, 필수): 반드시 `S256`을 포함해야 합니다. (아래 "PKCE" 섹션 참조)
- `token_endpoint_auth_methods_supported` (문자열 배열, 필수): Public 클라이언트를 위한 `none`과 Confidential 클라이언트를 위한 `private_key_jwt`를 모두 포함해야 합니다.
- `token_endpoint_auth_signing_alg_values_supported` (문자열 배열, 필수): `none`은 포함되어서는 안 되며, 현재는 `ES256`을 포함해야 합니다. (암호화 알고리즘 집합은 향후 발전할 수 있음)
- `scopes_supported` (문자열 배열, 필수): 반드시 `atproto`를 포함해야 하며, 과도기 권한 부여를 지원할 경우 해당 스코프도 포함해야 합니다. (자세한 내용은 "Scopes" 섹션 참조)
- `authorization_response_iss_parameter_supported` (불린, 선택): 반드시 `true`여야 합니다.
- `require_pushed_authorization_requests` (불린): 반드시 `true`여야 하며, (자세한 내용은 "PAR" 섹션 참조)
- `pushed_authorization_request_endpoint` (문자열, 필수): PAR 엔드포인트 URL이어야 합니다.
- `dpop_signing_alg_values_supported` (문자열 배열, 필수): 현재는 반드시 `ES256`을 포함해야 합니다. (자세한 내용은 "DPoP" 섹션 참조)
- `require_request_uri_registration` (불린, 선택): 기본값은 `true`이며, 명시적으로 설정할 필요는 없으나 `false`가 되어서는 안 됩니다.
- `client_id_metadata_document_supported` (불린, 필수): 반드시 `true`여야 합니다. (자세한 내용은 "클라이언트 ID 메타데이터" 섹션 참조)

`issuer`(출처)는 인증 서버의 전체 식별자입니다.

### 인증 인터페이스

인증 서버(PDS/입구)는 사용자가 인증 서버와 상호작용하여 로그인하고, 클라이언트의 인증 요청을 승인(또는 거부)하며, 활성 세션을 관리할 수 있는 웹 인터페이스(인증 인터페이스)를 구현해야 합니다.

서버 구현체는 사용자 인증 및 계정 복구를 위해 자체적인 기술(예: 안전한 쿠키, 이메일, 2단계 인증, 패스키, 외부 신원 제공자(OIDC 포함) 등)을 선택할 수 있습니다. 서버는 여러 동시 인증 세션을 지원할 수도 있습니다.

클라이언트가 인증 서버의 `authorization_endpoint` URL로 리디렉션할 경우, 서버는 먼저 사용자를 인증해야 합니다. 활성 인증 세션이 없으면 사용자는 로그인하라는 메시지를 받게 됩니다. 인증 요청에 `login_hint`가 제공되었다면, 이를 로그인 폼에 미리 채워 넣을 수 있습니다. 활성 인증 세션이 여러 개 존재하는 경우, 사용자는 목록에서 선택하거나 `login_hint`를 통해 자동 선택될 수 있습니다. 단일 활성 세션이 존재한다면, 인터페이스는 다른 계정으로 로그인할 옵션을 제공하며 승인 화면으로 넘어갈 수 있습니다. `login_hint`가 제공된 경우, 인증 서버는 사용자가 해당 계정으로만 인증하도록 허용해야 합니다. 그렇지 않으면 클라이언트가 계정 신원(`sub` 필드)을 검증할 때 인증 흐름이 실패합니다.

인증 승인 프롬프트는 클라이언트 앱을 식별하고 요청된 권한 범위를 설명해야 합니다.

표시할 클라이언트 메타데이터의 양은 해당 클라이언트가 인증 서버에 의해 "신뢰받는"지 여부에 따라 달라질 수 있습니다. (자세한 내용은 "클라이언트" 및 "보안 고려사항" 섹션 참조) 기본적으로 전체 `client_id` URL이 표시되어야 합니다.

스코프에 관한 설명은 "Scopes" 섹션을 참조하세요.

만약 Confidential 클라이언트이고 사용자가 과거에 동일한 스코프에 대해 이미 승인을 했다면, 인증 서버는 "무음 로그인(silent sign-in)"을 통해 인증 요청을 자동 승인할 수 있습니다. 인증 서버는 자체 정책에 따라 이를 처리할 수 있으며, 사용자가 명시적으로 설정해야 하거나 클라이언트가 "신뢰할 수 있는" 클라이언트여야 할 수 있습니다.

인증 서버는 별도의 웹 인터페이스를 통해 인증된 사용자가 활성 OAuth 세션을 확인하고 해당 세션을 삭제할 수 있도록 구현해야 합니다.

## 인증 흐름 요약

아래는 atproto OAuth 인증 흐름의 개략적인 설명입니다(사용자가 이미 atproto 계정을 보유하고 있다고 가정).

1. 클라이언트는 사용자에게 계정 식별자(핸들 또는 DID) 또는 PDS/입구 호스트네임을 요청합니다. ("신원 인증" 섹션 참조)
2. 계정 식별자인 경우, 클라이언트는 DID 문서를 통해 계정을 해결하고, 해당 DID 문서에서 선언된 PDS URL을 추출한 후, 리소스 서버 및 인증 서버 위치를 가져옵니다. 서버 호스트네임으로 시작하는 경우, 클라이언트는 해당 호스트네임을 통해 인증 서버를 해결합니다. 두 경우 모두, 인증 서버 메타데이터를 가져와 atproto OAuth의 요구사항에 부합하는지 검증합니다("인증 서버" 섹션 참조).
3. 클라이언트는 HTTPS POST 요청을 통해 Pushed Authorization Request(PAR)를 보냅니다. (자세한 내용은 "인증 요청" 섹션 참조) 주요 사항은 다음과 같습니다:
   - 무작위 생성된 `state` 토큰이 필수이며, 이는 후속 응답 콜백 시 세션 참조에 사용됩니다.
   - PKCE가 필수이므로, 비밀 값이 생성되어 저장되고 파생된 챌린지가 요청에 포함됩니다.
   - 요청 시 `scope` 값이 포함되며, 반드시 `atproto`를 포함해야 합니다.
   - Confidential 클라이언트의 경우, `client_assertion`을 포함해 `jwt-bearer` 타입으로 클라이언트 인증을 수행합니다.
   - 클라이언트는 사용자/단말/세션마다 새로운 DPoP 키를 생성하여 PAR 요청부터 사용합니다.
   - 계정 식별자로 시작한 경우, 클라이언트는 해당 초기 식별자를 `login_hint` 필드를 통해 전달해야 합니다.
   - atproto는 PAR를 사용하므로, 요청은 반드시 HTTPS POST 방식으로 인증 서버에 전송됩니다.
4. 인증 서버는 PAR 요청을 수신하고, `client_id` URL을 통해 클라이언트 메타데이터 문서를 가져옵니다. 서버는 요청 및 클라이언트 메타데이터를 검증한 후, 세션 정보를 저장하며 DPoP 키를 세션에 바인딩합니다. 서버는 DPoP nonce를 HTTP 헤더에 포함하여 `request_uri` 토큰을 클라이언트에 반환합니다.
5. 클라이언트는 `request_uri`를 받고 사용자를 리디렉션할 준비를 합니다. 이 시점에서 클라이언트는 보통 세션 정보를 안전한 저장소(예: 웹 서비스 백엔드의 데이터베이스, 브라우저의 IndexedDB 등)에 보관해야 합니다. 이후 클라이언트는 `request_uri`와 함께 브라우저를 통해 인증 서버의 인증 엔드포인트로 사용자를 리디렉션합니다. 단, 클라이언트는 세션 데이터를 `state` 파라미터에 직접 저장해서는 안 됩니다.
6. 인증 서버는 `request_uri`를 기반으로 이전 인증 요청 파라미터를 조회한 후, 사용자를 인증합니다(로그인 또는 계정 선택). 사용자에게 승인(또는 거부) 프롬프트가 표시됩니다. 사용자는 요청된 세분화 스코프를 조정할 수 있으며, 승인 여부를 결정합니다. 인증 서버는 사용자를 `redirect_uri`로 리디렉션하며, 이는 웹 콜백 URL 또는 네이티브 앱 URI일 수 있습니다.
7. 클라이언트는 URL 쿼리 파라미터(`state`와 `iss`)를 통해 세션 정보를 조회 및 검증합니다. 이후 `code` 파라미터를 사용해 인증 서버의 토큰 엔드포인트에 초기 토큰 요청을 보냅니다. 클라이언트는 이전에 저장한 값을 `code_verifier` 필드에 포함하여 PKCE 플로우를 완료합니다. Confidential 클라이언트는 토큰 요청 시 클라이언트 어서션 JWT를 포함해야 합니다("Confidential 클라이언트 인증" 섹션 참조). 인증 서버는 요청을 검증하고, 토큰과 함께 계정 식별자(DID)를 나타내는 `sub` 필드 및 발급된 액세스 토큰에 해당하는 `scope`를 반환합니다.
8. 이 시점에서, 클라이언트는 반드시 토큰 응답의 `sub` 필드에 있는 계정 식별자가 인증 서버의 `issuer`(쿼리 파라미터 `iss`에 포함)와 일치하는지 검증해야 합니다. (자세한 내용은 "신원 인증" 섹션 참조) 만약 응답에 `atproto` 스코프가 포함되지 않았다면, 클라이언트는 해당 세션을 거부해야 합니다.
9. 인증 전용 클라이언트는 여기서 인증 흐름을 종료할 수 있습니다.
10. 액세스 토큰을 사용하여, 클라이언트는 PDS("리소스 서버")에 대해 권한 있는 요청을 수행할 수 있습니다. 이때 반드시 DPoP과 액세스 토큰을 함께 사용해야 합니다. 토큰(액세스 및 리프레시 토큰)은 주기적으로 인증 서버의 토큰 엔드포인트에 재요청해야 하며, 이 과정 또한 DPoP을 요구합니다("토큰 및 세션 수명" 섹션 참조).

## 보안 고려사항

외부 당사자가 제공한 HTTP URL을 클라이언트와 공급자(서버)가 가져올 때 여러 보안 이슈가 발생할 수 있습니다. 특히, 악의적으로 조작된 URL로 인해 내부 IP 주소(또는 프라이빗 네트워크)에 접근하는 SSRF(서버 사이드 요청 위조) 공격이 대표적입니다. 또한, 악의적인 서버로부터의 대용량 응답, TCP 슬로우로리스(slow-loris) 공격 등 서비스 거부(DoS) 공격의 위험도 있습니다. 이러한 공격을 완화하기 위해 "강화된" HTTP 클라이언트 구현/설정을 사용하는 것을 강력히 권장합니다.

어떤 당사자라도 언제든지 임의의 내용으로 클라이언트 및 클라이언트 메타데이터 파일을 생성할 수 있습니다. 특히, `client_id`의 호스트네임은 전체 클라이언트를 대표한다고 전적으로 신뢰할 수 없습니다. 신뢰할 수 없는 사용자가 임의의 URL에 클라이언트 메타데이터 파일을 업로드할 수 있기 때문입니다. 특히, `client_uri`, `client_name` 및 `logo_uri` 필드는 검증되지 않으므로 악의적인 행위자가 합법적인 클라이언트를 가장할 수 있습니다. 인증 서버는 기본적으로 이러한 필드를 미확인 클라이언트에 대해 사용자에게 표시하지 않도록 강력히 권장합니다. 서비스 운영자는 신뢰할 수 있는 `client_id` 목록을 유지하고, 해당 앱에 대해서만 추가 메타데이터를 표시해야 합니다.

## 향후 변경 가능 사항

- 클라이언트 메타데이터 요청(인증 서버의 경우)이 일시적인 네트워크 장애, 서버 유지보수 등으로 실패할 수 있습니다. 이 경우 활성 클라이언트 세션을 즉시 폐기하는 것은 취약할 수 있으므로, 명시적인 유예 기간이 필요할 수 있습니다.
- 리소스 서버 메타데이터가 단일 인증 서버 URL만 참조하도록 요구하는 부분은 완화될 수 있습니다.
- 세션 및 토큰 수명에 관한 세부 사항은 추가 보안 검토에 따라 변경될 수 있습니다.


---
atproto/src/app/[locale]/specs/oauth/page.tsx
---
export const metadata = {
  title: 'OAuth',
  description: 'OAuth for Client/Server Authentication and Authorization',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/label/en.mdx
---
export const metadata = {
  title: 'Labels',
  description:
    'Self-authenticating string annotations on accounts or content for moderation and other purposes.',
}

# Labels

**Labels** are a form of metadata about any account or content in the atproto ecosystem. {{ className: 'lead' }}

They exist as free-standing, self-authenticated data objects, though they are also frequently distributed as part of API responses (in which context the signatures might not be included). Additionally, label "values" may be directly embedded in records themselves ("self-labels"). {{ className: 'lead' }}

Labels primarily consist of a source (DID), a subject (URI), and value. The value is a short string, similar to a tag or hashtag, which presumably has pre-defined semantics shared between the creator and consumer of the label. Additional metadata fields can give additional context, but at any point of time there should be only one coherent set of metadata for the combination of source, subject, and value. If there are multiple sets of metadata, the `created-at` timestamp is used to clarify which label is current. {{ className: 'lead' }}

The label concept and protocol primitive is flexible in scope, use cases, and data transport. One of the original design motivations was to enable some forms of composable moderation, with labels generated by moderation services. But labels are not moderation-exclusive, and may be reused freely for other purposes in atproto applications. {{ className: 'lead' }}

The core label schema is versioned, and this document describes labels version `1`. {{ className: 'lead' }}

## Schema and Data Model

Labels are protocol objects, similar to repository commits or MST nodes. They are canonically encoded as DAG-CBOR (a strict, normalized subset of CBOR) for for signing (see sections below). There is a Lexicon definition (`com.atproto.label.defs#label`) which represents labels, but it has slightly different field requirements than the core protocol object: the version field (`ver`) and signature (`sig`) are both optional in that version.

The fields on the label object are:

- `ver` (integer, required): label schema version. Current version is always `1`.
- `src` (string, DID format, required): the authority (account) which generated this label
- `uri` (string, URI format, required): the content that this label applies to. For a specific record, an `at://` URI. For an account, the `did:`.
- `cid` (string, CID format, optional): if provided, the label applies to a specific version of the subject `uri`
- `val` (string, 128 bytes max, required): the value of the label. Semantics and preferred syntax discussed below.
- `neg` (boolean, optional): if `true`, indicates that this label "negates" an earlier label with the same `src`, `uri`, and `val`.
- `cts` (string, datetime format, required): the timestamp when the label was created. Note that timestamps in a distributed system are not trustworthy or verified by default.
- `exp` (string, datetime format, optional): a timestamp at which this label expires (is not longer valid)
- `sig` (bytes, optional): cryptographic signature bytes. Uses the `bytes` type from the [Data Model](/specs/data-model), which encodes in JSON as a `$bytes` object with base64 encoding

When labels are being transferred as full objects between services, the `ver` and `sig` fields are required.

If the `neg` field is `false`, best practice is to simply not include the field at all.

The use of short three-character field names is different from most parts of atproto, and aligns more closely with JWTs. The motivation for this difference is to minimize the size of labels when many of them are included in network requests and responses.

## Value

The `val` field is core to the label. To keep the protocol flexible, and allow future development of ontologies, norms, and governance structures, very little about the semantics, behavior, and known values of this string are specified here.

The current expectation is that label value strings are "tokens" with fixed vocabulary. They are similar to hashtags.

At this time, we strongly recommend *against* the following patterns:

- packing additional structure in to value fields. for example, base64-encoded data, key/value syntax, lists or arrays of values, etc
- encoding arbitrary numerical values (eg, "scores" or "confidence")
- using punctuation characters (like `.`, `:`, `;`, `#`, `_`, `'`, `>`, or others) to structure the namespace of labels
- using URLs or URIs in values
- use of any whitespace
- use of non-ASCII characters, including emoji

These are all promising ideas, but we hope to coordinate and more formally specify this sort of syntax extension.

One current convention is to use a bang punctuation character (`!`) as a prefix for system-level labels which specify an expected behavior on a subject, but don't describe the content or indicate a reason for the behavior. For example, `!warn` as a behavior, as opposed to `scam` as a descriptive label which might result in the same warning behavior.

The behavior, definition, meaning, and policies around labels are generally communicated elsewhere. The value does not need to be entirely descriptive.

### Recommended String Syntax

The current recommended syntax for label strings is lower-case kebab-syntax (using `-` internally), using only ASCII letters. Specifically:

- lower-case alphabetical ASCII letters (`a` to `z`)
- dash (`-`) used for internal separation, but not as a first or last character
- no other punctuation or whitespace
- 128 bytes maximum length. Shorter is better (try to keep labels to a couple dozen characters at most), while still being somewhat descriptive.

## Label Lifecycle: Negation and Expiration

Labels are generally broadcast and persisted internally by receiving services. Some services may bulk re-broadcast or re-distribute labels to downstream services. They may ignore and drop any labels which are not relevant to their use-case. They may "hydrate" labels in to requests from clients.

When hydrating labels, services should generally only include "active" and relevant labels.

If the authoritative creator of a label wishes to retract or remove the label, they do so by publishing a new label with the same source, subject, and value, but with the negated field (`neg`) set to true, and a current timestamp (later than any previous timestamps). A negation label does not mean that the inverse of the label is “true”, only that the previous label has been retracted. For example, a label with value `spam` and `neg` true does not mean the subject is *not* spam, only that a previous `spam` label should be disregarded.

Receiving services that encounter a valid negation label may store the negation internally, and may re-broadcast the negation, but should not hydrate the negated label in API responses.

Likewise, may continue to persist expired labels (after the expiration timestamp), but should not continue to hydrate them in API responses.

## Signatures

Labels are signed using public-key [Cryptography](/specs/cryptography), similar to repository commit objects. Signatures should be validated when labels are transferred between services. It is assumed that most end-clients will not validate signatures themselves, and signatures may be removed from API responses sent to clients for network efficiency. Clients and other parties should have a mechanism to verify signatures, by querying individual signatures from labeling authorities, and receiving back the full label, including signature.

The process to sign or verify a signature is to construct a complete version of the label, using only the specified schema fields, and not including the `sig` field. This means including the `ver` field, but not any `$type` field or other un-specified fields which may have been included in a Lexicon representation of the label. This data object is then encoded in CBOR, following the deterministic IPLD/DAG-CBOR normalization rules. The CBOR bytes are hashed with SHA-256, and then the direct hash bytes (not a hex-encoded string) are signed (or verified) using the appropriate cryptographic key. The signature bytes are stored in the `sig` field as bytes (see [Data Model](/specs/data-model) for details representing bytes).

The key used for signing labels is found in the DID document for the issuing identity, and has fragment identifier `#atproto_label`. This key *may* have the same value as the `#atproto` signing key used for repository signatures. At this time, if an `#atproto_label` key is not found, implementation *should not* attempt to use other keys present in the DID document to verify the signature, they should simply consider the signature invalid and ignore the label.

### Signature Lifecycle

Signatures are verified at the time the label is received. They do not need to be re-verified before hydration in to API responses.

Signing key rotation can be difficult and disruptive for a large labeling service. The rough mechanism for doing a rotation is:

- labeling services should persist signatures alongside labels, and also persist an indicator of which key was used to sign the label
- when starting a rotation, pause creation and signing of new signatures
- update the DID document with the new key
- resume signing new labels with the new key
- when servicing queries for old labels, check which key was used for signing. if out of date, re-sign and persist the signatures for that batch of labels. the created-at timestamp should not be changed.
- older labels in the label event stream backfill period may have invalid signatures; this is acceptable

When encountering a label with an invalid signature, a good practice is to re-resolve the issuer identity (DID document) and check if there is an updated signing key. If there is, validation should be retried.

A downstream service can decide for themselves whether to bulk query and receive updated signatures when an upstream key rotation has occurred; or to fetch updated signatures on demand; or to consider old labels still valid even if the signature no longer validates against the current label signing key.

## Self-Labels in Records

One Lexicon design pattern is to include an array of label values inside a record. Downstream clients can interpret these as "self-labels", similarly to labels coming from external sources.

Note that the repository data storage mechanism provides context and lifecycle support similar to a full label object:

- the source of the label is the account controlling the repository
- the subject is the record itself, or possibly the overall repository account (depending on context)
- the CID is the current version of the record
- negation and expiration are not necessary, as the record can be deleted or updated to change the set of labels
- the created-at timestamp would be the same as the record itself (eg, via a `createdAt` field)
- authenticity (signature) is provided by the repository commit signing mechanism

## Labeler Service Identity

Labeler services each have a service identity, meaning a DID document. This is the DID that appears in label source (`src`) field.

The DID document will also have a key used for signing labels (with ID `#atproto_label`; see above for signature details), and a service endpoint (with ID `#atproto_labeler` and type `AtprotoLabeler`) which indicates the server URL (the URL includes method, hostname, and optional port, but no path segment at this time). Note that in real world use-cases, use of HTTPS on the default port (443) is strongly recommended and may be required by service operators.

Depending on the application, the identity may also have an atproto repository containing a “declaration record” which describes application-specific context about the labeler. This may be required for integration with a specific application, client, or AppView, but is not a requirement at the base atproto level.

## Label Distribution Endpoints

Two Lexicon endpoints are defined for labeler services to distribute labels:

`com.atproto.label.subscribeLabels`: an event stream (WebSocket) endpoint, which broadcasts new labels. Implements the `seq` backfill mechanism, similar to repository event stream, but with some small differences: the “backfill” period may extend to `cursor=0` (meaning that the full history of labels is available via the stream). Labels which have been redacted have the original label removed from the stream, but the negation remains.

`com.atproto.label.queryLabels`: a flexible query endpoint. Can be used to scroll over all labels (using a cursor parameter), or can filter to labels relating to a specific subject. 

Note that unlike public repository content, labels are not *required* to be publicly enumerable. It is acceptable for labeler services to make all labels publicly available using these endpoints, or to require authorization and access control, or to not implement these endpoints at all (if they have another mechanism for distributing labels).

## Labeler HTTP Headers

Labels are often “hydrated” in to HTTP API responses by atproto services, such as AppViews. To give clients control over which label sources they want included, two special HTTP headers are used, which PDS implementations are expected to pass-through when proxying requests:

`atproto-accept-labelers`: used in requests. A list of labeler service DIDs, with optional per-DID flags.

`atproto-content-labelers`: used in responses. Same content, syntax, and semantics as the `accept` header, but indicates which labelers could actually be queried. Presence in this header doesn’t mean that any labels from a given DID were actually included, only that it they would have been if such labels existed.

The syntax of these headers follows IETF RFC-8941 (”Structured Field Values for HTTP”), section 3.1.2 (”Parameters”). Values are separated by comma (ASCII `,` character), and values from repeated declaration of the header should be merged in to a single list. One or more optional parameters may follow the item value (the DID), separated by a semicolon (ASCII `;` character). For boolean parameters, the full RFC syntax (just as `param=?0` for false) is not currently supported. Instead, the presence of the parameter indicates it is “true”, and the absence indicates “false”. No other parameter values types (such as integers or strings) are supported at this time. 

The only currently supported parameter is the boolean parameter `redact`. This flag indicates that the service hydrating labels should handle the special protocol-level label values `!takedown` and `!suspend` by entirely redacting content from the API response, instead of simply labeling it. This may result in an application-specific tombstone entry, which might indicate the Labeler responsible for the redaction, or could result in the content being removed without a tombstone.

Complete example syntax for these headers:

```
# on a request
atproto-accept-labelers: did:web:mod.example.com;redact, did:plc:abc123, did:plc:xyz789

# on a response:
atproto-content-labelers: did:web:mod.example.com;redact, did:plc:abc123, did:plc:xyz789
```

If the syntax of the request header is invalid or can not be parsed, the service should return an error instead of ignoring the header.

If a labeler DID is repeated in the header, parameters should be combined from each instance. For example, if a DID is included once with `redact` and once without, the service should treat this the same as if the DID was included once, with `redact`. The `atproto-content-labelers` response header should represent how the request header was de-duplicated and interpreted.

If the request header is not supplied at all, the service may substitute a default. This is distinct from supplying the header with no value, in which case the service should not hydrate or apply any labels.

If any labeler DID is indicated with correct syntax, but the identity does not exist; does not include labeler service or key entries in the DID doc; has been taken down at the service level; or is otherwise inactive or non-functional; then that labeler should not be included in the `atproto-content-labelers` response header, but does not need to be treated as an error.

A service implementation may decide, as a policy matter, that specific conditions must be met or the request will error. For example, that a specific labeler DID must be included; or a minimum or maximum number of labelers can be included; or a minimum or maximum number of labelers with `redact` are included.

## Security Considerations

Note that there is no "domain differentiation" of the signature, meaning that there is potential security risk of signing a label which is also a valid object (and signature) in an entirely different context, like an authentication bearer token. This makes it important to ensure that no additional or unexpected fields are included in the object that is being signed.

## Usage and Implementation Guidelines

It is strongly recommended to stick to the “recommended string syntax” for label values at this time.

## Possible Future Changes

More mature governance, namespacing, and style guide recommendations on label values.


---
atproto/src/app/[locale]/specs/label/ko.mdx
---
export const metadata = {
  title: '라벨',
  description:
    '모더레이션 및 기타 목적으로 사용되는 계정 또는 콘텐츠에 대한 자체 인증 문자열 주석.',
}

# 라벨

**라벨**은 atproto 생태계 내의 모든 계정 또는 콘텐츠에 관한 메타데이터입니다. {{ className: 'lead' }}

라벨은 독립적인 자체 인증 데이터 객체로 존재하지만, 종종 API 응답의 일부로 배포되기도 합니다(이 경우 서명이 포함되지 않을 수 있음). 또한 라벨의 "값"은 레코드 내에 직접 포함될 수 있는데, 이를 "셀프 라벨"이라고 합니다. {{ className: 'lead' }}

라벨은 주로 발행자(DID), 대상(URI) 및 값으로 구성됩니다. 값은 태그나 해시태그와 유사한 짧은 문자열이며, 라벨의 생성자와 수신자 간에 미리 정의된 의미를 갖는다고 가정됩니다. 추가 메타데이터 필드는 부가적인 문맥을 제공할 수 있으나, 언제든지 발행자, 대상, 값의 조합에 대해 하나의 일관된 메타데이터 집합만 존재해야 합니다. 만약 여러 개의 메타데이터 집합이 존재한다면, `created-at` 타임스탬프를 사용하여 현재 유효한 라벨을 구분합니다. {{ className: 'lead' }}

라벨 개념 및 프로토콜 기본 요소는 범위, 사용 사례, 데이터 전송 방식에 있어 유연합니다. 원래 설계 동기 중 하나는 모더레이션 서비스를 통해 생성된 라벨을 사용하여 조합형 모더레이션을 가능하게 하는 것이었습니다. 하지만 라벨은 모더레이션에 국한되지 않으며, atproto 애플리케이션 내에서 다른 용도로도 자유롭게 재사용될 수 있습니다. {{ className: 'lead' }}

핵심 라벨 스키마는 버전 관리되며, 이 문서는 라벨 버전 `1`에 대해 설명합니다. {{ className: 'lead' }}

## 스키마 및 데이터 모델

라벨은 저장소 커밋이나 MST 노드와 유사한 프로토콜 객체입니다. 라벨은 서명을 위해 DAG-CBOR(엄격하고 정규화된 CBOR의 하위 집합)로 정식 인코딩됩니다(아래 섹션 참조). 라벨을 표현하는 Lexicon 정의(`com.atproto.label.defs#label`)가 있으나, 핵심 프로토콜 객체와 약간 다른 필드 요구사항을 가지고 있습니다: 버전 필드(`ver`)와 서명(`sig`)은 해당 버전에서는 선택 사항입니다.

라벨 객체의 필드는 다음과 같습니다:

- `ver` (정수, 필수): 라벨 스키마 버전. 현재 버전은 항상 `1`입니다.
- `src` (문자열, DID 형식, 필수): 이 라벨을 생성한 주체(계정).
- `uri` (문자열, URI 형식, 필수): 라벨이 적용되는 콘텐츠. 특정 레코드의 경우 `at://` URI, 계정의 경우 `did:` 형식.
- `cid` (문자열, CID 형식, 선택 사항): 제공될 경우, 라벨은 대상 `uri`의 특정 버전에 적용됩니다.
- `val` (문자열, 최대 128바이트, 필수): 라벨의 값. 아래에서 논의하는 의미와 선호하는 구문을 따릅니다.
- `neg` (불리언, 선택 사항): `true`로 설정되면, 동일한 `src`, `uri`, `val`을 가진 이전 라벨을 “무효화”함을 나타냅니다.
- `cts` (문자열, 날짜시간 형식, 필수): 라벨이 생성된 시각. 분산 시스템에서는 타임스탬프가 기본적으로 신뢰할 수 없거나 검증되지 않는 점에 유의하십시오.
- `exp` (문자열, 날짜시간 형식, 선택 사항): 이 라벨의 유효기간이 끝나는 시각.
- `sig` (바이트, 선택 사항): 암호화 서명 바이트. 이는 [데이터 모델](/specs/data-model)의 `bytes` 타입을 사용하며, JSON에서는 base64 인코딩된 `$bytes` 객체로 표현됩니다.

라벨이 전체 객체로서 서비스 간에 전송될 때는 `ver` 및 `sig` 필드가 필수입니다.

`neg` 필드가 `false`인 경우, 필드를 포함하지 않는 것이 최선의 관행입니다.

짧은 세 글자 필드 이름을 사용하는 것은 atproto의 다른 부분과 다르며, JWT와 더 밀접하게 정렬됩니다. 이는 네트워크 요청 및 응답에 다수의 라벨이 포함될 때 크기를 최소화하기 위함입니다.

## 값

`val` 필드는 라벨의 핵심입니다. 프로토콜의 유연성을 유지하고, 향후 온톨로지, 규범, 거버넌스 구조의 발전을 허용하기 위해, 이 문자열의 의미, 동작, 그리고 알려진 값에 대해 여기서는 거의 명시하지 않습니다.

현재 라벨 값 문자열은 미리 정해진 어휘를 가진 "토큰"으로 사용되는 것이 바람직하며, 해시태그와 유사합니다.

현재 시점에서 다음 패턴은 *사용하지 않을 것*을 강력히 권장합니다:

- 값 필드 내에 추가 구조를 포함하는 것 (예: base64 인코딩 데이터, key/value 구문, 값의 리스트나 배열 등)
- 임의의 숫자 값(예: "점수"나 "신뢰도") 인코딩
- 구분을 위해 구두점 문자 사용 (예: `.`, `:`, `;`, `#`, `_`, `'`, `>` 등)
- 값에 URL 또는 URI 인코딩
- 공백 문자 사용
- 비ASCII 문자(이모지 포함) 사용

이러한 아이디어들도 매력적이지만, 구문 확장에 대해 보다 공식적으로 지정하기 위한 조율을 희망합니다.

현재 한 가지 관례는 시스템 레벨 라벨을 나타내기 위해 느낌표(`!`)를 접두사로 사용하는 것입니다. 이는 대상에 대해 기대되는 행동을 지정하지만 콘텐츠 자체를 설명하거나 그 이유를 나타내지는 않습니다. 예를 들어, `!warn`은 경고 행동을 나타내며, `scam`은 설명적 라벨로 같은 경고 행동을 유발할 수 있습니다.

라벨의 행동, 정의, 의미, 정책은 일반적으로 다른 문서를 통해 전달됩니다. 값이 반드시 전부를 설명할 필요는 없습니다.

### 권장 문자열 구문

현재 라벨 값 문자열에 대한 권장 구문은 소문자 케밥 케이스(kebab-case)로, ASCII 문자만 사용하며 내부적으로 `-`를 사용하는 것입니다. 구체적으로:

- 소문자 알파벳 ASCII 문자 (`a` ~ `z`)
- 내부 구분자로 사용되는 대시(`-`): 단, 처음이나 마지막에 오지 않아야 함
- 기타 구두점이나 공백 문자는 사용하지 않음
- 최대 128바이트. 가능한 한 짧게(최대 몇십 자 내외) 유지하되, 어느 정도 설명력을 갖추도록 합니다.

## 라벨 수명 주기: 무효화와 만료

라벨은 일반적으로 수신 서비스에 의해 브로드캐스트되고 내부적으로 저장됩니다. 일부 서비스는 라벨을 다운스트림 서비스에 재배포할 수 있으며, API 응답에 포함될 때 라벨 서명이 제거될 수 있습니다.

라벨을 API 응답에 포함할 때는 일반적으로 "활성" 상태이며 관련 있는 라벨만 포함해야 합니다.

라벨의 권위 있는 생성자가 라벨을 취소하거나 제거하고자 할 경우, 동일한 발행자, 대상, 값을 가지되 `neg` 필드를 `true`로 설정하고 최신 타임스탬프를 가진 새로운 라벨을 발행합니다. 무효화 라벨은 이전 라벨의 반대를 “참”으로 만드는 것이 아니라, 이전 라벨을 무시해야 함을 의미합니다. 예를 들어, 값이 `spam`인 라벨에 `neg`가 `true`로 설정되었다고 해서 대상이 *스팸이 아니다*라는 의미는 아니며, 단지 이전의 `spam` 라벨을 무시해야 함을 나타냅니다.

유효한 무효화 라벨을 수신한 서비스는 내부적으로 이를 저장하고 재배포할 수 있으나, API 응답에 무효화된 라벨은 포함하지 않아야 합니다.

마찬가지로, 만료 시각이 지난 라벨은 저장될 수 있으나 API 응답에 포함해서는 안 됩니다.

## 서명

라벨은 공개키 [암호화](/specs/cryptography)를 사용하여 서명됩니다. 서명은 저장소 커밋 객체와 유사하게 생성됩니다. 서비스 간 라벨 전송 시 서명을 검증해야 하며, 대부분의 최종 클라이언트는 서명을 직접 검증하지 않고 네트워크 효율성을 위해 API 응답에서 서명을 제거할 수 있습니다. 클라이언트 및 기타 당사자는 개별 라벨에 대해 라벨 발행자의 서명을 질의하고 전체 라벨(서명 포함)을 수신함으로써 서명을 검증할 수 있는 메커니즘을 가져야 합니다.

서명 또는 서명 검증 절차는 라벨의 모든 스키마 필드를 포함하여 완전한 버전의 라벨을 구성하는 것으로 시작하며, `sig` 필드는 제외합니다. 이는 `ver` 필드는 포함하지만, Lexicon 표현에 있을 수 있는 `$type` 필드나 기타 명시되지 않은 필드는 포함하지 않는다는 의미입니다. 이 데이터 객체는 DAG-CBOR 정규화 규칙(결정론적 IPLD/DAG-CBOR)을 따르는 CBOR로 인코딩됩니다. 인코딩된 CBOR 바이트는 SHA-256으로 해시되고, 이 해시의 직접 바이트(16진수 문자열이 아님)를 적절한 암호화 키를 사용해 서명(또는 검증)합니다. 서명 바이트는 `sig` 필드에 바이트 형태로 저장됩니다(자세한 내용은 [데이터 모델](/specs/data-model)을 참조).

라벨 서명에 사용되는 키는 발행자 DID 문서 내에 있으며, 조각 식별자 `#atproto_label`을 가집니다. 이 키는 저장소 서명에 사용되는 `#atproto` 키와 동일할 수도 있습니다. 현재로서는 DID 문서에 `#atproto_label` 키가 없을 경우, 다른 키로 서명을 검증하려 해서는 안 되며, 단순히 서명을 무효한 것으로 간주하고 라벨을 무시해야 합니다.

### 서명 수명 주기

라벨을 수신할 때 서명이 검증되며, API 응답에 포함되기 전에는 재검증할 필요가 없습니다.

서명 키의 교체는 대규모 라벨링 서비스에 있어 어렵고 파괴적일 수 있습니다. 교체 절차는 다음과 같습니다:

- 라벨링 서비스는 라벨과 함께 서명을, 그리고 서명에 사용된 키에 대한 표시기를 함께 저장해야 합니다.
- 교체를 시작할 때는 새로운 서명 생성 작업을 일시 중지합니다.
- DID 문서를 업데이트하여 새로운 키를 등록합니다.
- 새로운 키로 라벨 서명을 재개합니다.
- 이전 라벨에 대해 질의 시, 사용된 키를 확인하고, 구버전이면 해당 배치의 라벨을 재서명하여 저장합니다. 이때 `created-at` 타임스탬프는 변경되지 않아야 합니다.
- 이벤트 스트림 기간 동안의 구버전 라벨은 서명이 유효하지 않을 수 있으며, 이는 허용됩니다.

유효하지 않은 서명이 있는 라벨을 만날 경우, 좋은 방법은 발행자의 DID 문서를 재조회하여 업데이트된 서명 키가 있는지 확인하는 것입니다. 만약 있다면, 서명 검증을 재시도해야 합니다.

다운스트림 서비스는 상위 키 교체 시, 구버전 라벨에 대해 일괄적으로 업데이트된 서명을 질의할지, 필요 시에 따라 개별적으로 질의할지, 아니면 구버전 서명이더라도 유효하다고 판단할지를 결정할 수 있습니다.

## 레코드 내 셀프 라벨

Lexicon 디자인 패턴 중 하나는 레코드 내부에 라벨 값 배열을 포함하는 것입니다. 다운스트림 클라이언트는 이를 외부 소스에서 온 라벨과 유사하게 "셀프 라벨"로 해석할 수 있습니다.

저장소 데이터 저장 메커니즘은 전체 라벨 객체와 유사한 문맥 및 수명 주기를 제공합니다:

- 라벨의 발행자는 저장소를 제어하는 계정입니다.
- 대상은 레코드 자체이거나 상황에 따라 전체 저장소 계정일 수 있습니다.
- CID는 레코드의 현재 버전을 나타냅니다.
- 레코드의 삭제나 업데이트를 통해 라벨 집합이 변경되므로, 무효화 및 만료 처리가 필요하지 않습니다.
- 생성 시각은 레코드 자체의 `createdAt` 필드와 동일합니다.
- 저장소 커밋 서명 메커니즘에 의해 진위(서명)가 보장됩니다.

## 라벨러 서비스 정체성

라벨러 서비스는 각각 서비스 정체성(DID 문서)을 가지고 있으며, 이는 라벨의 발행자(`src`) 필드에 나타납니다.

DID 문서에는 라벨 서명에 사용되는 키(식별자 `#atproto_label`)와 서비스 엔드포인트(식별자 `#atproto_labeler`, 타입 `AtprotoLabeler`)가 포함되며, 이는 서버 URL(메서드, 호스트 이름, 선택적 포트, 단 경로 세그먼트 없음)을 나타냅니다. 실제 사용 사례에서는 기본 포트(443)의 HTTPS 사용이 강력히 권장되며, 서비스 운영자가 이를 요구할 수 있습니다.

일부 애플리케이션에서는 정체성이 atproto 저장소(“declaration record”)를 포함할 수 있으며, 이는 라벨러에 대한 애플리케이션 별 문맥 정보를 설명합니다. 이는 특정 애플리케이션, 클라이언트 또는 AppView와의 통합을 위해 필요할 수 있으나, atproto 기본 수준에서는 필수 사항이 아닙니다.

## 라벨 배포 엔드포인트

라벨러 서비스가 라벨을 배포하기 위해 정의된 두 가지 Lexicon 엔드포인트는 다음과 같습니다:

- `com.atproto.label.subscribeLabels`: 새로운 라벨을 브로드캐스트하는 이벤트 스트림(WebSocket) 엔드포인트. 저장소 이벤트 스트림과 유사한 `seq` 백필 메커니즘을 구현하지만, 일부 차이점이 있습니다. 예를 들어, 백필 기간이 `cursor=0`까지 확장되어 전체 라벨 이력이 스트림을 통해 제공될 수 있습니다. 삭제된 라벨의 경우 원본 라벨은 스트림에서 제거되지만, 무효화 라벨은 남아 있습니다.
- `com.atproto.label.queryLabels`: 유연한 질의를 위한 엔드포인트. 커서를 사용하여 모든 라벨을 스크롤할 수 있으며, 특정 대상과 관련된 라벨로 필터링할 수도 있습니다.

공개 저장소 콘텐츠와 달리, 라벨은 반드시 공개적으로 열람 가능할 필요는 없습니다. 라벨러 서비스가 이 엔드포인트를 통해 모든 라벨을 공개할 수도 있고, 인증 및 접근 제어를 요구하거나, 또는 전혀 구현하지 않을 수도 있습니다(다른 라벨 배포 메커니즘이 있을 경우).

## 라벨러 HTTP 헤더

라벨은 종종 atproto 서비스(예: AppView)에 의해 HTTP API 응답에 “수화(hydrate)”됩니다. 클라이언트가 포함할 라벨 소스를 제어할 수 있도록 두 개의 특별 HTTP 헤더가 사용되며, PDS 구현체는 요청을 프록시할 때 이를 그대로 전달해야 합니다:

- `atproto-accept-labelers`: 요청 시 사용. 라벨러 서비스 DID 목록과 선택적 DID별 플래그 목록.
- `atproto-content-labelers`: 응답 시 사용. 구문, 내용, 의미는 `accept` 헤더와 동일하며, 실제로 질의 가능한 라벨러를 나타냅니다. 이 헤더에 DID가 포함되었다고 해서 해당 DID의 라벨이 반드시 포함되는 것은 아니며, 단지 포함되었을 경우 질의가 가능함을 의미합니다.

이 헤더들의 구문은 IETF RFC-8941("HTTP를 위한 구조화 필드 값")의 섹션 3.1.2("매개변수")를 따릅니다. 값은 쉼표(ASCII `,`)로 구분되며, 동일 헤더의 반복 선언 시 값은 하나의 목록으로 병합됩니다. 항목 값 뒤에는 하나 이상의 선택적 매개변수가 세미콜론(ASCII `;`)으로 구분되어 따라올 수 있습니다. 불리언 매개변수의 경우, RFC의 전체 구문(예: false의 경우 `param=?0`)은 현재 지원되지 않습니다. 대신, 매개변수가 존재하면 “true”로, 없으면 “false”로 간주합니다. 현재는 정수나 문자열과 같은 다른 매개변수 타입은 지원되지 않습니다.

현재 지원되는 유일한 매개변수는 불리언 매개변수 `redact`입니다. 이 플래그는 라벨 수화를 수행하는 서비스가 프로토콜 레벨의 라벨 값 `!takedown` 및 `!suspend`를 단순히 라벨링하는 대신, API 응답에서 콘텐츠를 전면적으로 삭제하도록 처리해야 함을 나타냅니다. 이 경우, 애플리케이션 별로 라벨러 정보를 표시하는 “툼브스톤” 항목이 생성되거나, 별도의 툼브스톤 없이 콘텐츠가 제거될 수 있습니다.

다음은 이러한 헤더의 완전한 예시 구문입니다:

```
# 요청 시
atproto-accept-labelers: did:web:mod.example.com;redact, did:plc:abc123, did:plc:xyz789

# 응답 시:
atproto-content-labelers: did:web:mod.example.com;redact, did:plc:abc123, did:plc:xyz789
```

요청 헤더의 구문이 잘못되었거나 파싱할 수 없는 경우, 서비스는 헤더를 무시하는 대신 오류를 반환해야 합니다.

헤더에 동일한 라벨러 DID가 반복해서 포함된 경우, 각 인스턴스의 매개변수를 결합해야 합니다. 예를 들어, 한 번은 `redact`가 포함되고 다른 한 번은 포함되지 않은 경우, 해당 DID는 단 한 번, `redact`가 포함된 것으로 처리해야 합니다. 응답 헤더 `atproto-content-labelers`는 요청 헤더가 중복 제거되고 해석된 결과를 나타냅니다.

요청 헤더가 전혀 제공되지 않을 경우, 서비스는 기본값을 대체할 수 있습니다. 이는 값이 없는 헤더를 제공하는 것과는 구분되며, 이 경우 서비스는 어떠한 라벨도 수화하거나 적용해서는 안 됩니다.

헤더에 올바른 구문으로 표시된 라벨러 DID라도, 해당 정체성이 존재하지 않거나 DID 문서에 라벨러 서비스 또는 키 항목이 포함되어 있지 않거나, 서비스 차원에서 중단되었거나, 그 외 비활성 상태인 경우, 해당 라벨러는 응답 헤더에 포함되지 않아야 하며, 오류로 처리할 필요는 없습니다.

서비스 구현체는 정책 상 특정 조건을 요구할 수 있으며, 그렇지 않을 경우 오류를 반환할 수 있습니다. 예를 들어, 특정 라벨러 DID가 반드시 포함되어야 한다거나, 허용되는 라벨러 수 혹은 `redact` 플래그가 있는 라벨러의 최소 또는 최대 수가 정해져 있을 수 있습니다.

## 보안 고려사항

라벨 서명에는 "도메인 구분"이 없으므로, 동일한 객체(및 서명)가 전혀 다른 컨텍스트에서 유효할 수 있는 위험이 존재합니다. 이는 서명 대상 객체에 예상치 못한 추가 필드가 포함되지 않도록 하는 것이 중요함을 의미합니다.

## 사용 및 구현 가이드라인

현재 시점에서는 라벨 값에 대해 “권장 문자열 구문”을 따르는 것이 강력히 권장됩니다.

## 향후 변경 가능 사항

향후 라벨 값에 대한 거버넌스, 네임스페이싱, 스타일 가이드에 대한 보다 성숙한 권고안이 마련될 수 있습니다.

---
atproto/src/app/[locale]/specs/label/page.tsx
---
export const metadata = {
  title: 'Labels',
  description:
    'Self-authenticating string annotations on accounts or content for moderation and other purposes.',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/handle/en.mdx
---
export const metadata = {
  title: 'Handle',
  description:
    'A specification for human-friendly account identifiers.',
}

# Handle

DIDs are the long-term persistent identifiers for accounts in atproto, but they can be opaque and unfriendly for human use. Handles are a less-permanent identifier for accounts. The mechanism for verifying the link between an account handle and an account DID relies on DNS, and possibly connections to a network host, so every handle must be a valid network hostname. *Almost* every valid "hostname" is also a valid handle, though there are a small number of exceptions. {{ className: 'lead' }}

The definition "hostnames" (as a subset of all possible "DNS names") has evolved over time and across several RFCs. Some relevant documents are [RFC-1035](https://www.rfc-editor.org/rfc/rfc1035), [RFC-3696](https://www.rfc-editor.org/rfc/rfc3696) section 2, and [RFC-3986](https://www.rfc-editor.org/rfc/rfc3986) section 3. {{ className: 'lead' }}

## Handle Identifier Syntax

Lexicon string format type: `handle`

To synthesize other standards, and define "handle" syntax specifically:

- The overall handle must contain only ASCII characters, and can be at most 253 characters long (in practice, handles may be restricted to a slightly shorter length)
- The overall handle is split in to multiple segments (referred to as "labels" in standards documents), separated by ASCII periods (`.`)
- No proceeding or trailing ASCII periods are allowed, and there must be at least two segments. That is, "bare" top-level domains are not allowed as handles, even if valid "hostnames" and "DNS names." "Trailing dot" syntax for DNS names is not allowed for handles.
- Each segment must have at least 1 and at most 63 characters (not including the periods). The allowed characters are ASCII letters (`a-z`), digits (`0-9`), and hyphens (`-`).
- Segments can not start or end with a hyphen
- The last segment (the "top level domain") can not start with a numeric digit
- Handles are not case-sensitive, and should be normalized to lowercase (that is, normalize ASCII `A-Z` to `a-z`)

To be explicit (the above rules already specify this), no whitespace, null bytes, joining characters, or other ASCII control characters are allowed in the handle, including as prefix/suffix.

Modern "hostnames" (and thus handles) allow ASCII digits in most positions, with the exception that the last segment (top-level domain, TLD) cannot start with a digit.

IP addresses are not valid syntax: IPv4 addresses have a final segment starting with a digit, and IPv6 addresses are separated by colons (`:`).

A reference regular expression (regex) for the handle syntax is:

```
/^([a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$/
```

## Additional Non-Syntax Restrictions

"Reserved" top-level domains should not fail syntax validation (eg, in atproto Lexicon validation), but they must immediately fail any attempt at registration, resolution, etc. See also: [https://en.wikipedia.org/wiki/Top-level_domain#Reserved_domains](https://en.wikipedia.org/wiki/Top-level_domain#Reserved_domains)

`.local` hostnames (for mDNS on local networks) should not be used in atproto.

The `.onion` TLD is a special case for Tor protocol hidden services. Resolution of handles via Tor would require ecosystem-wide support, so they are currently disallowed.

To summarize the above, the initial list of disallowed TLDs includes:

- `.alt`
- `.arpa`
- `.example`
- `.internal`
- `.invalid`
- `.local`
- `.localhost`
- `.onion`

The `.test` TLD is intended for examples, testing, and development. It may be used in atproto development, but should fail in real-world environments.

The `.invalid` TLD should only be used for the special `handle.invalid` value (see below). This value is syntactically valid in the Lexicon schema language, but should not be accepted as a valid handle in most contexts.

## Identifier Examples

Syntactically valid handles (which may or may not have existing TLDs):

```
jay.bsky.social
8.cn
name.t--t        // not a real TLD, but syntax ok
XX.LCS.MIT.EDU
a.co
xn--notarealidn.com
xn--fiqa61au8b7zsevnm8ak20mc4a87e.xn--fiqs8s
xn--ls8h.test
example.t        // not a real TLD, but syntax ok
```

Invalid syntax:

```
jo@hn.test
💩.test
john..test
xn--bcher-.tld
john.0
cn.8
www.masełkowski.pl.com
org
name.org.
```

Valid syntax, but must always fail resolution due to other restrictions:

```
2gzyxa5ihm7nsggfxnu52rck2vv4rvmdlkiu3zzui5du4xyclen53wid.onion
laptop.local
blah.arpa
```

## Handle Resolution

Handles have a limited role in atproto, and need to be resolved to a DID in almost all situations. Resolution mechanisms must demonstrate a reasonable degree of authority over the domain name at a point in time, and need to be relatively efficient to look up. There are currently two supported resolution mechanisms, one using a TXT DNS record containing the DID, and another over HTTPS at a special `/.well-known/` URL.

Clients can rely on network services (eg, their PDS) to resolve handles for them, using the `com.atproto.identity.resolveHandle` endpoint, and don't usually need to implement resolution directly themselves.

The DNS TXT method is the recommended and preferred resolution method for individual handle configuration, but services should fully support both methods. The intended use-case for the HTTPS method is existing large-scale web services which may not have the infrastructure to automate the registration of thousands or millions of DNS TXT records.

Handles should not be trusted or considered valid until the DID is also resolved and the current DID document is confirmed to link back to the handle. The link between handle and DID must be confirmed bidirectionally, otherwise anybody could create handle aliases for third-party accounts.

### DNS TXT Method

For this resolution method, a DNS TXT record is registered for the `_atproto` sub-domain under the handle hostname. The record value should have the prefix `did=`, followed by the full domain. This method aligns with [RFC-1464](https://www.rfc-editor.org/rfc/rfc1464.html), "Using the Domain Name System To Store Arbitrary String Attributes".

For example, the handle `bsky.app` would have a TXT record on the name `_atproto.bsky.app`, and the value would look like `did=did:plc:z72i7hdynmk6r22z27h6tvur`.

Any TXT records with values not starting with `did=` should be ignored. Only a single valid record should exist at any point in time. If multiple valid records with different DIDs are present, resolution should fail. In this case resolution can be re-tried after a delay, or using a recursive resolver.

Note that very long handles can not be resolved using this method if the additional `_atproto.` name segment pushes the overall name over the 253 character maximum for DNS queries. The HTTPS method will work for such handles.

DNSSEC is not required.

### HTTPS well-known Method

For this resolution method, a web server at the handle domain implements a special well-known endpoint at the path `/.well-known/atproto-did`. A valid HTTP response will have an HTTP success status (2xx), `Content-Type` header set to `text/plain`, and include the DID as the HTTP body with no prefix or wrapper formatting.

For example, the handle `bsky.app` would be resolved by an GET request to `https://bsky.app/.well-known/atproto-did`, and a valid response would look like:

```
HTTP/1.1 200 OK
Content-Length: 33
Content-Type: text/plain
Date: Wed, 14 Jun 2023 00:47:21 GMT

did:plc:z72i7hdynmk6r22z27h6tvur

```

The response `Content-Type` header does not need to be strictly verified.

The web server's response body should not contain any prefix or suffix whitespace, but clients should strip small amounts of prefix or suffix whitespace from the response body before attempting to parse as a DID.

Secure HTTPS on the default port (443) is required for all real-world handle resolutions. HTTP should only be used for local development and testing.

HTTP redirects (eg, 301, 302) are allowed, up to a reasonable number of redirect hops.

### Invalid Handles

If the handle for a known DID is confirmed to no longer resolve, it should be marked as invalid. In API responses, the special handle value `handle.invalid` can be used to indicate that there is no bi-directionally valid handle for the given DID. This handle can not be used in most situations (search queries, API requests, etc).

### Resolution Best Practices

It is ok to attempt both resolution methods in parallel, and to use the first successful result available. If the two methods return conflicting results (aka, different DIDs), the DNS TXT result should be preferred, though it is also acceptable to record the result as ambiguous and try again later.

It is considered a best practice for services to cache handle resolution results internally, up to some lifetime, and re-resolve periodically. DNS TTL values provide a possible cache lifetime, but are probably too aggressive (aka, too short a lifetime) for the handle resolution use case.

Use of a recursive DNS resolver can help with propagation delays, which are important for the use case of an account changing their handle and waiting for confirmation.

With both techniques, it is beneficial to initiate resolution requests from a relatively trusted network environment and configuration. Running resolution requests from multiple regions and environments can help mitigate (though not fully resolve) concerns about traffic manipulation or intentionally segmented responses.


## Usage and Implementation Guidelines

Handles may be prefixed with the "at" symbol (like `@jay.bsky.team`) in user interfaces, but this is not a valid syntax for a handle in records, APIs, and other back-end contexts.

Internationalized Domain Names ("IDN", or "punycode") are not directly relevant to the low-level handle syntax. In their encoded form, IDNs are already valid hostnames, and thus valid handles. Such handles must be stored and transmitted in encoded ASCII form. Handles that "look like" IDNs, but do not parse as valid IDNs, are valid handles, just as they are valid hostnames. Applications may, optionally, parse and display IDN handles as Unicode.

Handles are not case-sensitive, which means they can be safely normalized from user input to lower-case (ASCII) form. Only normalized (lowercase) handles should be stored in records or used in outbound API calls. Applications should not preserve user-provided case information and attempt to display handles in anything other than lower-case. For example, the handle input string `BlueskyWeb.xyz`  should be normalized, stored, and displayed as `blueskyweb.xyz`. Long all-lowercase handles can be a readability and accessibility challenge. Sub-domain separation (periods), hyphenation, or use of "display names" in application protocols can all help.

Very long handles are known to present user interface challenges, but they are allowed in the protocol, and application developers are expected to support them.

Handles which look similar to a well-known domain present security and impersonation challenges. For example, handles like `paypa1.com` or `paypal.cc` being confused for `paypal.com`. Very long handles can result in similar issues when truncated at the start or end (`paypal.com…`).

Handles should generally not be truncated to local context. For example, the handle `@jay.bsky.social` should not be displayed as `@jay`, even in the local context of a `bsky.social` service.

Providers of handle "namespaces" (eg, as subdomains on a registered domain) may impose any additional limits on handles that they wish. It is recommended to constrain the allowed segment length to something reasonable, and to reserve a common set of segment strings like `www`, `admin`, `mail`, etc. There are multiple public lists of "commonly disallowed usernames" that can be used as a starting point.

From a practical standpoint, handles should be limited to at most 244 characters, fewer than the 253 allowed for DNS names. This is because DNS verification works with the prefix `_atproto.`, which adds 9 characters, and that overall name needs to be valid.

Handle hostnames are expected to be mainstream DNS domain names, registered through the mainstream DNS name system. Handles with non-standard TLDs, or using non-standard naming systems, will fail to interoperate with other network services and protocol implementations in the atproto ecosystem.

PDS implementations hosting an account *may* prevent repo mutation if the account's handle can no longer be verified (aka, `handle.invalid` situation). Other network services should generally continue to display the content (to prevent breakage), possibly with a contextual note or warning indicator.

## Possible Future Changes

The handle syntax is relatively stable.

It is conceivable that `.onion` handles would be allowed at some point in the future.


---
atproto/src/app/[locale]/specs/handle/ko.mdx
---
export const metadata = {
  title: '핸들',
  description: '사람 친화적인 계정 식별자에 대한 명세입니다.',
}

# 핸들

DID는 atproto에서 계정을 위한 장기적이고 영속적인 식별자이지만, 사람이 읽기에는 다소 모호할 수 있습니다. 이에 반해 핸들은 계정을 식별하기 위한 덜 영구적인 식별자입니다. 계정 핸들과 계정 DID 간의 연결을 검증하는 메커니즘은 DNS(및 경우에 따라 네트워크 호스트와의 연결)에 의존하므로, 모든 핸들은 유효한 네트워크 호스트네임이어야 합니다. *거의* 모든 유효한 "호스트네임"이 핸들로도 유효하지만, 소수의 예외가 존재합니다. {{ className: 'lead' }}

"호스트네임"의 정의(즉, 모든 가능한 "DNS 이름"의 부분집합)는 시간이 지남에 따라, 또는 여러 RFC 문서에 따라 변해왔습니다. 관련 문서로는 [RFC-1035](https://www.rfc-editor.org/rfc/rfc1035), [RFC-3696](https://www.rfc-editor.org/rfc/rfc3696) 2절, 그리고 [RFC-3986](https://www.rfc-editor.org/rfc/rfc3986) 3절 등이 있습니다. {{ className: 'lead' }}

## 핸들 식별자 문법

Lexicon 문자열 포맷 타입: `handle`

다른 표준과의 조화를 위해, 그리고 "핸들" 문법을 구체적으로 정의하기 위해:

- 전체 핸들은 ASCII 문자만 포함해야 하며, 최대 253자까지 허용됩니다 (실제로는 핸들이 다소 짧게 제한될 수 있습니다)
- 전체 핸들은 여러 개의 세그먼트(표준 문서에서는 "라벨"이라고 함)로 나뉘며, ASCII 마침표(`.`)로 구분됩니다.
- 시작이나 끝에 마침표가 올 수 없으며, 최소 두 개의 세그먼트가 필요합니다. 즉, "tld"와 같은 단일 최상위 도메인은 핸들로 허용되지 않습니다. (DNS의 "trailing dot" 표기법 역시 핸들에서는 허용되지 않습니다.)
- 각 세그먼트는 1자 이상, 63자 이하여야 하며 (마침표 제외), 허용되는 문자는 ASCII 영문자(`a-z`), 숫자(`0-9`), 그리고 하이픈(`-`)입니다.
- 각 세그먼트는 하이픈으로 시작하거나 끝날 수 없습니다.
- 마지막 세그먼트(최상위 도메인)는 숫자로 시작할 수 없습니다.
- 핸들은 대소문자를 구분하지 않으며, 모두 소문자(ASCII `a-z`)로 정규화되어야 합니다.

즉, 위 규칙은 핸들 내에 공백, 널 바이트, 조인 문자, 기타 ASCII 제어 문자(앞뒤 공백 포함)가 포함되지 않음을 명시합니다.

현대의 "호스트네임" (따라서 핸들도)은 대부분의 위치에서 ASCII 숫자 사용을 허용하나, 마지막 세그먼트(최상위 도메인)는 숫자로 시작할 수 없습니다.

IP 주소는 유효한 문법이 아닙니다: IPv4 주소의 마지막 세그먼트는 숫자로 시작하며, IPv6 주소는 콜론(`:`)으로 구분됩니다.

핸들 문법에 대한 참고용 정규 표현식(regex)은 다음과 같습니다:

```
/^([a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?$/
```

## 추가적인 문법 외 제한사항

"예약된" 최상위 도메인은 문법 검증(예, atproto Lexicon 검증)에서는 통과할 수 있으나, 등록이나 해석 등의 시도 시 즉시 거부되어야 합니다. 참고: [https://en.wikipedia.org/wiki/Top-level_domain#Reserved_domains](https://en.wikipedia.org/wiki/Top-level_domain#Reserved_domains)

mDNS용 `.local` 호스트네임은 atproto에서 사용되어서는 안 됩니다.

`.onion` 최상위 도메인은 Tor 프로토콜의 히든 서비스에 해당합니다. Tor를 통한 핸들 해석은 생태계 전체의 지원이 필요하므로, 현재는 허용되지 않습니다.

즉, 초기의 금지된 TLD 목록은 다음과 같습니다:

- `.alt`
- `.arpa`
- `.example`
- `.internal`
- `.invalid`
- `.local`
- `.localhost`
- `.onion`

`.test` TLD는 예제, 테스트, 개발용으로 의도되어 있습니다. atproto 개발에서는 사용될 수 있으나, 실제 환경에서는 실패해야 합니다.

특별한 `handle.invalid` 값에만 `.invalid` TLD가 사용되어야 합니다. 이 값은 Lexicon 스키마 상으로는 문법적으로 유효하나, 대부분의 상황에서 유효한 핸들로는 인정되지 않아야 합니다.

## 식별자 예시

문법적으로 유효한 핸들 (실제 존재하는 TLD 여부와는 무관):

```
jay.bsky.social
8.cn
name.t--t        // 실제 TLD는 아니지만 문법상 문제 없음
XX.LCS.MIT.EDU
a.co
xn--notarealidn.com
xn--fiqa61au8b7zsevnm8ak20mc4a87e.xn--fiqs8s
xn--ls8h.test
example.t        // 실제 TLD는 아니지만 문법상 문제 없음
```

문법적으로 유효하지 않은 예:

```
jo@hn.test
💩.test
john..test
xn--bcher-.tld
john.0
cn.8
www.masełkowski.pl.com
org
name.org.
```

문법은 유효하지만, 다른 제한사항에 의해 항상 해석에 실패해야 하는 예:

```
2gzyxa5ihm7nsggfxnu52rck2vv4rvmdlkiu3zzui5du4xyclen53wid.onion
laptop.local
blah.arpa
```

## 핸들 해석

핸들은 atproto 내에서 제한적인 역할을 하며, 거의 모든 상황에서 DID로 해석되어야 합니다. 해석 메커니즘은 해당 도메인 이름에 대해 어느 정도의 권한을 증명할 수 있어야 하며, 효율적으로 조회 가능해야 합니다. 현재 두 가지 해석 메커니즘이 지원됩니다. 하나는 DID를 포함하는 DNS TXT 레코드를 이용하는 방법이며, 또 다른 하나는 HTTPS의 특별한 `/.well-known/` URL을 이용하는 방법입니다.

클라이언트는 일반적으로 `com.atproto.identity.resolveHandle` 엔드포인트를 사용하여 네트워크 서비스(예, PDS)를 통해 핸들을 해석하며, 직접 해석 로직을 구현할 필요는 없습니다.

DNS TXT 방식은 개별 핸들 설정에 권장되고 선호되는 해석 방법이지만, 서비스는 두 가지 방법 모두를 완벽히 지원해야 합니다. HTTPS 방식은 대규모 웹 서비스와 같이 수천, 수백만 개의 DNS TXT 레코드를 자동으로 등록하기 어려운 경우를 위한 것입니다.

핸들이 DID로 해석되고, 현재 DID 문서가 핸들과의 연결을 확인하기 전까지는 해당 핸들을 신뢰하거나 유효한 것으로 간주해서는 안 됩니다. 핸들과 DID 간의 연결은 양방향으로 확인되어야 하며, 그렇지 않으면 누구나 타인의 계정에 대해 핸들 별칭을 생성할 수 있게 됩니다.

### DNS TXT 방식

이 방식에서는 핸들 호스트네임의 `_atproto` 서브도메인에 DNS TXT 레코드를 등록합니다. 레코드 값은 `did=` 접두사를 포함한 후 전체 DID를 기술해야 합니다. 이 방식은 [RFC-1464](https://www.rfc-editor.org/rfc/rfc1464.html) ("임의의 문자열 속성을 저장하기 위해 도메인 네임 시스템 사용")과 일치합니다.

예를 들어, 핸들 `bsky.app`의 경우, `_atproto.bsky.app`에 TXT 레코드가 등록되며, 레코드 값은 다음과 같이 구성됩니다: `did=did:plc:z72i7hdynmk6r22z27h6tvur`.

`did=`로 시작하지 않는 TXT 레코드는 무시되어야 합니다. 한 시점에 유효한 레코드는 하나만 존재해야 하며, 만약 서로 다른 DID를 가진 여러 유효 레코드가 존재한다면 해석은 실패해야 합니다. 이 경우, 일정 시간 후 재시도하거나 재귀적 해석기를 사용해야 합니다.

추가로, 매우 긴 핸들의 경우, `_atproto.` 접두사가 전체 도메인 길이를 253자를 초과하게 만들어 DNS TXT 방식으로는 해석할 수 없습니다. 이 경우 HTTPS 방식은 정상 작동합니다.

DNSSEC는 필수가 아닙니다.

### HTTPS well-known 방식

이 방식에서는 핸들 도메인에 위치한 웹 서버가 `/.well-known/atproto-did` 경로에 특별한 엔드포인트를 구현합니다. 유효한 HTTP 응답은 2xx 상태 코드와 `Content-Type` 헤더가 `text/plain`으로 설정되며, 본문에 DID가 추가 포맷팅 없이 포함되어야 합니다.

예를 들어, 핸들 `bsky.app`의 경우, `https://bsky.app/.well-known/atproto-did`에 GET 요청을 보내 해석할 수 있으며, 유효한 응답은 다음과 같아야 합니다:

```
HTTP/1.1 200 OK
Content-Length: 33
Content-Type: text/plain
Date: Wed, 14 Jun 2023 00:47:21 GMT

did:plc:z72i7hdynmk6r22z27h6tvur
```

`Content-Type` 헤더의 엄격한 확인은 필요하지 않습니다.

응답 본문의 앞뒤 공백은 제거한 후 DID로 파싱하는 것이 허용됩니다.

실제 환경에서 핸들을 해석할 때는 기본 포트(443)의 HTTPS를 사용해야 하며, HTTP는 로컬 개발 및 테스트용으로만 사용해야 합니다.

HTTP 리다이렉트(예, 301, 302)도 허용되며, 합리적인 리다이렉트 홉 수 내에서는 유효합니다.

### 유효하지 않은 핸들

특정 DID에 대해 핸들이 더 이상 해석되지 않는 것으로 확인되면, 해당 핸들은 유효하지 않은 것으로 표시되어야 합니다. API 응답에서는 특별한 핸들 값인 `handle.invalid`가 주어져, 해당 DID에 대해 양방향으로 유효한 핸들이 없음을 나타냅니다. 이 핸들은 대부분의 상황(검색 쿼리, API 요청 등)에서 사용될 수 없습니다.

### 해석 권장 모범 사례

두 가지 해석 방식을 병렬로 시도하고, 먼저 성공한 결과를 사용하는 것이 좋습니다. 만약 두 방식이 상충하는 결과(즉, 서로 다른 DID)를 반환한다면, DNS TXT 방식의 결과를 우선시해야 합니다. 물론, 모호한 결과로 기록하고 나중에 재시도하는 것도 허용됩니다.

서비스 내부적으로 핸들 해석 결과를 일정 기간 캐시하고 주기적으로 재해석하는 것이 권장됩니다. DNS TTL 값이 캐시 수명으로 활용될 수 있으나, 일반적으로 핸들 해석의 경우에는 너무 짧은 수명으로 간주됩니다.

해석 요청은 신뢰할 수 있는 네트워크 환경과 설정에서 수행하는 것이 좋으며, 여러 지역과 환경에서 요청을 분산시키면(예: 재귀적 DNS 해석기 사용) 트래픽 조작이나 의도적인 응답 분할 문제를 완화할 수 있습니다.

## 사용 및 구현 가이드라인

핸들은 사용자 인터페이스에서 "@" 기호(예: `@jay.bsky.team`)를 접두사로 붙여 표시될 수 있으나, 이는 레코드, API, 백엔드 등에서는 유효한 문법이 아닙니다.

국제화 도메인 이름(IDN, 또는 "punycode")은 저수준 핸들 문법과 직접적인 관련이 없습니다. IDN의 인코딩된 형태는 이미 유효한 호스트네임(따라서 핸들)이며, 이런 핸들은 반드시 ASCII 인코딩 형태로 저장되고 전송되어야 합니다. IDN처럼 보이나 유효한 IDN으로 파싱되지 않는 핸들도, 유효한 호스트네임이라면 핸들로서 허용됩니다. 애플리케이션은 필요에 따라 IDN 핸들을 유니코드로 파싱하여 표시할 수 있습니다.

핸들은 대소문자를 구분하지 않으므로, 사용자 입력을 소문자(ASCII)로 정규화하는 것이 안전합니다. 소문자로 정규화된 핸들만 레코드에 저장되거나 아웃바운드 API 호출에 사용되어야 하며, 사용자 입력 그대로의 대소문자 정보를 보존해 표시하는 것은 피해야 합니다. 예를 들어, 사용자가 입력한 `BlueskyWeb.xyz`는 반드시 `blueskyweb.xyz`로 정규화되어 저장 및 표시되어야 합니다. 단, 길이가 긴 전부 소문자인 핸들은 가독성과 접근성에 도전이 될 수 있으므로, 세그먼트 구분(마침표), 하이픈 사용, 또는 애플리케이션 프로토콜 내 "디스플레이 이름" 사용 등이 고려될 수 있습니다.

매우 긴 핸들은 사용자 인터페이스 상에서 도전적일 수 있으나, 프로토콜 상에서는 허용되며 애플리케이션 개발자는 이를 지원해야 합니다.

유명 도메인과 유사하게 보이는 핸들은 보안 및 사칭 문제를 일으킬 수 있습니다. 예를 들어, `paypa1.com` 또는 `paypal.cc`와 같이 `paypal.com`과 혼동될 수 있는 핸들 또는 시작이나 끝이 잘려서 `paypal.com…`으로 표시되는 경우 등이 있습니다.

핸들은 지역적 맥락에 맞게 축약되어 표시되어서는 안 됩니다. 예를 들어, `@jay.bsky.social` 핸들이 지역적 맥락에서 단순히 `@jay`로 표시되어서는 안 됩니다.

핸들 "네임스페이스"를 제공하는 업체(예: 등록된 도메인의 서브도메인)는 추가적인 제한을 둘 수 있습니다. 일반적으로 각 세그먼트 길이를 합리적인 범위로 제한하고, `www`, `admin`, `mail` 등과 같이 흔히 사용되는 문자열은 예약하는 것이 권장됩니다.

실제로 핸들은 DNS 이름으로 허용되는 253자보다 짧은, 최대 244자로 제한하는 것이 좋습니다. 이는 DNS 검증 시 `_atproto.` 접두사가 추가되어 전체 길이가 제한을 초과할 수 있기 때문입니다.

핸들 호스트네임은 주류 DNS 도메인 이름이어야 하며, 비표준 TLD나 비표준 네이밍 시스템을 사용하는 핸들은 atproto 생태계 내 다른 네트워크 서비스나 프로토콜 구현과 상호 운용되지 않을 수 있습니다.

PDS(개인 데이터 서버)는 계정을 호스팅하면서 계정의 핸들이 더 이상 검증되지 않을 경우(즉, `handle.invalid` 상황) repo 변경을 막을 수 있습니다. 다른 네트워크 서비스는 콘텐츠 단절을 막기 위해 계속해서 해당 콘텐츠를 표시하되, 상황에 따라 경고나 주석을 추가할 수 있습니다.

## 향후 변경 가능성

핸들 문법은 비교적 안정적입니다.

미래에는 `.onion` 핸들이 허용될 가능성도 있습니다.


---
atproto/src/app/[locale]/specs/handle/page.tsx
---
export const metadata = {
  title: 'Handle',
  description: 'A specification for human-friendly account identifiers.',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/blob/en.mdx
---
export const metadata = {
  title: 'Blobs',
  description:
    'Media files referenced by records',
}

# Blobs

"Blobs" are media files stored alongside an account's repository. They include images, video, and audio, but could also include any other file format. Blobs are referenced by individual records by the `blob` lexicon datatype, which includes a content hash (CID) for the blob.

Blob files are uploaded and distributed separately from records. Blobs are authoritatively stored by the account's PDS instance, but views are commonly served by CDNs associated with individual applications ("AppViews"), to reduce traffic on the PDS. CDNs may serve transformed (resized, transcoded, etc) versions of the original blob.

While blobs are universally content addressed (by CID), they are always referenced and managed in the context of an individual account (DID).

The empty blob (zero bytes) is valid in general, though it may be disallowed by individual Lexicons/applications.

## Blob Metadata

Currently, the only "blessed" CID type for blobs is similar to that for repository records, but with the `raw` multicodec:

- CIDv1
- Multibase: binary serialization within DAG-CBOR (or `base32` for JSON mappings)
- Multicodec: `raw` (0x55)
- Multihash: `sha-256` with 256 bits (0x12)

An example blob CID, in base32 string encoding: `bafkreibjfgx2gprinfvicegelk5kosd6y2frmqpqzwqkg7usac74l3t2v4`

Blob metadata also includes the size of the blob (in bytes), and the MIME type. The size and CID are deterministic and must be valid and consistent. The MIME type is somewhat more subjective: it is possible for the same bytes to be valid for multiple MIME types.

## Blob Lifecycle

Blobs must be uploaded to the PDS before a record can be created referencing that blob. Note that the server does not know the intended Lexicon when receiving an upload, so can only apply generic blob limits and restrictions at initial upload time, and then enforce Lexicon-defined limits later when the record is created.

Clients use the `com.atproto.repo.uploadBlob` endpoint on their PDS, which will return verified metadata in the form of a Lexicon blob object. Clients "should" set the HTTP `Content-Type` header and "should" set the  `Content-Length` headers on the upload request. Chunked transfer encoding may also be permitted for uploads. Servers may sniff the blob mimetype to validate against the declared `Content-Type` header, and either return a modified mimetype in the response, or reject the upload. See "Security Considerations" below. If the actual blob upload size differs from the `Content-Length` header, the server should reject the upload.

After a successful upload, blobs are placed in temporary storage. They are not accessible for download or distribution while in this state. Servers should "garbage collect" (delete) un-referenced temporary blobs after an appropriate time span (see implementation guidelines below). Blobs which are in temporary storage should not be included in the `listBlobs` output.

The upload blob can now be referenced from records by including the returned blob metadata in a record. When processing record creation, the server extracts the set of all referenced blobs, and checks that they are either already referenced, or are in temporary storage. Once the record creation succeeds, the server makes the blob publicly accessible.

The same blob can be referenced by multiple records in the same repository. Re-uploading a blob which has already been stored and referenced results in no change to the existing blobs or records.

When a record referencing blobs is deleted, the server checks if any other current records from the same repository reference the blob. If not, the blob is deleted along with the record.

When an account is deleted, all the hosted blobs are deleted, within some reasonable time frame. When an account is deactivated, takendown, or suspended, blobs should not be publicly accessible.

Servers may decide to make individual blobs inaccessible, separately from any account takedown or other account lifecycle events.

Creation of new individual records which reference a blob which does not exist should be rejected at the time of creation (or update). However, it is possible for servers to host repository records which reference  blobs which are not available locally. For example, during a bulk repository import or account migration; data loss; or content deletion/removal for policy reasons.

Original blobs can be fetched from the PDS using the `com.atproto.sync.getBlob` endpoint. The server should return appropriate `Content-Type` and `Content-Length` HTTP headers. It is not a recommended or required pattern to serve media directly from the PDS to end-user browsers, and servers do not need to support or facilitate this use case. See "Security Considerations" below for more.

## Usage and Implementation Guidelines

Servers may have their own generic limits and policies for blobs, separate from any Lexicon-defined constraints. They might implement account-wide quotas on data storage; maximum blob sizes; content policies; etc. Any of these restrictions might be enforced at the initial upload. Server operators should be aware that limits and other restrictions may impact functionality with existing and future applications. To maximize interoperability, operators are recommended to prefer limits on overall account resource consumption (eg, "total blob size" quota, not "per blob" size limits).

Some applications may have a long delay between blob upload and reference from a record. To maximize interoperability, server implementations and operators are recommended to allow several hours of grace time before "garbage collecting", with at least one hour a firm lower bound.

## Security Considerations

Serving arbitrary user-uploaded files from a web server raises many content security issues. For example, cross-site scripting (XSS) of scripts or SVG content form the same "origin" as other web pages. It is effectively mandatory to enable a Content Security Policy (LINK: https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP) for the `getBlob` endpoint. It is effectively not supported to dynamically serve assets directly out of blob storage (the `getBlob` endpoint) directly to browsers and web applications. Applications must proxy blobs, files, and assets through an independent CDN, proxy, or other web service before serving to browsers and web agents, and such services are expected to implement security precautions.

An example set of content security headers for this endpoint is:

```
Content-Security-Policy: default-src 'none'; sandbox
X-Content-Type-Options: nosniff
```

Some media types may contain sensitive metadata. For example, EXIF metadata in JPEG image files may contain GPS coordinates. Servers might take steps to prevent accidental leakage of such metadata, for example by blocking upload of blobs containing them. See note in "Future Changes" section.

Parsing of media files is a notorious source of memory safety bugs and security vulnerabilities. Even content type detection (or "sniffing") can be a source of exploits. Servers are strongly recommended against parsing media files (image, video, audio, or any other non-trivial formats) directly, without the use of strong sandboxing mechanisms. In particular, PDS instances themselves should not directly implement media resizing or transcoding.

Richer media types raise the stakes for abusive and illegal content. Services should implement appropriate mechanisms to takedown such content when it is detected and reported.

Servers may need to take measures to prevent malicious resource consumption. For example, intentional exhaustion of disk space, network congestion, bandwidth utilization, etc. Rate-limits, size limits, and quotas are recommended.

## Possible Future Changes

The allowed CID type is expected to evolve over time. There has been interest in `blake3` for larger file types.

More specific mitigation of metadata leakage (eg, EXIF metadata stripping) should be recommended and/or enabled via API changes. There is a tension between providing default safety, and always intervening to manipulate "original" uploaded user data. Additionally, parsing and manipulating uploaded media files raises other categories of security concerns.


---
atproto/src/app/[locale]/specs/blob/ko.mdx
---
export const metadata = {
  title: '블롭',
  description: '레코드가 참조하는 미디어 파일',
}

# 블롭

"블롭"은 계정의 레포지토리와 함께 저장되는 미디어 파일입니다. 이들은 이미지, 비디오, 오디오 등을 포함하지만, 다른 파일 형식도 포함할 수 있습니다. 블롭은 각 레코드에서 `blob` 렉시콘 데이터 유형으로 참조되며, 여기에는 블롭의 콘텐츠 해시(CID)가 포함됩니다.

블롭 파일은 레코드와 별도로 업로드 및 배포됩니다. 블롭은 계정의 PDS 인스턴스에 의해 권위적으로 저장되지만, 보통 애플리케이션(AppViews)과 연관된 CDN을 통해 뷰가 제공되어 PDS의 트래픽을 줄입니다. CDN은 원본 블롭의 변환된(크기 조정, 트랜스코딩 등) 버전을 제공할 수 있습니다.

블롭은 보편적으로 콘텐츠 주소 지정(CID)되지만, 항상 개별 계정(DID)의 컨텍스트 내에서 참조 및 관리됩니다.

빈 블롭(0 바이트)은 일반적으로 유효하지만, 일부 렉시콘이나 애플리케이션에서는 허용되지 않을 수 있습니다.

## 블롭 메타데이터

현재 블롭에 대해 "인증된" CID 유형은 레포지토리 레코드와 유사하지만 `raw` 멀티코덱을 사용합니다:

- CIDv1
- 멀티베이스: DAG-CBOR 내의 바이너리 직렬화 (또는 JSON 매핑의 경우 `base32`)
- 멀티코덱: `raw` (0x55)
- 멀티해시: 256비트 `sha-256` (0x12)

예시 블롭 CID (base32 문자열 인코딩): `bafkreibjfgx2gprinfvicegelk5kosd6y2frmqpqzwqkg7usac74l3t2v4`

블롭 메타데이터는 또한 블롭의 크기(바이트 단위)와 MIME 타입을 포함합니다. 크기와 CID는 결정적이며 유효하고 일관되어야 합니다. MIME 타입은 다소 주관적입니다: 동일한 바이트들이 여러 MIME 타입에 대해 유효할 수 있습니다.

## 블롭 수명 주기

레코드가 해당 블롭을 참조하기 전에 블롭은 반드시 PDS에 업로드되어야 합니다. 서버는 업로드 시 의도된 렉시콘을 알 수 없으므로, 초기 업로드 시에는 일반적인 블롭 제한 및 제약만 적용할 수 있고, 이후 레코드 생성 시 렉시콘에서 정의한 제한을 적용할 수 있습니다.

클라이언트는 PDS의 `com.atproto.repo.uploadBlob` 엔드포인트를 사용하며, 이 엔드포인트는 렉시콘 블롭 객체의 형태로 검증된 메타데이터를 반환합니다. 클라이언트는 HTTP `Content-Type` 헤더와 `Content-Length` 헤더를 업로드 요청에 "설정해야" 합니다. 청크 전송 인코딩도 업로드에 허용될 수 있습니다. 서버는 업로드된 블롭의 MIME 타입을 감지하여 선언된 `Content-Type` 헤더와 비교 검증할 수 있으며, 수정된 MIME 타입을 응답으로 반환하거나 업로드를 거부할 수 있습니다. 실제 업로드된 블롭의 크기가 `Content-Length` 헤더와 다를 경우, 서버는 업로드를 거부해야 합니다.

업로드가 성공하면, 블롭은 임시 저장소에 배치됩니다. 이 상태의 블롭은 다운로드나 배포가 불가능합니다. 서버는 임시 상태의 참조되지 않은 블롭을 적절한 시간 경과 후에 "가비지 컬렉션"(삭제) 해야 합니다 (아래 구현 지침 참조). 임시 저장소에 있는 블롭은 `listBlobs` 출력에 포함되어서는 안 됩니다.

이제 업로드된 블롭은 반환된 블롭 메타데이터를 레코드에 포함하여 참조할 수 있습니다. 레코드 생성 처리 시, 서버는 참조된 모든 블롭들을 추출하고, 해당 블롭들이 이미 참조되고 있거나 임시 저장소에 있는지 확인합니다. 레코드 생성이 성공하면, 서버는 블롭을 공개적으로 접근 가능하게 만듭니다.

동일한 블롭은 같은 레포지토리 내 여러 레코드에서 참조될 수 있습니다. 이미 저장되어 참조되고 있는 블롭을 다시 업로드해도 기존의 블롭이나 레코드에는 변화가 없습니다.

블롭을 참조하는 레코드가 삭제되면, 서버는 같은 레포지토리 내 다른 현재 레코드가 해당 블롭을 참조하는지 확인합니다. 참조하는 레코드가 없다면, 해당 블롭은 레코드와 함께 삭제됩니다.

계정이 삭제되면, 모든 호스팅된 블롭은 합리적인 시간 내에 삭제됩니다. 계정이 비활성화되거나 중단, 정지된 경우, 블롭은 공개적으로 접근할 수 없어야 합니다.

서버는 개별 블롭을 계정의 중단이나 다른 계정 수명 주기 이벤트와 별개로 접근 불가능하게 만들 수도 있습니다.

존재하지 않는 블롭을 참조하는 새로운 레코드 생성은 생성(또는 업데이트) 시 거부되어야 합니다. 그러나, 서버가 로컬에서 사용 가능한 블롭이 아닌 레포지토리 레코드를 호스팅할 가능성은 있습니다. 예를 들어, 대량 레포지토리 가져오기나 계정 마이그레이션, 데이터 손실 또는 정책상의 이유로 콘텐츠가 삭제/제거되는 경우가 이에 해당합니다.

원본 블롭은 `com.atproto.sync.getBlob` 엔드포인트를 통해 PDS에서 가져올 수 있습니다. 서버는 적절한 `Content-Type` 및 `Content-Length` HTTP 헤더를 반환해야 합니다. 미디어 파일을 PDS에서 직접 브라우저로 제공하는 것은 권장되거나 요구되는 패턴이 아니므로, 서버는 이를 지원할 필요가 없습니다. (아래 "보안 고려 사항" 참조)

## 사용 및 구현 지침

서버는 렉시콘에서 정의한 제약과는 별개로 블롭에 대해 자체적인 일반 제한 및 정책을 가질 수 있습니다. 예를 들어, 계정 전체의 데이터 저장소 쿼터, 최대 블롭 크기, 콘텐츠 정책 등을 구현할 수 있으며, 이러한 제한은 초기 업로드 시 적용될 수 있습니다. 운영자는 제한 및 기타 제약 사항이 기존 및 향후 애플리케이션의 기능에 영향을 미칠 수 있음을 인지해야 합니다. 상호 운용성을 극대화하기 위해, 운영자는 개별 블롭 크기 제한보다는 계정 전체의 리소스 소비(예: "전체 블롭 크기" 쿼터)에 대한 제한을 선호하는 것이 좋습니다.

일부 애플리케이션은 블롭 업로드와 레코드 참조 사이에 긴 지연 시간이 있을 수 있습니다. 상호 운용성을 극대화하기 위해, 서버 구현 및 운영자는 "가비지 컬렉션"을 최소 한 시간 이상의 확실한 시간 후, 몇 시간의 유예 기간을 두고 수행하는 것이 좋습니다.

## 보안 고려 사항

사용자가 업로드한 임의의 파일을 웹 서버에서 제공하는 것은 다양한 콘텐츠 보안 문제를 야기할 수 있습니다. 예를 들어, 스크립트나 SVG 콘텐츠에서의 교차 사이트 스크립팅(XSS)이 동일 출처 정책에 위배될 수 있습니다. `getBlob` 엔드포인트에는 Content Security Policy(CSP, [MDN 문서](https://developer.mozilla.org/ko/docs/Web/HTTP/CSP))를 활성화하는 것이 필수적입니다. PDS에서 직접 블롭을 브라우저나 웹 애플리케이션에 동적으로 제공하는 것은 실질적으로 지원되지 않습니다. 애플리케이션은 블롭, 파일, 자산을 독립적인 CDN, 프록시 또는 기타 웹 서비스로 중계하여 제공해야 하며, 이러한 서비스는 보안 예방 조치를 구현해야 합니다.

예시로, 이 엔드포인트에 적합한 콘텐츠 보안 헤더는 다음과 같습니다:

```
Content-Security-Policy: default-src 'none'; sandbox
X-Content-Type-Options: nosniff
```

일부 미디어 타입은 민감한 메타데이터를 포함할 수 있습니다. 예를 들어, JPEG 이미지 파일의 EXIF 메타데이터에는 GPS 좌표가 포함될 수 있습니다. 서버는 이러한 메타데이터의 우발적 누출을 방지하기 위해, 예를 들어 메타데이터를 포함하는 블롭의 업로드를 차단하는 등의 조치를 취할 수 있습니다. (자세한 내용은 "향후 변경 사항" 섹션의 주석을 참조하십시오.)

미디어 파일 파싱은 메모리 안전 버그 및 보안 취약점의 주요 원인이 될 수 있습니다. 콘텐츠 타입 감지(스니핑) 또한 취약점의 원인이 될 수 있으므로, 서버는 강력한 샌드박싱 메커니즘 없이 미디어 파일(이미지, 비디오, 오디오 또는 기타 복잡한 형식)을 직접 파싱하는 것을 강력히 권장하지 않습니다. 특히, PDS 인스턴스 자체가 미디어 크기 조정이나 트랜스코딩을 직접 구현해서는 안 됩니다.

더 복잡한 미디어 타입은 악의적이거나 불법적인 콘텐츠의 위험을 증가시킵니다. 서버는 그러한 콘텐츠가 감지되고 신고될 경우 적절한 제거 메커니즘을 구현해야 합니다.

서버는 악의적인 리소스 소비를 방지하기 위한 조치를 취해야 할 수도 있습니다. 예를 들어, 의도적인 디스크 공간 고갈, 네트워크 혼잡, 대역폭 사용 등이 이에 해당합니다. 속도 제한, 크기 제한 및 쿼터 적용이 권장됩니다.

## 향후 변경 사항

허용된 CID 유형은 시간이 지남에 따라 진화할 것으로 예상됩니다. 대용량 파일에 대해 `blake3`에 대한 관심이 있습니다.

EXIF 메타데이터 제거와 같이 메타데이터 누출을 보다 구체적으로 완화하는 방안이 API 변경을 통해 권장되거나 활성화되어야 합니다. 이는 기본적인 안전성을 제공하는 것과 사용자가 업로드한 "원본" 데이터를 항상 변경하지 않고 제공하는 것 사이의 긴장을 야기합니다. 또한, 업로드된 미디어 파일을 파싱하고 조작하는 것은 다른 종류의 보안 문제를 발생시킬 수 있습니다.

---
atproto/src/app/[locale]/specs/blob/page.tsx
---
export const metadata = {
  title: 'Blobs',
  description: 'Media files referenced by records',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/xrpc/en.mdx
---
export const metadata = {
  title: 'HTTP API (XRPC)',
  description:
    'Cross-system queries and procedures over HTTP',
}

# HTTP API (XRPC)

HTTP APIs for client-server and server-server requests in atproto use a set of common conventions called XRPC. Endpoint path names include an NSID indicating the [Lexicon](/specs/lexicon) specifying the request and response schemas, usually with JSON content type. {{ className: 'lead' }}

## Lexicon HTTP Endpoints

The HTTP request path starts with `/xrpc/`, followed by an NSID. Paths must always be top-level, not below a prefix. The NSID maps to the `id` field in the associated Lexicon.

The two requests types that can be expressed in Lexicons are "query" (HTTP GET) and "procedure" (HTTP POST). Following HTTP REST semantics, queries (GET) are cacheable and should not mutate resource state. Procedures are not cacheable and may mutate state.

Lexicon `params` (under the `parameters` field) map to HTTP URL query parameters. Only certain Lexicon types can be included in params, as specified by the `params` type. Multiple query parameters with the same name can be used to represent an array of parameters. When encoding `boolean` parameters, the strings `true` and `false` should be used. Strings should not be quoted. If a `default` value is included in the schema, it should be included in every request to ensure consistent caching behavior.

Request and response body content types can be specified in Lexicon. The schema can require an exact MIME type, or a blob pattern indicating a range of acceptable types (eg, `image/*`).

JSON body schemas are specified in Lexicon using the usual atproto data model. Full validation by the server or client requires knowledge of the Lexicon, but partial validation against the abstract data model is always possible.

CORS support is encouraged but not required.

### Error Responses

All unsuccessful responses should follow a standard error response schema. The `Content-Type` should be `application/json`, and the payload should be a JSON object with the following fields:

- `error` (string, required): type name of the error (generic ASCII constant, no whitespace)
- `message` (string, optional): description of the error, appropriate for display to humans

The error type should map to an error name defined in the endpoint's Lexicon schema. This enables more specific error-handling by client software. This is particularly encouraged on `400`, `500`, and `502` status codes, where further information will be useful.

### Blob Upload and Download

Blobs are something of a special case because they can have any MIME type and are not stored directly in repositories, and thus are not directly associated with an NSID or Lexicon (though they do end up referenced from Lexicons).

The convention for working with blobs is for clients to upload them via the `com.atproto.repo.uploadBlob` endpoint, which returns a `blob` JSON object containing a CID and basic metadata about the blob. Client can then include this `blob` data in future requests (eg, include in new records). Constraints like MIME type and file size are only validated at this second step. The server may implement content type sniffing at the upload step and return a MIME type different from any `Content-Type` header provided, but a `Content-Type` header is still expected on the upload HTTP request.

Blobs for a specific account can be listed and downloaded using endpoints in the `com.atproto.sync.*` NSID space. These endpoints give access to the complete original blob, as uploaded. A common pattern is for applications to mirror both the original blob and any downsized thumbnail or preview versions via separate URLs (eg, on a CDN), instead of deep-linking to the `getBlob` endpoint on the original PDS.

### Cursors and Pagination

A common pattern in Lexicon design is to include a `cursor` parameter for pagination. The client should not include the `cursor` parameter in the first request, and should keep all other parameters fixed between requests. If a cursor is included in a response, the next batch of responses can be fetched by including that value in a follow-on, continuing until the cursor is not included any longer, indicating the end of the result set has been reached.

## Authentication

The primary client/server authentication and authorization scheme for XRPC is OAuth, described in the [Auth Specification](./oauth).

Not all endpoints require authentication, but there is not yet a consistent way to enumerate which endpoints do or do not.

There is also a legacy authentication scheme using HTTP Bearer auth with JWT tokens, including refresh tokens, described here. Initial login uses the `com.atproto.server.createSession` endpoint, including the password and an account identifier (eg, handle or registered email address). This returns a `refreshJwt` (as well as an initial `accessJwt`).

Most requests should be authenticated using an access JWT, but the validity lifetime for these tokens is short. Every couple minutes, a new access JWT can be requested by hitting the `com.atproto.server.refreshSession` endpoint, using the refresh JWT instead of an access JWT.

Clients should treat the tokens as opaque string tokens: the JWT fields and semantics are not a stable part of the specification.

Servers (eg, PDS implementations) which generate and valiate auth JWTs should implement domain separation between access and refresh tokens, using the `typ` header field: access tokens should use `at+jwt`, and refresh tokens should use `refresh+jwt`. Note that `at+jwt` (defined in [RFC 9068](https://www.rfc-editor.org/rfc/rfc9068.html)) is short for "access token", and is not a reference to the "at" in atproto.

### App Passwords

App Passwords are a mechanism to reduce security risks when logging in to third-party clients and web applications. Accounts can create and revoke app passwords separate from their primary password. They are used to log in the same way as the primary password, but grant slightly restricted permissions to the client application, preventing destructive actions like account or changes to authentication settings (including app passwords themselves).

Clients and apps themselves do not need to do anything special to use app passwords. It is a best practice for most clients and apps to include a reminder to use an app password when logging in. App passwords usually have the form `xxxx-xxxx-xxxx-xxxx`, and clients can check against this format to prevent accidental logins with primary passwords (unless the primary password itself has this format).

### Admin Token (Temporary Specification)

Some administrative XRPC endpoints require authentication with admin privileges. The current scheme for this is to use HTTP Basic authentication with user "admin" and a fixed token in the password field, instead of HTTP Bearer auth with a JWT. This means that admin requests do not have a link to the account or identity of the client beyond "admin".

As a reminder, HTTP Basic authentication works by joining the username and password together with a colon (`:`), and encoding the resulting string using `base64` ("standard" version). The encoded string is included in the `Authorization` header, prefixed with `Basic ` (with separating space).

As an example, if the admin token was `secret-token`, the header would look like:

```
Authorization: Basic YWRtaW46c2VjcmV0LXRva2Vu
```

The set of endpoints requiring admin auth is likely to get out of date in this specification, but currently includes:

- `com.atproto.admin.*`
- `com.atproto.server.createInviteCode`
- `com.atproto.server.createInviteCodes`

### Inter-Service Authentication (JWT)

This section describes a mechanism for authentication between services using signed JWTs.

The current mechanism is to use short-lived JWTs signed by the account's atproto signing key. The receiving service can validate the signature by checking this key against the account's DID document.

The JWT parameters are:

- `alg` header field (string, required): indicates the signing key type (see [Cryptography](/specs/cryptography))
    - use `ES256K` for `k256` keys
    - use `ES256` for `p256` keys
- `typ` header field (string, required): currently `JWT`, but intend to update to a more specific value.
- `iss` body field (string, required): account DID that the request is being sent on behalf of. This may include a suffix service identifier; see below
- `aud` body field (string, required): service DID associated with the service that the request is being sent to
- `exp` body field (number, required): token expiration time, as a UNIX timestamp with seconds precision. Should be a short time window, as revocation is not implemented. 60 seconds is a good token lifetime.
- `iat` body field (number, required): token creation time, as a UNIX timestamp with seconds precision.
- `lxm` body field (string, optional): short for "lexicon method". NSID syntax. Indicates the endpoint that this token authorizes. Servers must always validate this field if included, and should require it for security-sensitive operations. May become required in the future.
- `jti` body field (string, required): unique random string nonce. May be used by receiving services to prevent reuse of token and replay attacks.
- JWT signature (string, required): base64url-encoded signature using the account DID's signing key.

When the token is generated in the context of a specific service in the issuer's DID document, the issuer field may have the corresponding *service* identifier in the `iss` field, separated by a `#` character. For example, `did:web:label.example.com#atproto_labeler` for a labeler service. When this is included the appropriate signing key is determined based on a fixed mapping of service identifiers to key identifiers:

- service identifier `atproto_labeler` maps to key identifier `atproto_label`

If the service identifier is not included, the scope is general purpose and the `atproto` key identifier should be used.

The receiving service may require or prohibit specific service identifiers for access to specific resources or endpoints.

The signature is computed using the regular JWT process, using the account's signing key (the same used to sign repo commits). As Typescript pseudo-code, this looks like:

```
const headerPayload = utf8ToBase64Url(jsonStringify(header)) + '.' + utf8ToBase64Url(jsonString(body))
const signature = hashAndSign(accountSigningKey, utf8Bytes(headerPayload))
const jwt = headerPayload + '.' + bytesToBase64Url(signature)
```

## Service Proxying

The PDS acts as a generic proxy between clients and other atproto services. Clients can use an HTTP header to specify which service in the network they want the request forwarded to (eg, a specific AppView or Labeler service). The PDS will do some safety checks, then forward the request on with an inter-service authentication token (JWT, described above) issued and signed by the user's identity.

The HTTP header is `atproto-proxy`, and the value is a DID (identifying a service), followed by a service endpoint identifier, joined by a `#` character. The PDS resolves the service DID, extracts a service endpoint URL from the DID document, and proxies the request on to the identified server.

An example request header, to proxy to a labeling service, is:

```
atproto-proxy: did:web:labeler.example.com#atproto_labeler
```

A few requirements must be met for proxying to happen. These conditions may be extended in the future to address network abuse concerns.

- the target service must have a resolvable DID, a well-formed DID document, and a corresponding service entry with a matching identifier
- only atproto endpoint paths are supported. This means an `/xrpc/` prefix, followed by a valid NSID and endpoint name. Note that the `/xrpc/` prefix may become configurable in the future
- the request must be from an authenticated user with an active account on the PDS
- rate-limits at the PDS still apply


## Summary of HTTP Headers

Clients can use the following request headers:

`Content-Type`: If a request body is present, this header should be included and indicate the content type.

`Authorization`: Contains auth information. See "Authentication" section of this specification for details.

`atproto-proxy`: used for proxying to other atproto services. See "Service Proxying" section of this document for details.

`atproto-accept-labelers`: used by clients to request labels from specific labelers to be included and applied in the response. See [Label](/specs/label) specification for details.

## Summary of HTTP Status Codes

`200 OK`: The request was successful. If there is a response body (optional), there should be a `Content-Type` header.

`400 Bad Request`: Request was invalid, and was not processed

`401 Unauthorized`: Authentication is required for this endpoint. There should be a `WWW-Authenticate` header.

`403 Forbidden`: The client lacks permission for this endpoint

`404 Not Found`: Can indicate a missing resource. This can also indicate that the server does not support atproto, or does not support this endpoint. See response error message (or lack thereof) to clairfy.

`413 Payload Too Large`: Request body was too large. If possible, split in to multiple smaller requests.

`429 Too Many Requests`: A resource limit has been exceeded, client should back off. There may be a `Retry-After` header indicating a specific back-off time period.

`500 Internal Server Error`: Generic internal service error. Client may retry after a delay.

`501 Not Implemented`: The specified endpoint is known, but not implemented. Client should *not* retry. In particular, returned when WebSockets are requested by not implemented by server.

`502 Bad Gateway`, `503 Service Unavailable`, or `504 Gateway Timeout`: These all usually indicate temporary or permanent service downtime. Clients may retry after a delay.

## Usage and Implementation Guidelines

Clients are encouraged to implement timeouts, limited retries, and randomized exponential backoff. This increases robustness in the inevitable case of sporadic downtime, while minimizing load on struggling servers.

Servers *should* implement custom JSON error responses for all requests with an `/xrpc/` path prefix, but realistically, many services will return generic load-balancer or reverse-proxy HTML error pages. Clients should be robust to non-JSON error responses.

HTTP servers and client libraries usually limit the overall size of URLs, including query parameters, and these limits constrain the use of parameters in XRPC.

PDS implementations are free to restrict blob uploads as they see fit. For example, they may have a global maximum size or restricted set of allowed MIME types. These should be a superset of blob constraints for all supported Lexicons.

## Security and Privacy Considerations

Only HTTPS should be used over the open internet.

Care should be taken with personally identifiable information in blobs, such as EXIF metadata. It is currently the *client's* responsibility to strip any sensitive EXIF metadata from blobs before uploading. It would be reasonable for a PDS to help prevent accidental metadata leakage as well; see future changes section below.

## Possible Future Changes

The auth system is likely to be entirely overhauled.

Lexicons should be able to indicate whether auth is required.

The role of the PDS as a generic gateway may be formalized and extended. A generic mechanism for proxying specific XRPC endpoints on to other network services may be added. Generic caching of queries and blobs may be specified. Mutation of third-party responses by the PDS might be explicitly allowed.

An explicit decision about whether HTTP redirects are supported.

Cursor pagination behavior should be clarified when a cursor is returned but the result list is empty, and when a cursor value is repeated.

To help prevent accidental publishing of sensitive metadata embedded in media blobs, a query parameter may be added to the upload blob endpoint to opt-out of metadata stripping, and default to either blocking upload or auto-striping such metadata for all blobs.

The `lxm` JWT field for inter-service auth may become required.


---
atproto/src/app/[locale]/specs/xrpc/ko.mdx
---
export const metadata = {
  title: 'HTTP API (XRPC)',
  description: 'HTTP를 통한 크로스 시스템 쿼리 및 프로시저',
}

# HTTP API (XRPC)

atproto에서 클라이언트-서버 및 서버-서버 요청을 위한 HTTP API는 XRPC라 불리는 일련의 공통 규약을 사용합니다. 엔드포인트 경로 이름에는 요청 및 응답 스키마를 지정하는 [Lexicon](/specs/lexicon)이 나타내는 NSID가 포함되며, 일반적으로 JSON 콘텐츠 타입을 사용합니다. {{ className: 'lead' }}

## Lexicon HTTP 엔드포인트

HTTP 요청 경로는 `/xrpc/`로 시작하며, 그 뒤에 NSID가 이어집니다. 경로는 항상 최상위에 있어야 하며, 접두사 아래에 있어서는 안 됩니다. NSID는 관련 Lexicon의 `id` 필드에 매핑됩니다.

Lexicon에서 표현할 수 있는 두 가지 요청 유형은 "query"(HTTP GET)와 "procedure"(HTTP POST)입니다. HTTP REST 의미론에 따라, 쿼리(GET)는 캐시 가능하며 리소스 상태를 변경해서는 안 됩니다. 프로시저(POST)는 캐시되지 않으며 상태를 변경할 수 있습니다.

Lexicon의 `parameters` 필드 아래에 있는 `params`는 HTTP URL 쿼리 파라미터에 매핑됩니다. 단, 특정 Lexicon 타입만 params에 포함될 수 있으며, 이는 해당 `params` 타입에 명시되어 있습니다. 동일한 이름의 쿼리 파라미터가 여러 번 사용되면 파라미터 배열을 나타냅니다. `boolean` 파라미터를 인코딩할 때는 문자열 `true`와 `false`를 사용해야 하며, 문자열은 따옴표 없이 사용해야 합니다. 스키마에 `default` 값이 포함된 경우, 일관된 캐싱 동작을 위해 모든 요청에 반드시 포함되어야 합니다.

요청 및 응답 본문의 콘텐츠 타입은 Lexicon에서 지정할 수 있습니다. 스키마는 정확한 MIME 타입을 요구하거나, 허용 가능한 타입 범위를 나타내는 블롭 패턴(e.g., `image/*`)을 요구할 수 있습니다.

Lexicon의 JSON 본문 스키마는 일반적인 atproto 데이터 모델을 사용하여 지정됩니다. 서버나 클라이언트에서 완전한 검증을 하려면 Lexicon에 대한 지식이 필요하지만, 추상 데이터 모델에 대해서는 부분적인 검증이 항상 가능합니다.

CORS 지원은 권장되지만 필수 사항은 아닙니다.

### 오류 응답

모든 실패한 응답은 표준 오류 응답 스키마를 따라야 합니다. `Content-Type`은 `application/json`이어야 하며, 페이로드는 다음 필드를 포함하는 JSON 객체여야 합니다:

- `error` (string, 필수): 오류 타입 이름 (공백 없는 일반 ASCII 상수)
- `message` (string, 선택): 사용자에게 표시하기 적합한 오류 설명

오류 타입은 엔드포인트의 Lexicon 스키마에 정의된 오류 이름에 매핑되어야 합니다. 이를 통해 클라이언트 소프트웨어가 보다 구체적인 오류 처리를 할 수 있으며, 이는 특히 `400`, `500`, `502` 상태 코드에서 유용합니다.

### 블롭 업로드 및 다운로드

블롭은 MIME 타입에 구애받지 않고 다양한 데이터를 담을 수 있으며, 리포지토리에 직접 저장되지 않기 때문에 NSID나 Lexicon과 직접적으로 연관되지 않습니다(하지만 Lexicon에서 참조될 수 있습니다).

블롭을 다루는 규약은 클라이언트가 `com.atproto.repo.uploadBlob` 엔드포인트를 통해 업로드하는 것이며, 이 엔드포인트는 CID와 블롭에 대한 기본 메타데이터를 포함하는 `blob` JSON 객체를 반환합니다. 클라이언트는 이후 요청(예: 새로운 레코드에 포함)에서 이 `blob` 데이터를 사용할 수 있습니다. MIME 타입 및 파일 크기 등의 제약은 두 번째 단계에서만 검증됩니다. 서버는 업로드 단계에서 콘텐츠 타입 스니핑을 수행해 요청에 제공된 `Content-Type`과 다른 MIME 타입을 반환할 수 있지만, 업로드 HTTP 요청에는 여전히 `Content-Type` 헤더가 필요합니다.

특정 계정에 대한 블롭은 `com.atproto.sync.*` NSID 공간의 엔드포인트를 사용하여 나열 및 다운로드할 수 있습니다. 이러한 엔드포인트는 업로드된 원본 블롭에 대한 완전한 접근을 제공합니다. 일반적인 패턴은 애플리케이션이 원본 블롭과 함께 CDN 등에서 다운사이즈된 썸네일 또는 미리보기 버전을 별도의 URL로 미러링하는 것입니다(즉, 원본 PDS의 `getBlob` 엔드포인트에 직접 링크하지 않는 방식).

### 커서와 페이지네이션

Lexicon 설계에서 일반적인 패턴은 페이지네이션을 위한 `cursor` 파라미터를 포함하는 것입니다. 클라이언트는 첫 요청에 `cursor` 파라미터를 포함하지 않아야 하며, 이후 요청 간에는 다른 모든 파라미터를 고정해야 합니다. 응답에 커서가 포함된 경우, 그 값을 후속 요청에 포함하여 다음 배치의 응답을 가져올 수 있으며, 커서가 더 이상 포함되지 않을 때까지 반복합니다. 이는 결과 집합의 끝을 나타냅니다.

## 인증

XRPC의 기본 클라이언트/서버 인증 및 권한 부여 방식은 [Auth Specification](./oauth)에 설명된 OAuth입니다.

모든 엔드포인트가 인증을 요구하는 것은 아니지만, 어떤 엔드포인트가 인증을 요구하는지 일관되게 열거하는 방법은 아직 마련되지 않았습니다.

또한 HTTP Bearer 인증과 JWT 토큰(리프레시 토큰 포함)을 사용하는 이전 방식의 인증 체계도 존재합니다. 초기 로그인의 경우 `com.atproto.server.createSession` 엔드포인트를 사용하며, 이때 비밀번호와 계정 식별자(예: 핸들이나 등록된 이메일 주소)를 포함합니다. 이 요청은 초기 `accessJwt`와 함께 `refreshJwt`를 반환합니다.

대부분의 요청은 access JWT를 사용하여 인증해야 하지만, access JWT의 유효 기간은 짧습니다. 몇 분마다 새로운 access JWT를 `com.atproto.server.refreshSession` 엔드포인트를 호출하여 refresh JWT로 요청할 수 있습니다.

클라이언트는 JWT 토큰을 불투명한 문자열로 취급해야 하며, JWT 내부의 필드와 의미는 명세의 안정적인 부분이 아닙니다.

토큰을 생성하고 검증하는 서버(PDS 구현체)는 access와 refresh 토큰 간 도메인 분리를 위해 `typ` 헤더 필드를 사용해야 합니다. access 토큰은 `at+jwt`를, refresh 토큰은 `refresh+jwt`를 사용해야 합니다. (`at+jwt`는 [RFC 9068](https://www.rfc-editor.org/rfc/rfc9068.html)에서 정의된 "access token"의 약어이며, atproto의 "at"과는 관련이 없습니다.)

### 앱 비밀번호

앱 비밀번호는 타사 클라이언트나 웹 애플리케이션에 로그인할 때 보안 위험을 줄이기 위한 메커니즘입니다. 계정은 기본 비밀번호와 별도로 앱 비밀번호를 생성하고 취소할 수 있습니다. 앱 비밀번호는 기본 비밀번호와 동일한 방식으로 로그인하는 데 사용되지만, 계정 변경이나 인증 설정(앱 비밀번호 자체 포함)과 같이 파괴적인 행동은 제한됩니다.

클라이언트와 애플리케이션은 앱 비밀번호를 특별히 다룰 필요가 없으나, 대부분의 경우 로그인 시 앱 비밀번호 사용을 권장하는 안내 메시지를 포함하는 것이 모범 사례입니다. 앱 비밀번호는 보통 `xxxx-xxxx-xxxx-xxxx` 형식을 가지며, 클라이언트는 이 형식을 검사하여 기본 비밀번호로 인한 우발적인 로그인을 방지할 수 있습니다(기본 비밀번호가 이 형식을 따르지 않는 한).

### 관리자 토큰 (임시 명세)

일부 관리 XRPC 엔드포인트는 관리자 권한 인증을 요구합니다. 현재 이 방식은 JWT를 사용하는 HTTP Bearer 인증 대신, 사용자 "admin"과 고정 토큰을 비밀번호 필드에 포함하는 HTTP Basic 인증을 사용합니다. 이는 관리자 요청이 클라이언트의 계정이나 정체성과 직접적으로 연결되지 않고 단순히 "admin"으로 인식됨을 의미합니다.

참고로, HTTP Basic 인증은 사용자 이름과 비밀번호를 콜론(`:`)으로 결합한 후 해당 문자열을 표준 `base64`로 인코딩하여 동작합니다. 인코딩된 문자열은 `Authorization` 헤더에 `Basic `(공백 포함) 접두어와 함께 포함됩니다.

예를 들어, 관리자 토큰이 `secret-token`이라면 헤더는 다음과 같이 구성됩니다:

```
Authorization: Basic YWRtaW46c2VjcmV0LXRva2Vu
```

현재 관리자 인증이 필요한 엔드포인트는 다음과 같습니다:

- `com.atproto.admin.*`
- `com.atproto.server.createInviteCode`
- `com.atproto.server.createInviteCodes`

### 서비스 간 인증 (JWT)

이 섹션은 서명된 JWT를 사용하여 서비스 간 인증을 수행하는 메커니즘을 설명합니다.

현재 메커니즘은 계정의 atproto 서명 키로 서명된 단기 JWT를 사용하는 것입니다. 요청을 받는 서비스는 계정의 DID 문서에 있는 키를 사용하여 서명을 검증할 수 있습니다.

JWT의 주요 매개변수는 다음과 같습니다:

- **`alg` 헤더 필드 (string, 필수):** 서명에 사용된 키 유형을 나타냅니다 (자세한 내용은 [Cryptography](/specs/cryptography) 참조)
  - `k256` 키의 경우 `ES256K` 사용
  - `p256` 키의 경우 `ES256` 사용
- **`typ` 헤더 필드 (string, 필수):** 현재는 `JWT`이지만, 추후 보다 구체적인 값으로 업데이트될 예정입니다.
- **`iss` 본문 필드 (string, 필수):** 요청이 대표하는 계정의 DID입니다. 이 값은 접미사 서비스 식별자를 포함할 수 있습니다(아래 참조).
- **`aud` 본문 필드 (string, 필수):** 요청이 전달되는 대상 서비스와 연관된 서비스 DID입니다.
- **`exp` 본문 필드 (number, 필수):** 토큰 만료 시간(UNIX 타임스탬프, 초 단위)입니다. 단기 시간 창이어야 하며, 토큰 취소 기능은 구현되어 있지 않습니다. 약 60초 정도가 적절합니다.
- **`iat` 본문 필드 (number, 필수):** 토큰 생성 시간(UNIX 타임스탬프, 초 단위)입니다.
- **`lxm` 본문 필드 (string, 선택):** "lexicon method"의 약어로, NSID 형식을 따릅니다. 이 토큰이 권한을 부여하는 엔드포인트를 나타내며, 포함된 경우 서버는 반드시 이를 검증해야 합니다. 보안에 민감한 작업에서는 이를 요구하는 것이 좋으며, 추후 필수 항목이 될 수 있습니다.
- **`jti` 본문 필드 (string, 필수):** 고유한 무작위 문자열 논스(nonce)로, 토큰 재사용 및 재생 공격을 방지하는 데 사용됩니다.
- **JWT 서명 (string, 필수):** 계정 DID의 서명 키를 사용해 base64url 인코딩된 서명입니다.

토큰이 발행자의 DID 문서 내 특정 서비스 컨텍스트에서 생성될 경우, `iss` 필드에는 `#` 문자로 구분된 해당 서비스 식별자가 포함될 수 있습니다. 예를 들어, 라벨러 서비스를 위한 `did:web:label.example.com#atproto_labeler`와 같이 사용됩니다. 이 경우, 적절한 서명 키는 서비스 식별자와 고정 매핑된 키 식별자에 따라 결정됩니다:

- 서비스 식별자 `atproto_labeler`는 키 식별자 `atproto_label`에 매핑됩니다.

서비스 식별자가 포함되지 않은 경우 범용으로 간주하며, `atproto` 키 식별자를 사용해야 합니다.

요청을 받는 서비스는 특정 리소스나 엔드포인트에 접근할 때 특정 서비스 식별자의 사용을 요구하거나 금지할 수 있습니다.

서명은 계정의 서명 키(레포지토리 커밋에 서명하는 데 사용된 동일한 키)를 사용하여 일반적인 JWT 프로세스를 통해 계산됩니다. Typescript 의사 코드는 다음과 같습니다:

```
const headerPayload = utf8ToBase64Url(jsonStringify(header)) + '.' + utf8ToBase64Url(jsonString(body))
const signature = hashAndSign(accountSigningKey, utf8Bytes(headerPayload))
const jwt = headerPayload + '.' + bytesToBase64Url(signature)
```

## 서비스 프록시

PDS는 클라이언트와 다른 atproto 서비스 간의 일반 프록시 역할을 수행합니다. 클라이언트는 HTTP 헤더를 사용하여 요청을 전달할 네트워크 상의 특정 서비스를 지정할 수 있습니다(예: 특정 AppView 또는 라벨러 서비스). PDS는 일부 안전성 검사를 수행한 후, 사용자의 정체성으로 발급 및 서명된 서비스 간 인증 토큰(JWT, 위에 설명됨)을 사용하여 요청을 해당 서버로 전달합니다.

사용되는 HTTP 헤더는 `atproto-proxy`이며, 그 값은 `#` 문자로 구분된 서비스 엔드포인트 식별자를 포함한 DID입니다. PDS는 서비스 DID를 해석하고, 해당 DID 문서에서 서비스 엔드포인트 URL을 추출하여 요청을 프록시합니다.

라벨러 서비스로 프록시하기 위한 예시 요청 헤더는 다음과 같습니다:

```
atproto-proxy: did:web:labeler.example.com#atproto_labeler
```

프록시가 동작하기 위해서는 다음과 같은 조건을 충족해야 합니다. 이 조건들은 추후 네트워크 남용 문제를 해결하기 위해 확장될 수 있습니다.

- 대상 서비스는 해석 가능한 DID, 올바른 형식의 DID 문서, 그리고 일치하는 서비스 식별자를 가진 서비스 엔트리를 보유해야 합니다.
- atproto 엔드포인트 경로만 지원됩니다. 즉, `/xrpc/` 접두사 뒤에 유효한 NSID와 엔드포인트 이름이 있어야 합니다. (참고로 `/xrpc/` 접두사는 추후 설정 가능할 수 있습니다.)
- 요청은 PDS에 활성화된 계정을 가진 인증된 사용자로부터 전송되어야 합니다.
- PDS의 속도 제한(rate limit)이 여전히 적용됩니다.

## HTTP 헤더 요약

클라이언트는 다음과 같은 요청 헤더를 사용할 수 있습니다:

- **`Content-Type`:** 요청 본문이 있는 경우, 이 헤더를 포함하여 콘텐츠 타입을 명시해야 합니다.
- **`Authorization`:** 인증 정보를 포함합니다. 자세한 내용은 이 명세의 "인증" 섹션을 참조하세요.
- **`atproto-proxy`:** 다른 atproto 서비스로 프록시하기 위해 사용됩니다. 자세한 내용은 "서비스 프록시" 섹션을 참조하세요.
- **`atproto-accept-labelers`:** 클라이언트가 특정 라벨러로부터 라벨을 요청하여 응답에 포함시키도록 할 때 사용됩니다. 자세한 내용은 [라벨](/specs/label) 명세를 참조하세요.

## HTTP 상태 코드 요약

- **`200 OK`:** 요청이 성공적으로 처리되었습니다. 응답 본문이 있는 경우, `Content-Type` 헤더가 포함되어야 합니다.
- **`400 Bad Request`:** 요청이 잘못되어 처리되지 않았습니다.
- **`401 Unauthorized`:** 이 엔드포인트는 인증이 필요합니다. `WWW-Authenticate` 헤더가 포함되어야 합니다.
- **`403 Forbidden`:** 클라이언트에게 이 엔드포인트에 대한 권한이 없습니다.
- **`404 Not Found`:** 리소스가 없음을 나타낼 수 있으며, 이는 서버가 atproto를 지원하지 않거나 해당 엔드포인트를 지원하지 않음을 의미할 수 있습니다. 오류 메시지(또는 메시지 부재)를 통해 이를 명확히 해야 합니다.
- **`413 Payload Too Large`:** 요청 본문이 너무 큽니다. 가능하면 여러 개의 더 작은 요청으로 나누어야 합니다.
- **`429 Too Many Requests`:** 자원 한도가 초과되었으므로 클라이언트는 요청을 중단해야 합니다. `Retry-After` 헤더가 포함되어 특정 백오프 시간을 안내할 수 있습니다.
- **`500 Internal Server Error`:** 일반적인 내부 서비스 오류입니다. 클라이언트는 일정 지연 후 재시도할 수 있습니다.
- **`501 Not Implemented`:** 지정된 엔드포인트는 알려져 있으나 구현되지 않았습니다. 클라이언트는 재시도해서는 안 됩니다. (예를 들어, 서버가 WebSockets 요청을 지원하지 않을 때 반환됩니다.)
- **`502 Bad Gateway`, `503 Service Unavailable`, 또는 `504 Gateway Timeout`:** 일반적으로 일시적 또는 영구적인 서비스 중단을 나타냅니다. 클라이언트는 일정 지연 후 재시도할 수 있습니다.

## 사용 및 구현 가이드라인

클라이언트는 타임아웃, 제한된 재시도, 그리고 무작위 지수 백오프(randomized exponential backoff)를 구현하는 것이 권장됩니다. 이는 일시적인 다운타임 발생 시 견고성을 높이고, 동시에 과도한 서버 부하를 줄이는 데 도움이 됩니다.

서버는 `/xrpc/` 경로 접두사가 있는 모든 요청에 대해 사용자 정의 JSON 오류 응답을 구현하는 것이 바람직하지만, 현실적으로 많은 서비스는 일반적인 로드 밸런서나 리버스 프록시의 HTML 오류 페이지를 반환할 수 있으므로, 클라이언트는 JSON이 아닌 오류 응답에도 유연하게 대처해야 합니다.

HTTP 서버와 클라이언트 라이브러리는 일반적으로 URL(쿼리 파라미터 포함)의 전체 크기에 제한이 있으며, 이러한 제한은 XRPC에서 파라미터 사용에 제약을 줄 수 있습니다.

PDS 구현체는 지원하는 Lexicon에서 정의된 블롭 제약 사항의 상위 집합이어야 하는 전역 최대 크기나 허용 MIME 타입 집합 등을 기준으로 블롭 업로드를 제한할 수 있습니다.

## 보안 및 개인정보 보호 고려사항

개방된 인터넷에서는 HTTPS만 사용해야 합니다.

EXIF 메타데이터와 같이 개인 식별 정보가 포함된 블롭에 대해서는 주의가 필요합니다. 현재로서는 클라이언트가 블롭 업로드 전에 민감한 EXIF 메타데이터를 제거할 책임이 있으며, PDS가 우발적 메타데이터 유출 방지를 위해 도움을 줄 수 있는 방안(예: 메타데이터 자동 제거)도 추후 고려될 수 있습니다.

## 향후 변경 가능 사항

- 인증 시스템은 전면 개편될 가능성이 있습니다.
- Lexicon은 인증 요구 여부를 명시할 수 있어야 합니다.
- PDS의 역할이 일반 게이트웨이로 공식화되고 확장될 수 있습니다. 특정 XRPC 엔드포인트를 타 서비스로 프록시하는 일반적인 메커니즘이 추가될 수 있으며, 쿼리 및 블롭의 일반 캐싱에 대한 명세가 포함될 수 있습니다. 또한 PDS가 타 서비스의 응답을 변형하는 것이 명시적으로 허용될 수 있습니다.
- HTTP 리다이렉트 지원 여부에 대한 명시적인 결정이 필요합니다.
- 커서 페이지네이션 동작은, 커서가 반환되었으나 결과 목록이 비어 있거나 커서 값이 반복되는 경우에 대해 명확하게 정의되어야 합니다.
- 미디어 블롭에 내포된 민감한 메타데이터의 우발적 공개를 방지하기 위해, 업로드 블롭 엔드포인트에 메타데이터 제거를 선택적으로 비활성화하는 쿼리 파라미터가 추가될 수 있으며, 기본값은 모든 블롭 업로드 시 차단 또는 자동 제거로 설정될 수 있습니다.
- 서비스 간 인증을 위한 `lxm` JWT 필드는 추후 필수 항목이 될 수 있습니다.


---
atproto/src/app/[locale]/specs/xrpc/page.tsx
---
export const metadata = {
  title: 'HTTP API (XRPC)',
  description: 'Cross-system queries and procedures over HTTP',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/did/en.mdx
---
export const metadata = {
  title: 'DID',
  description:
    'Persistent decentralized identifiers (as used in atproto)',
}

# AT Protocol DIDs

The AT Protocol uses [Decentralized Identifiers](https://en.wikipedia.org/wiki/Decentralized_identifier) (DIDs) as persistent, long-term account identifiers. DID is a W3C standard, with many standardized and proposed DID method implementations. {{ className: 'lead' }}

## Blessed DID Methods

Currently, atproto supports two DID methods:

- `did:web`, which is a W3C standard based on HTTPS (and DNS). The identifier section is a hostname. This method is supported in atproto to provide an independent alternative to `did:plc`. The method is inherently tied to the domain name used, and does not provide a mechanism for migration or recovering from loss of control of the domain name. In the context of atproto, only hostname-level `did:web` DIDs are supported: path-based DIDs are not supported. The same restrictions on top-level domains that apply to handles (eg, no `.arpa`) also apply to `did:web` domains. The special `localhost` hostname is allowed, but only in testing and development environments. Port numbers (with separating colon hex-encoded) are only allowed for `localhost`, and only in testing and development.
- `did:plc`, which is a novel DID method developed by Bluesky. See the [did-method-plc](https://github.com/did-method-plc/did-method-plc) GitHub repository for details.

In the future, a small number of additional methods may be supported. It is not the intention to support all or even many DID methods, even with the existence of universal resolver software.


## AT Protocol DID Identifier Syntax

Lexicon string format type: `did`

The DID Core specification constraints on DID identifier syntax, regardless of the method used. A summary of those syntax constraints, which may be used to validate DID generically in atproto are:

- The entire URI is made up of a subset of ASCII, containing letters (`A-Z`, `a-z`), digits (`0-9`), period, underscore, colon, percent sign, or hyphen (`._:%-`)
- The URI is case-sensitive
- The URI starts with lowercase `did:`
- The method segment is one or more lowercase letters (`a-z`), followed by `:`
- The remainder of the URI (the identifier) may contain any of the above-allowed ASCII characters, except for percent-sign (`%`)
- The URI (and thus the remaining identifier) may not end in `:`.
- Percent-sign (`%`) is used for "percent encoding" in the identifier section, and must always be followed by two hex characters
- Query (`?`) and fragment (`#`) sections are allowed in DID URIs, but not in DID identifiers. In the context of atproto, the query and fragment parts are not allowed.

DID identifiers do not generally have a maximum length restriction, but in the context of atproto, there is an initial hard limit of 2048 characters.

In the context of atproto, implementations do not need to validate percent encoding. The percent symbol is allowed in DID identifier segments, but the identifier should not end in a percent symbol. A DID containing invalid percent encoding *should* fail any attempt at registration, resolution, etc.

A reasonable starting-point regex for DIDs in the context of atproto is:

```
// NOTE: does not constrain overall length
/^did:[a-z]+:[a-zA-Z0-9._:%-]*[a-zA-Z0-9._-]$/
```


### Examples

Valid DIDs for use in atproto (correct syntax, and supported method):

```
did:plc:z72i7hdynmk6r22z27h6tvur
did:web:blueskyweb.xyz
```

Valid DID syntax (would pass Lexicon syntax validation), but unsupported DID method:

```
did:method:val:two
did:m:v
did:method::::val
did:method:-:_:.
did:key:zQ3shZc2QzApp2oymGvQbzP8eKheVshBHbU4ZYjeXqwSKEn6N
```

Invalid DID identifier syntax (regardless of DID method):

```
did:METHOD:val
did:m123:val
DID:method:val
did:method:
did:method:val/two
did:method:val?two
did:method:val#two
```

## DID Documents

After a DID document has been resolved, atproto-specific information needs to be extracted. This parsing process is agnostic to the DID method used to resolve the document.

The current **handle** for the DID is found in the `alsoKnownAs` array. Each element of this array is a URI. Handles will have the URI scheme `at://`, followed by the handle, with no path or other URI parts. The current primary handle is the first valid handle URI found in the ordered list. Any other handle URIs should be ignored.

It is crucial to validate the handle bidirectionally, by resolving the handle to a DID and checking that it matches the current DID document.

The DID is the primary account identifier, and an account whose DID document does not contain a valid and confirmed handle can still, in theory, participate in the atproto ecosystem. Software should be careful to either not display any handle for such account, or obviously indicate that any handle associated with it is invalid.

The public **signing key** for the account is found under the `verificationMethod` array, in an object with `id` ending `#atproto`, and the `controller` matching the DID itself. The first valid atproto signing key in the array should be used, and any others ignored. The `type` field will indicate the cryptographic curve type, and the `publicKeyMultibase` field will be the public key in multibase encoding. See below for details for parsing these fields.

A valid signing key is required for atproto functionality, and an account with no valid key in their DID document is broken.

The **PDS service network location** for the account is found under the `service` array, with `id` ending `#atproto_pds`, and `type` matching `AtprotoPersonalDataServer`. The first matching entry in the array should be used, and any others ignored. The `serviceEndpoint` field must contain an HTTPS URL of server. It should contain only the URI scheme (`http` or `https`), hostname, and optional port number, not any "userinfo", path prefix, or other components.

A working PDS is required for atproto account functionality, and an account with no valid PDS location in their DID document is broken.

Note that a valid URL doesn't mean the the PDS itself is currently functional or hosting content for the account. During account migrations or server downtime there may be windows when the PDS is not accessible, but this does not mean the account should immediately be considered broken or invalid.


## Representation of Public Keys

The atproto cryptographic systems are described in [Cryptography](/specs/cryptography), including details of byte and string encoding of public keys.

Public keys in DID documents under `verificationMethod`, including atproto signing keys, are represented as an object with the following fields:

- `id` (string, required): the DID followed by an identifying fragment. Use `#atproto` as the fragment for atproto signing keys
- `type` (string, required): the fixed string `Multikey`
- `controller` (string, required): DID controlling the key, which in the current version of atproto must match the account DID itself
- `publicKeyMultibase` (string, required): the public key itself, encoded in multibase format (with multicodec type indicator, and "compressed" key bytes)

The `publicKeyMultibase` format for `Multikey` is the same encoding scheme as used with `did:key`, but without the `did:key:` prefix. See [Cryptography](/specs/cryptography) for details.

Note that there is not yet a formal W3C standard for using P-256 public keys in DID `verificationMethod` sections, but that the `Multikey` standard does clarify what the encoding encoding should be for this key type.


### Legacy Representation

Some older DID documents, which may still appear in `did:web` docs, had slightly different key encodings and `verificationMethod` syntax. Implementations may support these older DID documents during a transition period, but the intention is to require DID specification compliance going forward.

The older `verificationMethod` for atproto signing keys contained:

- `id` (string, required): the fixed string `#atproto`, without the full DID included
- `type` (string, required): a fixed name identifying the key's curve type
    - `p256`: `EcdsaSecp256r1VerificationKey2019` (note the "r")
    - `k256`: `EcdsaSecp256k1VerificationKey2019` (note the "k")
- `controller` (string, required): DID controlling the key, which in the current version of atproto must match the account DID itself
- `publicKeyMultibase` (string, required): the public key itself, encoded in multibase format (*without* multicodec, and *uncompressed* key bytes)

Note that the `EcdsaSecp256r1VerificationKey2019` type is not a final W3C standard.

The `EcdsaSecp256r1VerificationKey2019` `verificationMethod` is not a final W3C standard. We will move to whatever ends up standardized by W3C for representing P-256 public keys with `publicKeyMultibase`. This may mean a transition to `Multikey`, and we would transition K-256 representations to that `type` as well.

A summary of the multibase encoding in this context:

- Start with the full public key bytes. Do not use the "compressed" or "compact" representation (unlike for `did:key` or `Multikey` encoding)
- Do *not* prefix with a multicodec value indicating the key type
- Encode the key bytes with `base58btc`, yielding a string
- Add the character `z` as a prefix, to indicate the multibase, and include no other multicodec indicators

The decoding process is the same in reverse, using the curve type as context.

Here is an example of a single public key encoded in the legacy and current formats:

```
// legacy multibase encoding of K-256 public key
{
    "id": ...,
    "controller": ...,
    "type": "EcdsaSecp256k1VerificationKey2019",
    "publicKeyMultibase": "zQYEBzXeuTM9UR3rfvNag6L3RNAs5pQZyYPsomTsgQhsxLdEgCrPTLgFna8yqCnxPpNT7DBk6Ym3dgPKNu86vt9GR"
}

// preferred multibase encoding of same K-256 public key
{
    "id": ...,
    "controller": ...,
    "type": "Multikey",
    "publicKeyMultibase": "zQ3shXjHeiBuRCKmM36cuYnm7YEMzhGnCmCyW92sRJ9pribSF"
}
```


## Usage and Implementation Guidelines

Protocol implementations should be flexible to processing content containing DIDs based on unsupported DID methods. This is important to allow gradual evolution of the protocol ecosystem over time. In other words, implementations should distinguish between at least the distinct cases "invalid DID syntax", "unsupported DID method" and "supported DID method, but specific DID resolution failed".

While longer DIDs are supported in the protocol, a good best practice is to use relatively short DIDs, and to avoid DIDs longer than 64 characters.

DIDs are case-sensitive. While the currently-supported methods are *not* case sensitive, and could be safely lowercased, protocol implementations should reject DIDs with invalid casing. It is permissible to attempt case normalization when receiving user-controlled input, such as when parsing public URL path components, or text input fields.

## Possible Future Changes

The hard maximum DID length limit may be reduced, at the protocol syntax level. We are not aware of any DID methods that we would consider supporting which have identifiers longer than, say, 256 characters.

There is a good chance that the set of "blessed" DID methods will slowly expand over time.


---
atproto/src/app/[locale]/specs/did/ko.mdx
---
export const metadata = {
  title: 'DID',
  description:
    'AT Protocol에서 사용되는 영구적인 분산 식별자 (Decentralized Identifiers)',
}

# AT Protocol DIDs

AT Protocol은 계정을 영구적이고 장기적으로 식별하기 위해 [분산 식별자(Decentralized Identifiers, DID)](https://en.wikipedia.org/wiki/Decentralized_identifier)를 사용합니다. DID는 W3C 표준이며, 여러 DID 메서드가 표준화되었거나 제안되어 있습니다. {{ className: 'lead' }}

## 승인된(Blessed) DID 메서드

현재 atproto에서는 두 가지 DID 메서드를 지원합니다:

- `did:web`: HTTPS(및 DNS)를 기반으로 한 W3C 표준입니다. 식별자 부분은 호스트네임입니다. atproto에서 `did:plc`와 독립적인 대안으로 사용하기 위해 지원됩니다. 이 메서드는 도메인 이름에 밀접하게 연결되어 있으며, 도메인 이름 제어권 상실에 따른 이전 또는 복구 메커니즘을 제공하지 않습니다. atproto의 문맥에서는 오직 호스트네임 수준의 `did:web`만 지원하며, 경로 기반 DID는 지원되지 않습니다. 핸들에 적용되는 최상위 도메인 제한(예: `.arpa` 불허)도 `did:web` 도메인에 동일하게 적용됩니다. 특별히 테스트 및 개발 환경에서는 `localhost` 호스트네임이 허용되며, 포트 번호(콜론으로 구분, 16진수로 인코딩)는 오직 `localhost`에 한해서만 허용됩니다.
- `did:plc`: Bluesky에서 개발한 새로운 DID 메서드입니다. 자세한 내용은 [did-method-plc GitHub 저장소](https://github.com/did-method-plc/did-method-plc)를 참고하세요.

향후 소수의 추가 메서드가 지원될 수 있으나, 모든 DID 메서드 또는 다수의 메서드를 지원하는 것은 의도하지 않습니다. (보편적 해석기(universal resolver) 소프트웨어가 존재함에도 불구하고)

## AT Protocol DID 식별자 문법

Lexicon 문자열 형식 타입: `did`

DID Core 명세에서 사용되는 DID 식별자 문법의 제약 사항(메서드에 관계없이)은 atproto에서 DID를 검증할 때 사용할 수 있는 요약은 다음과 같습니다:

- 전체 URI는 ASCII의 부분 집합으로 구성되며, 허용되는 문자는 영문자(`A-Z`, `a-z`), 숫자(`0-9`), 점, 밑줄, 콜론, 퍼센트 기호 또는 하이픈(`._:%-`)입니다.
- URI는 대소문자를 구분합니다.
- URI는 소문자 `did:`로 시작해야 합니다.
- 메서드 섹션은 하나 이상의 소문자(`a-z`)로 구성되며, 그 뒤에 콜론(`:`)이 옵니다.
- URI의 나머지 부분(식별자)은 위에 허용된 ASCII 문자 중 퍼센트 기호(`%`)를 제외한 문자들로 구성될 수 있습니다.
- URI(따라서 식별자)는 콜론(`:`)으로 끝날 수 없습니다.
- 퍼센트 기호(`%`)는 식별자 내에서 "퍼센트 인코딩"에 사용되며, 항상 두 개의 16진수 문자가 뒤따라야 합니다.
- DID URI에는 쿼리(`?`)와 프래그먼트(`#`) 섹션이 허용되지만, DID 식별자에서는 허용되지 않습니다. atproto 문맥에서는 쿼리와 프래그먼트가 허용되지 않습니다.

일반적으로 DID 식별자는 최대 길이 제한이 없으나, atproto에서는 초기에 2KB의 하드 제한을 두고 있습니다.

atproto의 구현체는 퍼센트 인코딩의 유효성을 검사할 필요가 없습니다. 퍼센트 기호는 DID 식별자 내에서 허용되지만, 식별자는 퍼센트 기호로 끝나면 안 됩니다. 잘못된 퍼센트 인코딩을 포함하는 DID는 등록, 해석(resolve) 등 시도 시 실패해야 합니다.

atproto 문맥에서 DID를 위한 시작점으로 사용할 수 있는 정규 표현식 예시는 다음과 같습니다:

```js
// 전체 길이 제한은 고려하지 않음
/^did:[a-z]+:[a-zA-Z0-9._:%-]*[a-zA-Z0-9._-]$/
```

### 예시

atproto에서 사용 가능한 올바른 DID (문법이 올바르고 지원되는 메서드):

```
did:plc:z72i7hdynmk6r22z27h6tvur
did:web:blueskyweb.xyz
```

Lexicon 문법 검증에 통과하지만, 지원되지 않는 DID 메서드의 예:

```
did:method:val:two
did:m:v
did:method::::val
did:method:-:_:.
did:key:zQ3shZc2QzApp2oymGvQbzP8eKheVshBHbU4ZYjeXqwSKEn6N
```

문법상 올바르지 않은 DID 식별자의 예 (메서드에 관계없이):

```
did:METHOD:val
did:m123:val
DID:method:val
did:method:
did:method:val/two
did:method:val?two
did:method:val#two
```

## DID 문서

DID 문서가 해석(해결)된 후, atproto 고유의 정보를 추출할 필요가 있습니다. 이 파싱 과정은 사용된 DID 메서드와 무관하게 진행됩니다.

현재 **핸들(handle)** 은 `alsoKnownAs` 배열에서 찾을 수 있습니다. 이 배열의 각 요소는 URI입니다. 핸들은 `at://` 스킴을 가지며, 뒤에 핸들이 오며 경로나 다른 URI 부분이 포함되지 않습니다. 기본 핸들은 유효한 핸들 URI 중 순서상 첫 번째 항목이며, 나머지는 무시해야 합니다.

핸들을 양방향으로 검증하는 것이 중요합니다. 즉, 핸들을 DID로 해석한 후 해당 DID 문서와 일치하는지 확인해야 합니다.

DID는 주요 계정 식별자이며, 유효하고 확인된 핸들을 포함하지 않은 DID 문서를 가진 계정도 이론적으로는 atproto 생태계에 참여할 수 있습니다. 다만, 소프트웨어는 그러한 계정의 핸들을 표시하지 않거나, 해당 핸들이 유효하지 않음을 명확히 나타내야 합니다.

계정의 공개 **서명 키**는 DID 문서 내 `verificationMethod` 배열에 있으며, `id`가 `#atproto`로 끝나고, `controller`가 DID 자체와 일치하는 객체에서 찾을 수 있습니다. 배열에서 첫 번째 유효한 atproto 서명 키를 사용하며, 나머지는 무시합니다. `type` 필드는 암호화 곡선 타입을 나타내며, `publicKeyMultibase` 필드는 multibase 인코딩된 공개 키를 포함합니다. 아래에서 해당 필드의 파싱에 관한 자세한 내용을 확인할 수 있습니다.

유효한 서명 키는 atproto 기능에 필수이며, DID 문서에 유효한 키가 없는 계정은 정상적으로 작동하지 않습니다.

계정의 **PDS (Personal Data Server)** 서비스 위치는 DID 문서 내 `service` 배열에 있으며, `id`가 `#atproto_pds`로 끝나고, `type`이 `AtprotoPersonalDataServer`와 일치하는 항목에서 찾을 수 있습니다. 배열에서 첫 번째 일치 항목을 사용하며, 나머지는 무시합니다. `serviceEndpoint` 필드는 서버의 HTTPS URL을 포함해야 합니다. 이 URL은 스킴(`http` 또는 `https`), 호스트네임, 선택적 포트 번호만 포함해야 하며, 사용자 정보, 경로 접두사 또는 기타 구성 요소를 포함해서는 안 됩니다.

작동하는 PDS는 atproto 계정 기능에 필수이며, DID 문서에 유효한 PDS 위치가 없는 계정은 올바르게 작동하지 않습니다.

유효한 URL이라고 해서 반드시 해당 PDS가 현재 계정의 콘텐츠를 호스팅하거나 정상 작동 중임을 의미하지는 않습니다. 계정 이전이나 서버 다운타임 시 PDS에 접근할 수 없는 기간이 있을 수 있으나, 이것만으로 계정이 즉시 고장났거나 유효하지 않다고 판단해서는 안 됩니다.

## 공개 키의 표현

atproto의 암호화 시스템에 관한 자세한 내용은 [Cryptography](/specs/cryptography)를 참고하세요. 여기에는 공개 키의 바이트 및 문자열 인코딩 방식에 관한 세부 사항이 포함되어 있습니다.

DID 문서의 `verificationMethod`에 포함된 공개 키(특히 atproto 서명 키)는 다음 필드를 가진 객체로 표현됩니다:

- `id` (문자열, 필수): DID 뒤에 식별 프래그먼트를 붙인 값입니다. atproto 서명 키의 경우 프래그먼트로 `#atproto`를 사용합니다.
- `type` (문자열, 필수): 고정 문자열 `Multikey`
- `controller` (문자열, 필수): 키를 제어하는 DID로, 현재 atproto 버전에서는 계정 DID와 일치해야 합니다.
- `publicKeyMultibase` (문자열, 필수): multibase 형식(다중 코드표시기(multicodec) 타입 식별자와 "압축된" 키 바이트 포함)으로 인코딩된 공개 키

`Multikey` 형식의 `publicKeyMultibase`는 `did:key`에서 사용되는 인코딩 방식과 동일하지만, `did:key:` 접두사는 포함하지 않습니다. 자세한 내용은 [Cryptography](/specs/cryptography)를 참조하세요.

참고로, 과거의 DID 문서(특히 `did:web` 문서)에서는 약간 다른 키 인코딩 및 `verificationMethod` 문법이 사용된 바 있습니다. 구현체는 전환 기간 동안 이러한 오래된 DID 문서를 지원할 수 있으나, 앞으로는 DID 명세 준수를 요구할 예정입니다.

과거 atproto 서명 키의 `verificationMethod`는 다음과 같이 구성되었습니다:

- `id` (문자열, 필수): 전체 DID 대신 고정 문자열 `#atproto` 사용
- `type` (문자열, 필수): 키의 곡선 타입을 식별하는 고정 이름
  - `p256`: `EcdsaSecp256r1VerificationKey2019` (주의: "r")
  - `k256`: `EcdsaSecp256k1VerificationKey2019` (주의: "k")
- `controller` (문자열, 필수): 키를 제어하는 DID로, 현재 atproto 버전에서는 계정 DID와 일치해야 합니다.
- `publicKeyMultibase` (문자열, 필수): multicodec 없이, 압축되지 않은 키 바이트로 인코딩된 공개 키

`EcdsaSecp256r1VerificationKey2019` 타입은 최종 W3C 표준이 아닙니다.

요약하면, 레거시 방식의 multibase 인코딩은 다음과 같습니다:

- 전체 공개 키 바이트를 사용합니다. (`did:key` 또는 `Multikey` 인코딩과 달리 "압축" 또는 "간결한" 표현을 사용하지 않습니다)
- multicodec 타입을 나타내는 값을 접두사로 사용하지 않습니다.
- 키 바이트를 `base58btc`로 인코딩하여 문자열을 생성합니다.
- multibase를 나타내기 위해 문자열 앞에 `z` 문자를 추가하며, 다른 multicodec 식별자는 포함하지 않습니다.

역순으로 디코딩할 때는 곡선 타입을 참고하여 처리합니다.

다음은 동일한 K-256 공개 키가 레거시 형식과 현재 권장 형식으로 인코딩된 예시입니다:

```json
// 레거시 multibase 인코딩 (K-256 공개 키)
{
    "id": ...,
    "controller": ...,
    "type": "EcdsaSecp256k1VerificationKey2019",
    "publicKeyMultibase": "zQYEBzXeuTM9UR3rfvNag6L3RNAs5pQZyYPsomTsgQhsxLdEgCrPTLgFna8yqCnxPpNT7DBk6Ym3dgPKNu86vt9GR"
}

// 권장되는 multibase 인코딩 (동일한 K-256 공개 키)
{
    "id": ...,
    "controller": ...,
    "type": "Multikey",
    "publicKeyMultibase": "zQ3shXjHeiBuRCKmM36cuYnm7YEMzhGnCmCyW92sRJ9pribSF"
}
```

## 사용 및 구현 지침

프로토콜 구현체는 지원되지 않는 DID 메서드를 기반으로 한 DID를 처리할 때에도 유연해야 합니다. 이는 시간이 지나면서 프로토콜 생태계가 점진적으로 발전할 수 있도록 하기 위함입니다. 즉, 구현체는 "잘못된 DID 문법", "지원되지 않는 DID 메서드", "지원되는 DID 메서드이나 특정 DID 해석(resolve) 실패"와 같은 경우를 구분하여 처리해야 합니다.

DID는 프로토콜 상에서 더 긴 문자열도 지원하지만, 가급적 짧은 DID를 사용하고 64자보다 긴 DID는 피하는 것이 좋습니다.

DID는 대소문자를 구분합니다. 현재 지원되는 메서드는 대소문자를 구분하지 않으므로 안전하게 소문자로 변환할 수 있지만, 프로토콜 구현체는 잘못된 대소문자 사용이 있는 DID는 거부해야 합니다. 사용자 입력(예: 공개 URL 경로나 텍스트 입력 필드)을 처리할 때는 대소문자 정규화를 시도하는 것이 허용됩니다.

## 향후 변경 가능 사항

하드 최대 DID 길이 제한은 프로토콜 문법 수준에서 축소될 수 있습니다. 지원할 DID 메서드 중 식별자가 256자를 초과하는 경우는 현재 인지하고 있지 않습니다.

또한, "승인된(Blessed)" DID 메서드의 목록은 시간이 지나면서 점진적으로 확장될 가능성이 높습니다.


---
atproto/src/app/[locale]/specs/did/page.tsx
---
export const metadata = {
  title: 'DID',
  description: 'Persistent decentralized identifiers (as used in atproto)',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/account/en.mdx
---
export const metadata = {
  title: 'Accounts',
  description:
    'Account Hosting and Lifecycle',
}

# Account Hosting

All users in the atproto network have an "identity", based around a unique and immutable DID. Active users also have an "account" on a Personal Data Server (PDS). The PDS provides a number of network services, including repository hosting, authorization and authentication, and blob storage. Accounts have a lifecycle (including deletions and takedowns). Identities may migrate between hosting providers. Downstream services, including relays and AppViews, may redistribute account content.

Each service which redistributes content must decide independently what accounts and content to host and redistribute. For example, they might focus on a subset of accounts in the network, define their own content policies, or have regional legal obligations. All services are expected to respect certain protocol-level account actions (described below), such as temporary account deactivation. Combined, each network service has a synthesized “hosting status” for each account they distribute public data for.

This document describes the various hosting states an account and identity can have in the network, what the expectations are for downstream services when states change, and how accounts can be migrated between hosting providers (PDS instances).

## Hosting Status

On any network service, a known account is either `active` or not (a boolean flag) at a given point in time. If an account is not active, the service should not redistribute content for that account (repositories, individual records, blobs, etc). If the account is unknown (never seen before), the state can be undefined.

Identity metadata (DID documents and handle status), and the account hosting status itself, are distinct from hosted content, and can be redistributed.

Network services are encouraged to implement API endpoints (such as `com.atproto.sync.getRepoStatus`) which describe current hosting status for individual accounts. If they expose an event stream, they should also emit `#account` events when their local account hosting status updates for an account.

In addition to the `active` boolean, an account might in a more specific state (all of which correspond with `active=false`):

- `deleted`: user or host has deleted the account, and content should be removed from the network. Implied permanent or long-term, though may be reverted (deleted accounts may reactivate on the same or another host).
- `deactivated`: user has temporarily paused their overall account. Content should not be displayed or redistributed, but does not need to be deleted from infrastructure. Implied time-limited. Also the initial state for an account after migrating to another PDS instance.
- `takendown`: host or service has takendown the account. Implied permanent or long-term, though may be reverted.
- `suspended`: host or service has temporarily paused the account. Implied time-limited.

New account states will be added in the future. To prevent broken expectations, relevant API endpoints (such as `com.atproto.sync.getRepoStatus`, and the `#account` event on the `com.atproto.sync.subscribeRepos` event stream) break out `active` as a boolean flag, and then clarify non-active accounts with a separate `status` string. Services should use the `active` flag to control overall account visibility (observable behavior) with the `status` string acting as clarification which might determine more specific infrastructure behaviors (such as data deletion).

The deactivation and suspension states are implied to be temporary, though they might have indefinite time periods. Deletion and takedowns are implied to be more final, but can still technically be reverted (and frequently are, in practice).

Downstream services might decide to delete content rapidly upon takedown (like deletion), or simply stop redistributing content until a later time.

Identities might also be tombstoned, which is also implied to be permanent and final, but can technically be reversed in certain conditions. If the account's identity is `tombstoned`, the account hosting status should be interpreted as `deleted`.

To summarize, when an account status is non-`active`, the content that hosts should not redistributed includes:

- repository exports (CAR files)
- repo records
- transformed records ("views", embeds, etc)
- blobs
- transformed blobs (thumbnails, etc)

### Hosting Status Propagation

Unlike other aspects of atproto, account status is not self-certifying, and there is not a method for authenticated transfer or redistributing of account status between arbitrary parties.

Most commonly, account status is propagated “hop by hop”, with each “downstream” service accepting and adopting the hosting status for accounts at their upstream. If an intermediate service (such as a relay) overrides the status of their upstream (for example, an infrastructure account takedown), they will broadcast and propagate that action downstream to subscribers.

By default, if an account’s current active PDS is not available, account status should remain unchanged, at least for some time period. This keeps the network resilient and accounts online during temporary infrastructure outages. Services may decide their own policies for accounts whose PDS hosts remain offline for long periods of time.

The general expectation is that downstream services will not list an account as `active` if it is inactive at an upstream service. However, the concept of “upstream” is informal, and services may need to define their own policies for some corner-cases. For example, an AppView might consume from multiple independent relay providers, who report differing statuses for the same account, due to policy differences. If the network connection between a relay and PDS is disrupted, they may report different statuses for the same account. 

If there is doubt about about an account’s general status in the network, the account’s current active PDS host can be queried using the `com.atproto.sync.getRepoStatus` endpoint. If the account is not `active` there, it is generally expected to not be active elsewhere, especially in the case of user-initiated actions (deactivation or deletion).

### Content Deletion

Separate from overall account status, individual pieces of content may be removed. When accounts remove records from their public repositories, hosting services should remove public access to that content.

Applications which would benefit from explicit “tombstones” (indications that content previously existed) should explicitly design them in to Lexicon schemas. In the absence of explicit application-level tombstones, services are expected not to differentiate between content which has never existed and content which has been entirely deleted. Note that this is different from acknowledging account-level deletion, which is encouraged.

## PDS Account Migration

At a high level, the current PDS host for an account is indicated in the account’s DID document. If a DID document is updated to point at a new PDS host, the account status is `active` at that host, and there is a functioning repository (with valid signature and commit `rev`), then the account has effectively been migrated.

At a protocol level, this is how account migration works. And in some situations, when any previous PDS instance is unavailable or has inactive account status, getting in to the above state may be the only account recovery path available.

However, in the common case, when there is an active account on a functional current PDS, a more seamless account migration process is possible. It can summarized in a few high-level steps:

- creating an account on the new PDS (which may start with `active` false, such as `deactivated`)
- migrating data from the old PDS to the new PDS
- updating identity (DID document) to point to the new PDS and using a new atproto signing key. For PLC DIDs, usually involves updating PLC rotation keys as well.
- updating account status on both PDS instances, such that old is inactive, and new is active
- emitting a `#commit` event from the new PDS, signed by the new atproto signing key, with a higher commit `rev`.

## Usage and Implementation Guidelines

One possible account migration flow is described in detail in a [separate guide](/guides/account-migration). Note that the specific mechanisms described are not formally part of the protocol, and are not the only way to migrate accounts.

Guidelines for specific firehose event sequencing during different account events are described in an [Account Lifecycle Best Practices guide](/guides/account-lifecycle).

## Security Considerations

Account hosting status is not authenticated, and is specific to every individual network service. The current active PDS host is a good default for querying overall account state, though it may not represent broad network consensus (eg, intermediary takedowns), and it is technically possible for PDS hosts to misrepresent account activation state.

Processing account status changes may be resource intensive for downstream services. Rate-limits at the account (DID) level and upstream service provider (eg, PDS) level are recommended to prevent resource exhaustion attacks from bad actors. It may be appropriate for relevant accounts to be put in an inactive state (eg, `suspended`) in such a situation, to prevent the accounts from being "locked open".

Control of identity (DID and handle) is critical in the atproto authority model. Users should take special care to select and assess their PDS host when they delegate management of their identity to that host (eg, when using a `did:plc`).

## Future Work

Account migration between hosting providers is one of the core design goals for atproto, and it is expected that new protocol features will support account migration.

The migration process itself may also be improved and simplified.


---
atproto/src/app/[locale]/specs/account/ko.mdx
---
export const metadata = {
  title: '계정',
  description: '계정 호스팅 및 수명 주기',
}

# 계정 호스팅

atproto 네트워크의 모든 사용자는 고유하고 불변의 DID를 기반으로 하는 "정체성"을 가지고 있습니다. 활성 사용자들은 Personal Data Server (PDS)에 "계정"을 가지고 있으며, PDS는 레포지토리 호스팅, 권한 부여 및 인증, 블롭 저장 등 여러 네트워크 서비스를 제공합니다. 계정은 수명 주기를 가지며(삭제 및 중단 등을 포함), 정체성은 호스팅 제공자 간에 이전될 수 있습니다. 릴레이 및 AppViews 등 하위 서비스는 계정 콘텐츠를 재배포할 수 있습니다.

각 콘텐츠를 재배포하는 서비스는 계정과 콘텐츠를 호스팅 및 재배포할 때 개별적으로 결정해야 합니다. 예를 들어, 네트워크의 일부 계정에 집중하거나, 자체 콘텐츠 정책을 정의하거나, 지역별 법적 의무를 가질 수 있습니다. 모든 서비스는 아래에 설명된 임시 계정 비활성화와 같은 프로토콜 수준의 계정 동작을 준수해야 합니다. 이와 같이, 각 네트워크 서비스는 공개 데이터를 재배포하는 각 계정에 대해 종합적인 “호스팅 상태”를 갖게 됩니다.

이 문서는 네트워크 내에서 계정과 정체성이 가질 수 있는 다양한 호스팅 상태, 상태 변화 시 하위 서비스의 기대 사항, 그리고 계정을 다른 호스팅 제공자(PDS 인스턴스)로 이전하는 방법을 설명합니다.

## 호스팅 상태

어떤 네트워크 서비스에서든 알려진 계정은 특정 시점에 `active` 상태이거나 그렇지 않은 것으로 판단됩니다(불리언 값). 계정이 활성 상태가 아니라면, 해당 서비스는 그 계정의 콘텐츠(레포지토리, 개별 레코드, 블롭 등)를 재배포해서는 안 됩니다. 계정이 알려지지 않은 경우(이전에 본 적이 없는 경우), 상태는 정의되지 않을 수 있습니다.

정체성 메타데이터(DID 문서 및 핸들 상태)와 계정 호스팅 상태 자체는 호스팅된 콘텐츠와는 구분되며, 재배포될 수 있습니다.

네트워크 서비스는 계정의 현재 호스팅 상태를 설명하는 API 엔드포인트(예: `com.atproto.sync.getRepoStatus`)를 구현하는 것이 권장됩니다. 만약 이벤트 스트림을 노출한다면, 계정의 로컬 호스팅 상태가 업데이트될 때 `#account` 이벤트를 발행해야 합니다.

boolean `active` 외에도, 계정은 보다 구체적인 상태를 가질 수 있으며(모두 `active=false`에 해당):

- `deleted`: 사용자나 호스트가 계정을 삭제하여, 콘텐츠가 네트워크에서 제거되어야 함을 의미합니다. 영구적이거나 장기적인 의미를 내포하지만, 복구될 수 있습니다(삭제된 계정은 동일하거나 다른 호스트에서 재활성화될 수 있음).
- `deactivated`: 사용자가 전체 계정을 일시 중지한 상태입니다. 콘텐츠는 표시되거나 재배포되어서는 안 되지만, 인프라에서 삭제할 필요는 없습니다. 이는 시간 제한적임을 내포하며, 계정이 다른 PDS 인스턴스로 이전된 후의 초기 상태이기도 합니다.
- `takendown`: 호스트나 서비스가 계정을 중단시킨 상태입니다. 영구적이거나 장기적인 의미를 내포하지만, 복구될 수 있습니다.
- `suspended`: 호스트나 서비스가 계정을 일시 중지한 상태입니다. 이는 시간 제한적임을 내포합니다.

앞으로 새로운 계정 상태가 추가될 예정입니다. 예기치 않은 문제를 방지하기 위해, 관련 API 엔드포인트(예: `com.atproto.sync.getRepoStatus` 및 `com.atproto.sync.subscribeRepos` 이벤트 스트림의 `#account` 이벤트)는 불리언 `active`와 별도로 비활성 계정을 `status` 문자열로 명확히 구분합니다. 서비스는 계정의 전체 가시성을 제어하기 위해 `active` 플래그를 사용하고, `status` 문자열은 더 구체적인 인프라 동작(예: 데이터 삭제 등)을 결정하는 보조 정보로 활용해야 합니다.

비활성 상태인 deactivation과 suspension은 일시적임을 내포하지만, 무기한일 수 있습니다. 삭제(deletion)와 중단(takendown)은 보다 확정적인 의미를 내포하지만, 기술적으로는 복구될 수 있으며 실제로도 자주 복구됩니다.

하위 서비스는 중단(takendown)의 경우 콘텐츠를 빠르게 삭제할 수도 있고(삭제와 유사하게), 일정 기간 동안 단순히 재배포를 중단할 수도 있습니다.

정체성이 `tombstoned` 상태인 경우도 있는데, 이는 영구적이고 확정적인 의미를 내포하지만 특정 조건에서는 기술적으로 복구될 수 있습니다. 계정의 정체성이 `tombstoned`인 경우, 계정 호스팅 상태는 `deleted`로 해석되어야 합니다.

요약하면, 계정 상태가 `active`가 아닌 경우, 호스팅 서비스가 재배포해서는 안 되는 콘텐츠는 다음과 같습니다:

- 레포지토리 내보내기(CAR 파일)
- 레포지토리 레코드
- 변환된 레코드("뷰", 임베드 등)
- 블롭
- 변환된 블롭(썸네일 등)

### 호스팅 상태 전파

다른 atproto 요소와 달리, 계정 상태는 자체적으로 인증되지 않으며, 임의의 당사자 간에 인증된 전송이나 재배포 방법이 존재하지 않습니다.

대부분의 경우, 계정 상태는 “hop by hop” 방식으로 전파됩니다. 각 “하위” 서비스는 상위 서비스로부터 전달받은 계정의 호스팅 상태를 수용하고 채택합니다. 만약 중간 서비스(예: 릴레이)가 상위 서비스의 상태를 재정의한다면(예: 인프라에 의한 계정 중단), 해당 변경 사항은 하위 구독자에게 방송 및 전파되어야 합니다.

기본적으로, 계정의 현재 활성 PDS가 사용 불가능한 경우, 계정 상태는 적어도 일정 기간 동안은 변경되지 않아야 합니다. 이는 일시적인 인프라 장애 동안에도 네트워크의 탄력성을 유지하고 계정을 온라인 상태로 보존하기 위함입니다. 각 서비스는 장기간 동안 PDS 호스트가 오프라인인 계정에 대해 자체 정책을 결정할 수 있습니다.

일반적으로, 하위 서비스는 상위 서비스에서 비활성 상태로 보고된 계정을 `active`로 표시해서는 안 됩니다. 하지만 “상위”의 개념은 비공식적이므로, 일부 모서리 케이스에 대해서는 각 서비스가 자체 정책을 정의할 필요가 있습니다. 예를 들어, AppView가 서로 다른 정책을 가진 여러 독립적인 릴레이 공급자로부터 동일 계정에 대해 상이한 상태를 수신하거나, 릴레이와 PDS 간의 네트워크 연결에 문제가 있을 경우가 해당됩니다.

계정의 전반적인 상태에 대해 의문이 있는 경우, 해당 계정의 현재 활성 PDS 호스트에 `com.atproto.sync.getRepoStatus` 엔드포인트를 통해 조회할 수 있습니다. 만약 그곳에서 계정이 `active`가 아니라면, 특히 사용자가 직접 수행한 행동(비활성화 또는 삭제)의 경우, 다른 곳에서도 일반적으로 활성 상태로 간주되어서는 안 됩니다.

### 콘텐츠 삭제

전체 계정 상태와 별개로, 개별 콘텐츠가 제거될 수 있습니다. 계정이 공개 레포지토리에서 레코드를 삭제하는 경우, 호스팅 서비스는 해당 콘텐츠에 대한 공개 접근을 제거해야 합니다.

명시적인 “tombstones”(이전에 콘텐츠가 존재했음을 나타내는 표시)를 통해 이점을 얻고자 하는 애플리케이션은 Lexicon 스키마에 이를 명시적으로 설계해야 합니다. 명시적인 애플리케이션 수준의 tombstone이 없는 경우, 서비스는 존재하지 않았던 콘텐츠와 완전히 삭제된 콘텐츠를 구분하지 않아야 합니다. 이는 계정 수준의 삭제를 인정하는 것과는 다릅니다(계정 수준 삭제는 권장됨).

## PDS 계정 이전

높은 수준에서, 계정의 현재 PDS 호스트는 계정의 DID 문서에 표시됩니다. 만약 DID 문서가 새로운 PDS 호스트를 가리키도록 업데이트되고, 해당 호스트에서 계정 상태가 `active`이며(예를 들어, `deactivated`일 수 있음) 유효한 서명 및 커밋 `rev`를 가진 레포지토리가 존재한다면, 계정은 효과적으로 이전된 것으로 간주됩니다.

프로토콜 수준에서는 이것이 계정 이전이 작동하는 방식입니다. 또한, 이전 PDS 인스턴스가 사용 불가능하거나 계정 상태가 비활성인 경우, 위의 조건을 충족하는 것이 유일한 계정 복구 경로가 될 수 있습니다.

그러나 일반적인 경우, 기능하는 현재 PDS에서 활성 계정이 존재한다면 보다 원활한 계정 이전 프로세스가 가능합니다. 이는 몇 가지 높은 수준의 단계로 요약할 수 있습니다:

- 새로운 PDS에서 계정 생성(초기 상태는 `active`가 false일 수 있으며, 예를 들어 `deactivated` 상태)
- 이전 PDS에서 새로운 PDS로 데이터 이전
- 정체성(DID 문서) 업데이트: 새로운 PDS를 가리키고 새로운 atproto 서명 키 사용. PLC DID의 경우, 일반적으로 PLC 회전 키도 업데이트합니다.
- 두 PDS 인스턴스에서 계정 상태 업데이트: 이전 계정은 비활성, 새로운 계정은 활성으로 설정
- 새로운 atproto 서명 키로 서명되고 더 높은 커밋 `rev`를 포함하는 `#commit` 이벤트를 새로운 PDS에서 발행

## 사용 및 구현 가이드라인

가능한 계정 이전 흐름 중 하나는 [별도의 가이드](/guides/account-migration)에 자세히 설명되어 있습니다. 구체적으로 설명된 메커니즘은 프로토콜의 공식 부분은 아니며, 계정 이전의 유일한 방법은 아닙니다.

서로 다른 계정 이벤트 동안의 firehose 이벤트 순서에 대한 모범 사례는 [계정 수명 주기 베스트 프랙티스 가이드](/guides/account-lifecycle)에 설명되어 있습니다.

## 보안 고려사항

계정 호스팅 상태는 인증되지 않으며, 각 개별 네트워크 서비스에 따라 달라집니다. 계정의 현재 활성 PDS 호스트는 전체 계정 상태를 조회하기 위한 좋은 기본값이지만, 이는 넓은 네트워크 합의를 대표하지 않을 수 있으며(예: 중간 계정 중단), PDS 호스트가 계정 활성 상태를 잘못 표현할 가능성도 존재합니다.

하위 서비스가 계정 상태 변경을 처리하는 데 많은 리소스가 소요될 수 있습니다. 잘못된 행위자로 인한 리소스 고갈 공격을 방지하기 위해, 계정(DID) 수준 및 상위 서비스 제공자(PDS) 수준에서의 속도 제한(rate-limit)이 권장됩니다. 이러한 상황에서는 관련 계정을 비활성 상태(예: `suspended`)로 전환하여 계정이 “계속 활성 상태로 잠기지 않도록” 하는 것이 적절할 수 있습니다.

정체성(DID 및 핸들)의 관리는 atproto 권한 모델에서 매우 중요합니다. 사용자는 자신의 정체성 관리를 해당 호스트에 위임할 때(예: `did:plc` 사용 시), PDS 호스트를 신중하게 선택하고 평가해야 합니다.

## 향후 작업

호스팅 제공자 간의 계정 이전은 atproto의 핵심 설계 목표 중 하나이며, 새로운 프로토콜 기능이 계정 이전을 지원할 것으로 예상됩니다.

계정 이전 프로세스 자체도 개선되고 단순화될 수 있습니다.


---
atproto/src/app/[locale]/specs/account/page.tsx
---
export const metadata = {
  title: 'Accounts',
  description: 'Account Hosting and Lifecycle',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/event-stream/en.mdx
---
export const metadata = {
  title: 'Event Stream',
  description:
    'Network wire protocol for subscribing to a stream of Lexicon objects',
  wip: true
}

# Event Stream

In addition to regular [HTTP API](/specs/xrpc) endpoints, atproto supports continuous event streams. Message schemas and endpoint names are transport-agnostic and defined in [Lexicons](/specs/lexicon). The initial encoding and transport scheme uses binary [DAG-CBOR](https://ipld.io/docs/codecs/known/dag-cbor/) encoding over [WebSockets](https://en.wikipedia.org/wiki/WebSocket). {{ className: 'lead' }}

The Lexicon type for streams is `subscription`. The schema includes an identifier (`id`) for the endpoint, a `message` schema (usually a union, allowing multiple message types), and a list of error types (`errors`).

Clients subscribe to a specific stream by initiating a connection at the indicated endpoint. Streams are currently one-way, with messages flowing from the server to the client. Clients may provide query parameters to configure the stream when opening the connection.

A **backfill window** mechanism allows clients to catch up with stream messages they may have missed. At a high level, this works by assigning monotonically increasing sequence numbers to stream events, and allowing clients to specify an initial sequence number when initiating a connection. The intent of this mechanism is to ensure reliable delivery of events following disruptions during a reasonable time window (eg, hours or days). It is not to enable clients to roll all the way back to the beginning of the stream.

All of the initial subscription Lexicons in the `com.atproto` namespace use the backfill mechanism. However, a backfill mechanism (and even cursors, which we define below) is not _required_ for streams. Subscription endpoints which do not require reliable delivery do not need to implement a backfill mechanism or use sequence numbers.

The initial subscription endpoints are also public and do not require authentication or prior permission to subscribe (though resource limits may be imposed on client). But subscription endpoints may require authentication at connection time, using the existing HTTP API (XRPC) authentication methods.


## Streaming Wire Protocol (v0)

To summarize, messages are encoded as DAG-CBOR and sent over a binary WebSocket. Clients connect to to a specific HTTP endpoint, with query parameters, then upgrade to WebSocket. Every WebSocket frame contains two DAG-CBOR objects, with bytes concatenated together: a header (indicating message type), and the actual message.

The WebSockets "living standard" is currently maintained by [WHATWG](https://en.wikipedia.org/wiki/WHATWG), and can be found in full at [https://websockets.spec.whatwg.org/](https://websockets.spec.whatwg.org/).

### Connection

Clients initialize stream subscriptions by opening an HTTP connection and upgrading to a WebSocket. HTTPS and "WebSocket Secure" (`wss://`) on the default port (443) should be used for all connections on the internet. HTTP, cleartext WebSocket (`ws://`), and non-standard ports should only be used for testing, development, and local connections (for example, behind a reverse proxy implementing SSL). From the client perspective, failure to upgrade connection to a WebSocket is an error.

Query parameters may be provided in the initial HTTP request to configure the stream in an application-specific way, as specified in the endpoint's Lexicon schema.

Errors are usually returned through the stream itself. Connection-time errors are sent as the first message on the stream, and then the server drops the connection. But some errors can not be handled through the stream, and are returned as HTTP errors:

- `405 Method Not Allowed`: Returned to client for non-GET HTTP requests to a stream endpoint.
- `426 Upgrade Required`: Returned to client if `Upgrade` header is not included in a request to a stream endpoint.
- `429 Too Many Requests`: Frequently used for rate-limiting. Client may try again after a delay. Support for the `Retry-After` header is encouraged.
- `500 Internal Server Error`: Client may try again after a delay
- `501 Not Implemented`: Service does not implement WebSockets or streams, at least for this endpoint. Client should not try again.
- `502 Bad Gateway`, `503 Service Unavailable`, `504 Gateway Timeout`: Client may try again after a delay

Servers *should* return HTTP bodies as JSON with the standard XRPC error message schema for these status codes. But clients also need to be robust to unexpected response body formats. A common situation is receiving a default load-balancer or reverse-proxy error page during scheduled or unplanned downtime.

Either the server or the client may decided to drop an open stream connection if there have been no messages for some time. It is also acceptable to leave connections open indefinitely.

### Framing

Each binary WebSocket frame contains two DAG-CBOR objects, concatenated. The first is a **header** and the second is the **payload.**

The header DAG-CBOR object has the following fields:

- `op` ("operation", integer, required): fixed values, indicating what this frame contains
    - `1`: a regular message, with type indicated by `t`
    - `-1`: an error message
- `t` ("type", string, optional): required if `op` is `1`, indicating the Lexicon sub-type for this message, in short form. Does not include the full Lexicon identifier, just a fragment. Eg: `#commit`. Should not be included in header if `op` is `-1`.

Clients should ignore frames with headers that have unknown `op` or `t` values. Unknown fields in both headers and payloads should be ignored. Invalid framing or invalid DAG-CBOR encoding are hard errors, and the client should drop the entire connection instead of skipping the frame. Servers should ignore any frames received from the client, not treat them as errors.

Error payloads all have the following fields:

- `error` (string, required): the error type name, with no namespace or `#` prefix
- `message` (string, optional): a description of the error

Streams should be closed immediately following transmitting or receiving an error frame.

Message payloads must always be objects. They should omit the `$type` field, as this information is already indicated in the header. There is no specific limit on the size of WebSocket frames in atproto, but they should be kept reasonably small (around a couple megabytes).

If a client can not keep up with the rate of messages, the server may send a "too slow" error and close the connection.

### Sequence Numbers

Streams can optionally make use of per-message sequence numbers to improve the reliability of transmission. Clients keep track of the last sequence number they received and successfully processed, and can specify that number after a re-connection to receive any missed messages, up to some roll-back window. Servers persist no client state across connections. The semantics are similar to [Apache Kafka](https://en.wikipedia.org/wiki/Apache_Kafka)'s consumer groups and other stream-processing protocols.

Subscription Lexicons must include a `seq` field (integer type), and a `cursor` query parameter (integer type). Not all message types need to include `seq`. Errors do not, and it is common to have an `#info` message type that is not persisted.

Sequence numbers are always positive integers (non-zero), and increase monotonically, but otherwise have flexible semantics. They may contain arbitrary gaps. For example, they might be timestamps.

To prevent confusion when working with Javascript (which by default represents all numbers as floating point), sequence numbers should be limited to the range of integers which can safely be represented by a 64-bit float. That is, the integer range `1` to `2^53` (not inclusive on the upper bound).

The connection-time rules for cursors and sequence numbers:

- no `cursor` is specified: the server starts transmitting from the current stream position
- `cursor` is higher than current `seq` ("in the future"): server sends an error message and closes connection
- `cursor` is in roll-back window: server sends any persisted messages with greater-or-equal `seq` number, then continues once "caught up" with current stream
- `cursor` is older than roll-back window: the first message in stream is an info indicating that `cursor` is too-old, then starts at the oldest available `seq` and sends the entire roll-back window, then continues with current stream
- `cursor` is `0`: server will start at the oldest available `seq`, send the entire roll-back window, then continue with current stream

The scope for sequence numbers is the combination of service provider (hostname) and endpoint (NSID). This roughly corresponds to the `wss://` URL used for connections. That is, sequence numbers may or may not be unique across different stream endpoints on the same service.

Services should ensure that sequence numbers are not re-used, usually by committing events (with sequence number) to robust persistent storage before transmitting them over streams.

In some catastrophic failure modes (or large changes to infrastructure), it is possible that a server would lose data from the backfill window, and need to reset the sequence number back to `1`. In this case, if a client re-connects with a higher number, the server would send back a `FutureCursor` error to the client. The client needs to decide what strategy to follow in these scenarios. We suggest that clients treat out-of-order or duplicate sequence numbers as an error, not process the message, and drop the connection. Most clients should not reset sequence state without human operator intervention, though this may be a reasonable behavior for some ephemeral clients not requiring reliable delivery of every event in the stream.

## Usage and Implementation Guidelines

The current stream transport is primarily designed for server-to-server data synchronization. It is also possible for web applications to connect directly from end-user browsers, but note that decoding binary frames and DAG-CBOR is non-trivial.

The combination of HTTP redirects and WebSocket upgrades is not consistently supported by WebSocket client libraries. Support is not specifically required or forbidden in atproto.

Supported versions of the WebSockets standard are not specified by atproto. The current stable WebSocket standard is version 13. Implementations should make reasonable efforts to support modern versions, with some window of backwards compatibility.

WebSockets have distinct resource rate-limiting and denial-of-service issues. Network bandwidth limits and throttling are recommended for both servers and clients. Servers should tune concurrent connection limits and buffer sizes to prevent resource exhaustion.

If services need to reset sequence state, it is recommended to chose a new initial sequence number with a healthy margin above any previous sequence number. For example, after persistent storage loss, or if clearing prior stream state.

URLs referencing a stream endpoint at a particular host should generally use `wss://` as the URI scheme (as opposed to `https://`).

## Security and Privacy Considerations

As mentioned in the "Connection" section, only `wss://` (SSL) should be used for stream connections over the internet. Public services should reject non-SSL connections.

Most HTTP XRPC endpoints work with content in JSON form, while stream endpoints work directly with DAG-CBOR objects as untrusted input. Precautions must be taken against hostile data encoding and data structure manipulation. Specific issues are discussed in the [Data Model](/specs/data-model) and [Repository](/specs/repository) specifications.

## Possible Future Changes

Event Streams are one of the newest components of the AT Protocol, and the details are more likely to be iterated on compared to other components.

The sequence number scheme may be tweaked to better support sharded streams. The motivation would be handle higher data throughputs over the public internet by splitting across multiple connections.

Additional transports (other than WebSocket) and encodings (other than DAG-CBOR) may be specified. For example, JSON payloads in text WebSocket frames would be simpler to decode in browsers.

Additional WebSocket features may be adopted:

- transport compression "extensions" like `permessage-deflate`
- definition of a sub-protocol
- bi-directional messaging
- 1000-class response codes

Ambiguities in this specification may be resolved, or left open. For example:

- HTTP redirects
- CORS and other issues for browser connections
- maximum message/frame size

Authentication schemes may be supported, similar to those for regular HTTP XRPC endpoints.


---
atproto/src/app/[locale]/specs/event-stream/ko.mdx
---
export const metadata = {
  title: '이벤트 스트림',
  description:
    'Lexicon 객체 스트림 구독을 위한 네트워크 와이어 프로토콜',
  wip: true
}

# 이벤트 스트림

일반적인 [HTTP API](/specs/xrpc) 엔드포인트 외에도, atproto는 연속적인 이벤트 스트림을 지원합니다. 메시지 스키마와 엔드포인트 이름은 전송 방식에 구애받지 않으며, [Lexicons](/specs/lexicon)에서 정의됩니다. 초기 인코딩 및 전송 방식은 바이너리 [DAG-CBOR](https://ipld.io/docs/codecs/known/dag-cbor/) 인코딩을 [WebSockets](https://en.wikipedia.org/wiki/WebSocket) 위에서 사용합니다. {{ className: 'lead' }}

Lexicon에서 스트림의 타입은 `subscription`입니다. 이 스키마는 엔드포인트 식별자(`id`), 메시지 스키마(일반적으로 여러 메시지 타입을 허용하는 union 형태), 그리고 에러 타입 목록(`errors`)을 포함합니다.

클라이언트는 지정된 엔드포인트에 연결을 시작함으로써 특정 스트림을 구독합니다. 현재 스트림은 서버에서 클라이언트로 단방향 메시지를 전송합니다. 클라이언트는 연결을 시작할 때 쿼리 파라미터를 제공하여 스트림을 애플리케이션 별로 구성할 수 있습니다.

**백필(Backfill) 윈도우** 메커니즘을 통해 클라이언트는 놓친 스트림 메시지를 다시 받을 수 있습니다. 기본적으로 스트림 이벤트에는 단조 증가하는 시퀀스 번호가 부여되며, 클라이언트는 연결을 시작할 때 초기 시퀀스 번호를 지정할 수 있습니다. 이 메커니즘의 목적은 연결 장애 후 일정 기간(예: 수 시간 또는 수 일) 내에 안정적인 이벤트 전달을 보장하는 것이지, 클라이언트가 스트림의 시작부터 모든 메시지를 요청할 수 있도록 하는 것이 아닙니다.

초기 구독 Lexicon들은 모두 `com.atproto` 네임스페이스 내에서 백필 메커니즘을 사용합니다. 하지만 백필 메커니즘(및 아래에서 정의하는 커서)은 스트림에 _필수_ 사항이 아닙니다. 안정적인 전달이 필요 없는 구독 엔드포인트는 백필 메커니즘이나 시퀀스 번호를 구현할 필요가 없습니다.

초기 구독 엔드포인트는 공개되어 있으며, 구독을 위해 인증이나 사전 권한이 필요하지 않습니다(단, 클라이언트에 대해 리소스 제한이 적용될 수 있음). 다만, 스트림 엔드포인트는 기존 HTTP API(XRPC) 인증 방법을 사용하여 연결 시 인증을 요구할 수 있습니다.

## 스트리밍 와이어 프로토콜 (v0)

요약하면, 메시지는 DAG-CBOR로 인코딩되어 바이너리 WebSocket을 통해 전송됩니다. 클라이언트는 특정 HTTP 엔드포인트에 연결하고 쿼리 파라미터를 포함한 후, WebSocket으로 업그레이드합니다. 모든 WebSocket 프레임은 두 개의 DAG-CBOR 객체(바이트가 연결되어 있음)를 포함하는데, 첫 번째는 헤더(메시지 타입을 나타냄)이고 두 번째는 실제 메시지입니다.

WebSocket의 "Living Standard"는 현재 [WHATWG](https://en.wikipedia.org/wiki/WHATWG)에서 유지 관리되며, 전체 명세는 [https://websockets.spec.whatwg.org/](https://websockets.spec.whatwg.org/)에서 확인할 수 있습니다.

### 연결

클라이언트는 HTTP 연결을 열고 WebSocket으로 업그레이드하여 스트림 구독을 초기화합니다. 인터넷 상의 연결에는 기본 포트(443)의 HTTPS와 "WebSocket Secure"(즉, `wss://`)를 사용해야 합니다. 테스트, 개발, 또는 로컬 연결(예: SSL을 구현하는 리버스 프록시 뒤)에서는 HTTP, 비보안 WebSocket(`ws://`), 또는 비표준 포트를 사용할 수 있습니다. 클라이언트 관점에서는 WebSocket으로 업그레이드하지 못하는 경우 에러로 간주됩니다.

엔드포인트의 Lexicon 스키마에서 지정한 대로, 쿼리 파라미터를 통해 애플리케이션 별로 스트림을 구성할 수 있습니다.

에러는 보통 스트림 자체를 통해 반환됩니다. 연결 시 발생하는 에러는 스트림의 첫 번째 메시지로 전송된 후 서버가 연결을 종료합니다. 다만, 일부 에러는 스트림을 통해 처리할 수 없어 HTTP 에러로 반환됩니다:

- `405 Method Not Allowed`: 스트림 엔드포인트에 GET이 아닌 HTTP 요청을 보낸 경우
- `426 Upgrade Required`: 요청에 `Upgrade` 헤더가 포함되지 않은 경우
- `429 Too Many Requests`: 주로 속도 제한에 사용되며, 클라이언트는 일정 시간 후 재시도할 수 있음 (특히 `Retry-After` 헤더 지원이 권장됨)
- `500 Internal Server Error`: 클라이언트는 일정 시간 후 재시도할 수 있음
- `501 Not Implemented`: 서비스가 해당 엔드포인트에 대해 WebSocket이나 스트림을 구현하지 않은 경우(클라이언트는 재시도하지 않아야 함)
- `502 Bad Gateway`, `503 Service Unavailable`, `504 Gateway Timeout`: 클라이언트는 일정 시간 후 재시도할 수 있음

서버는 이들 상태 코드에 대해 표준 XRPC 에러 메시지 스키마에 따른 JSON 형태의 HTTP 본문을 반환하는 것이 *권장*되지만, 클라이언트는 예상치 못한 응답 본문 형식에도 유연하게 대응해야 합니다. 예를 들어, 예정된 점검이나 예기치 않은 다운타임 중에 로드 밸런서 또는 리버스 프록시의 기본 에러 페이지를 받을 수 있습니다.

서버나 클라이언트 모두, 일정 시간 동안 메시지가 없을 경우 열린 스트림 연결을 종료할 수 있습니다. 또는 연결을 무한정 유지할 수도 있습니다.

### 프레이밍

각 바이너리 WebSocket 프레임은 두 개의 DAG-CBOR 객체(연결된 바이트)를 포함합니다. 첫 번째 객체는 **헤더**이고, 두 번째 객체는 **페이로드**입니다.

헤더 DAG-CBOR 객체는 다음 필드를 포함합니다:

- `op` ("operation", 정수, 필수): 이 프레임에 포함된 내용의 유형을 나타내는 고정 값
  - `1`: `t`로 표시된 타입의 일반 메시지
  - `-1`: 에러 메시지
- `t` ("type", 문자열, 선택): `op`가 `1`인 경우 필수이며, Lexicon 하위 타입(예: `#commit`)을 짧은 형식으로 나타냅니다. `op`가 `-1`인 경우 헤더에 포함되어서는 안 됩니다.

클라이언트는 알 수 없는 `op` 또는 `t` 값을 가진 헤더를 포함하는 프레임은 무시해야 합니다. 헤더와 페이로드에 포함된 알 수 없는 필드 역시 무시되어야 합니다. 프레임의 프레이밍이나 DAG-CBOR 인코딩이 잘못된 경우, 클라이언트는 해당 프레임을 건너뛰지 않고 전체 연결을 종료해야 합니다. 서버는 클라이언트로부터 수신된 프레임에 대해 에러로 처리하지 않고 무시해야 합니다.

에러 페이로드는 다음 필드를 포함합니다:

- `error` (문자열, 필수): 네임스페이스나 `#` 접두사 없이 에러 타입 이름
- `message` (문자열, 선택): 에러에 대한 설명

에러 프레임 전송 또는 수신 후에는 즉시 스트림 연결이 종료되어야 합니다.

메시지 페이로드는 항상 객체여야 하며, `$type` 필드는 생략해야 합니다(헤더에서 이미 해당 정보가 전달되기 때문입니다). atproto에서는 WebSocket 프레임의 크기에 특정 제한은 없으나, 프레임은 합리적인 크기(약 몇 메가바이트 내외)로 유지해야 합니다.

클라이언트가 메시지 처리 속도를 따라가지 못하는 경우, 서버는 "너무 느림" 에러를 전송하고 연결을 종료할 수 있습니다.

### 시퀀스 번호

스트림은 전송의 안정성을 높이기 위해 메시지 당 시퀀스 번호를 선택적으로 사용할 수 있습니다. 클라이언트는 마지막으로 받은 시퀀스 번호를 추적하며, 재연결 시 해당 번호 이후의 누락된 메시지를 요청할 수 있습니다. 서버는 연결 간에 클라이언트 상태를 유지하지 않습니다. 이 방식은 [Apache Kafka](https://en.wikipedia.org/wiki/Apache_Kafka)나 기타 스트림 처리 프로토콜과 유사한 의미를 가집니다.

구독 Lexicon은 반드시 `seq` 필드(정수 타입)와 `cursor` 쿼리 파라미터(정수 타입)를 포함해야 합니다. 모든 메시지 타입에 `seq`를 포함할 필요는 없습니다. 에러 메시지는 시퀀스 번호를 포함하지 않으며, 종종 지속되지 않는 `#info` 메시지 타입이 사용됩니다.

시퀀스 번호는 항상 0이 아닌 양의 정수이며 단조 증가하지만, 그 외의 의미는 유연합니다. 시퀀스 번호는 불연속적일 수 있습니다. 예를 들어, 타임스탬프를 나타낼 수도 있습니다.

자바스크립트가 기본적으로 모든 숫자를 부동 소수점으로 표현하는 문제를 피하기 위해, 시퀀스 번호는 64비트 부동 소수점으로 안전하게 표현 가능한 범위, 즉 `1`부터 `2^53` 미만으로 제한하는 것이 좋습니다.

시퀀스 번호의 범위는 서비스 제공자(호스트명)와 엔드포인트(NSID)의 조합에 의해 결정됩니다. 이는 대략적으로 연결에 사용된 `wss://` URL에 해당하며, 같은 서비스 내의 다른 스트림 엔드포인트 간에 시퀀스 번호가 고유하지 않을 수 있음을 의미합니다.

서비스는 시퀀스 번호가 재사용되지 않도록 보장해야 하며, 일반적으로 이벤트(시퀀스 번호 포함)를 전송하기 전에 내구성 있는 영구 저장소에 기록해야 합니다.

심각한 장애나 인프라 변경 시, 서버가 백필 윈도우의 데이터를 잃고 시퀀스 번호를 `1`로 재설정할 가능성도 있습니다. 이 경우 클라이언트가 더 높은 번호로 재연결하면, 서버는 `FutureCursor` 에러를 반환합니다. 클라이언트는 이러한 상황에 대해 적절한 전략을 결정해야 합니다. 대부분의 클라이언트는 순서가 맞지 않거나 중복된 시퀀스 번호를 에러로 처리하고 메시지를 무시한 후 연결을 종료해야 합니다. 대부분의 클라이언트는 사람이 개입하지 않는 한 시퀀스 상태를 재설정하지 않아야 하지만, 신뢰성이 덜 요구되는 일부 클라이언트에서는 허용될 수 있습니다.

## 사용 및 구현 가이드라인

현재 스트림 전송 방식은 주로 서버 간 데이터 동기화를 위해 설계되었습니다. 최종 사용자 브라우저에서 직접 연결할 수도 있으나, 바이너리 프레임과 DAG-CBOR 디코딩은 간단하지 않음을 유의해야 합니다.

HTTP 리다이렉트와 WebSocket 업그레이드의 조합은 WebSocket 클라이언트 라이브러리마다 일관되게 지원되지 않습니다. atproto에서는 이를 특별히 지원하거나 금지하지 않습니다.

지원되는 WebSocket 표준 버전은 atproto에서 명시되어 있지 않습니다. 현재 안정적인 WebSocket 표준은 버전 13이며, 구현체는 최신 버전을 지원하기 위해 합리적인 노력을 기울이고 일정 범위의 하위 호환성을 제공해야 합니다.

WebSocket은 고유의 리소스 속도 제한 및 서비스 거부(DoS) 문제를 갖고 있으므로, 서버와 클라이언트 모두 네트워크 대역폭 제한 및 스로틀링을 적용하는 것이 권장됩니다. 서버는 동시 연결 제한과 버퍼 크기를 적절히 조절하여 리소스 고갈을 방지해야 합니다.

서비스가 시퀀스 상태를 재설정해야 하는 경우, 이전 시퀀스 번호보다 충분히 큰 새로운 초기 시퀀스 번호를 선택하는 것이 좋습니다. 예를 들어, 영구 저장소 손실이나 이전 스트림 상태 초기화 시에 해당됩니다.

특정 호스트의 스트림 엔드포인트를 참조하는 URL은 일반적으로 `https://` 대신 `wss://` URI 스킴을 사용해야 합니다.

## 보안 및 프라이버시 고려사항

앞서 "연결" 섹션에서 언급한 바와 같이, 인터넷 상의 스트림 연결은 반드시 `wss://`(SSL)을 사용해야 합니다. 공개 서비스는 비SSL 연결을 거부해야 합니다.

대부분의 HTTP XRPC 엔드포인트는 JSON 형식의 콘텐츠를 사용하지만, 스트림 엔드포인트는 신뢰할 수 없는 입력으로서 DAG-CBOR 객체를 직접 처리합니다. 따라서 악의적인 데이터 인코딩이나 데이터 구조 조작에 대비한 주의가 필요하며, 구체적인 내용은 [데이터 모델](/specs/data-model) 및 [레포지토리](/specs/repository) 명세에서 다룹니다.

## 향후 변경 가능 사항

이벤트 스트림은 AT Protocol의 가장 최신 구성 요소 중 하나로, 다른 구성 요소에 비해 세부 사항이 자주 변경될 가능성이 있습니다.

시퀀스 번호 체계는 샤딩된 스트림을 더 효과적으로 지원하기 위해 조정될 수 있습니다. 이는 공개 인터넷 상에서 더 높은 데이터 처리량을 다루기 위함입니다.

WebSocket 이외의 추가 전송 방식이나, DAG-CBOR 이외의 다른 인코딩 방식이 명세될 수 있습니다. 예를 들어, 텍스트 WebSocket 프레임 내의 JSON 페이로드는 브라우저에서 디코딩하기 더 간단할 수 있습니다.

추가적인 WebSocket 기능이 채택될 수 있습니다:

- `permessage-deflate`와 같은 전송 압축 "확장"
- 서브 프로토콜 정의
- 양방향 메시징
- 1000 클래스 응답 코드

명세 내 모호한 부분은 해결되거나 그대로 열어둘 수 있습니다. 예를 들어:

- HTTP 리다이렉트
- 브라우저 연결을 위한 CORS 및 기타 문제
- 최대 메시지/프레임 크기

정규 HTTP XRPC 엔드포인트와 유사하게 인증 방식이 지원될 수 있습니다.


---
atproto/src/app/[locale]/specs/event-stream/page.tsx
---
export const metadata = {
  title: 'Event Stream',
  description:
    'Network wire protocol for subscribing to a stream of Lexicon objects',
  wip: true,
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/nsid/en.mdx
---
export const metadata = {
  title: 'Namespaced Identifiers (NSIDs)',
  description:
    'A specification for global semantic IDs.',
}

# Namespaced Identifiers (NSIDs)

Namespaced Identifiers (NSIDs) are used to reference Lexicon schemas for records, XRPC endpoints, and more. {{ className: 'lead' }}

The basic structure and semantics of an NSID are a fully-qualified hostname in Reverse Domain-Name Order, followed by a simple name. The hostname part is the **domain authority,** and the final segment is the **name**. {{ className: 'lead' }}


### NSID Syntax

Lexicon string type: `nsid`

The domain authority part of an NSID must be a valid handle with the order of segments reversed. That is followed by a name segment which must be an ASCII camel-case string.

For example, `com.example.fooBar` is a syntactically valid NSID, where `com.example` is the domain authority, and `fooBar` is the name segment.

The comprehensive list of syntax rules is:

- Overall NSID:
    - must contain only ASCII characters
    - separate the domain authority and the name by an ASCII period character (`.`)
    - must have at least 3 segments
    - can have a maximum total length of 317 characters
- Domain authority:
    - made of segments separated by periods (`.`)
    - at most 253 characters (including periods), and must contain at least two segments
    - each segment must have at least 1 and at most 63 characters (not including any periods)
    - the allowed characters are ASCII letters (`a-z`), digits (`0-9`), and hyphens (`-`)
    - segments can not start or end with a hyphen
    - the first segment (the top-level domain) can not start with a numeric digit
    - the domain authority is not case-sensitive, and should be normalized to lowercase (that is, normalize ASCII `A-Z` to `a-z`)
- Name:
    - must have at least 1 and at most 63 characters
    - the allowed characters are ASCII letters and digits only (`A-Z`, `a-z`, `0-9`)
    - hyphens are not allowed
    - the first character can not be a digit
    - case-sensitive and should not be normalized

A reference regex for NSID is:

```
/^[a-zA-Z]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)+(\.[a-zA-Z]([a-zA-Z0-9]{0,62})?)$/
```


### NSID Syntax Variations

A **fragment** may be appended to an NSID in some contexts to refer to a specific sub-field within the schema. The fragment is separated from the NSID by an ASCII hash character (`#`). The fragment identifier string (after the `#`) has the same syntax restrictions as the final segment of an NSID: ASCII alphabetic, one or more characters, length restricted, etc.

When referring to a group or pattern of NSIDs, a trailing ASCII star character (`*`) can be used as a "glob" character. For example, `com.atproto.*` would refer to any NSIDs under the `atproto.com` domain authority, including nested sub-domains (sub-authorities). A free-standing `*` would match all NSIDs from all authorities. Currently, there may be only a single start character; it must be the last character; and it must be at a segment boundary (no partial matching of segment names). This means the start character must be proceeded by a period, or be a bare star matching all NSIDs.


### Examples

Syntactically valid NSIDs:

```
com.example.fooBar
net.users.bob.ping
a-0.b-1.c
a.b.c
com.example.fooBarV2
cn.8.lex.stuff
```

Invalid NSIDs:

```
com.exa💩ple.thing
com.example
com.example.3
```


### Usage and Implementation Guidelines

A **strongly-encouraged** best practice is to use authority domains with only ASCII alphabetic characters (that is, no digits or hyphens). This makes it significantly easier to generate client libraries in most programming languages.

The overall NSID is case-sensitive for display, storage, and validation. However, having multiple NSIDs that differ only by casing is not allowed. Namespace authorities are responsible for preventing duplication and confusion. Implementations should not force-lowercase NSIDs.

It is common to use "subdomains" as part of the "domain authority" to organize related NSIDs. For example, the NSID `com.atproto.sync.getHead` uses the `sync` segment. Note that this requires control of the full domain `sync.atproto.com`, in addition to the domain `atproto.com`.

Lexicon language documentation will provide style guidelines on choosing and organizing NSIDs for both record types and XRPC methods. In short, records are usually single nouns, not pluralized. XRPC methods are usually in "verbNoun" form.


### Possible Future Changes

It is conceivable that NSID syntax would be relaxed to allow Unicode characters in the final segment.

The "glob" syntax variation may be modified to extended to make the distinction between single-level and nested matching more explicit.

The "fragment" syntax variation may be relaxed in the future to allow nested references.

No automated mechanism for verifying control of a "domain authority" currently exists. Also, not automated mechanism exists for fetching a lexicon schema for a given NSID, or for enumerating all NSIDs for a base domain.


---
atproto/src/app/[locale]/specs/nsid/ko.mdx
---
export const metadata = {
  title: 'Namespaced Identifiers (NSIDs)',
  description: '전역 의미 체계 식별자에 대한 명세입니다.',
}

# Namespaced Identifiers (NSIDs)

Namespaced Identifiers (NSIDs)는 레코드, XRPC 엔드포인트 등 다양한 Lexicon 스키마를 참조하는 데 사용됩니다. {{ className: 'lead' }}

NSID의 기본 구조와 의미는 역순 도메인 네임 형식의 완전한 호스트 이름 다음에 간단한 이름이 이어지는 형태입니다. 호스트 이름 부분은 **도메인 권한**을 나타내며, 마지막 세그먼트는 **이름**을 나타냅니다. {{ className: 'lead' }}

### NSID 구문

Lexicon 문자열 타입: `nsid`

NSID의 도메인 권한 부분은 역순으로 배열된 유효한 핸들이어야 합니다. 그 뒤에는 ASCII 카멜케이스 문자열이어야 하는 이름 세그먼트가 이어집니다.

예를 들어, `com.example.fooBar`는 구문상으로 유효한 NSID로, `com.example`는 도메인 권한이며, `fooBar`는 이름 세그먼트입니다.

구문 규칙의 전체 목록은 다음과 같습니다:

- **전체 NSID:**
    - 오직 ASCII 문자만 포함해야 함
    - 도메인 권한과 이름은 ASCII 마침표 문자(`.`)로 구분되어야 함
    - 최소 3개의 세그먼트를 포함해야 함
    - 총 길이가 최대 317자여야 함
- **도메인 권한:**
    - 마침표(`.`)로 구분된 세그먼트로 구성됨
    - (마침표 포함) 최대 253자이며, 최소 두 개의 세그먼트를 포함해야 함
    - 각 세그먼트는 최소 1자, 최대 63자여야 함 (마침표 제외)
    - 허용되는 문자는 ASCII 알파벳(`a-z`), 숫자(`0-9`), 하이픈(`-`)임
    - 세그먼트는 하이픈으로 시작하거나 끝날 수 없음
    - 첫 번째 세그먼트(최상위 도메인)는 숫자로 시작할 수 없음
    - 도메인 권한은 대소문자를 구분하지 않으며, 소문자로 정규화되어야 함 (즉, ASCII `A-Z`를 `a-z`로 변환)
- **이름:**
    - 최소 1자, 최대 63자여야 함
    - 허용되는 문자는 ASCII 알파벳(`A-Z`, `a-z`)만 가능함
    - 숫자와 하이픈은 허용되지 않음
    - 대소문자를 구분하며 정규화해서는 안 됨

NSID에 대한 참조 정규 표현식은:

```
/^[a-zA-Z]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)+(\.[a-zA-Z]([a-zA-Z]{0,61}[a-zA-Z])?)$/
```

### NSID 구문 변형

일부 상황에서는 스키마 내 특정 하위 필드를 참조하기 위해 NSID에 **프래그먼트**를 추가할 수 있습니다. 프래그먼트는 NSID와 ASCII 해시 문자(`#`)로 구분됩니다. 프래그먼트 식별자 문자열(즉, `#` 뒤의 부분)은 NSID의 마지막 세그먼트와 동일한 구문 제한을 가집니다: ASCII 알파벳, 하나 이상의 문자, 길이 제한 등.

NSID의 그룹이나 패턴을 참조할 때는, 끝에 ASCII 별표 문자(`*`)를 "글로브(glob)" 문자로 사용할 수 있습니다. 예를 들어, `com.atproto.*`는 `atproto.com` 도메인 권한 아래의 모든 NSID(중첩된 하위 도메인 포함)를 참조합니다. 독립적인 `*`는 모든 도메인 권한의 모든 NSID와 매칭됩니다. 현재는 단 하나의 별표 문자만 허용되며, 반드시 마지막 문자여야 하고, 세그먼트 경계에 있어야 합니다 (세그먼트 이름의 일부만 매칭하는 것은 불가능). 즉, 별표 문자는 마침표 뒤에 오거나, 모든 NSID와 매칭되는 독립된 별표여야 합니다.

### 예시

구문상 유효한 NSID:

```
com.example.fooBar
net.users.bob.ping
a-0.b-1.c
a.b.c
cn.8.lex.stuff
```

유효하지 않은 NSID:

```
com.exa💩ple.thing
com.example
```

### 사용 및 구현 지침

**강력히 권장되는** 최선의 관행은 도메인 권한에 ASCII 알파벳 문자만 사용하는 것입니다 (즉, 숫자나 하이픈을 사용하지 않음). 이는 대부분의 프로그래밍 언어에서 클라이언트 라이브러리를 생성하는 데 크게 용이합니다.

전체 NSID는 표시, 저장, 검증에 있어서 대소문자를 구분합니다. 그러나 대소문자만 다른 여러 NSID를 허용해서는 안 됩니다. 네임스페이스 권한은 중복과 혼란을 방지할 책임이 있습니다. 구현체는 NSID를 강제로 소문자로 변환해서는 안 됩니다.

관련 NSID를 체계적으로 구성하기 위해 도메인 권한의 일부로 "서브도메인"을 사용하는 것이 일반적입니다. 예를 들어, NSID `com.atproto.sync.getHead`는 `sync` 세그먼트를 사용합니다. 이는 `atproto.com` 도메인뿐만 아니라 `sync.atproto.com` 전체 도메인에 대한 제어 권한이 필요함을 의미합니다.

Lexicon 언어 문서에는 레코드 유형 및 XRPC 메서드에 대해 NSID를 선택하고 구성하는 스타일 가이드라인이 제공될 것입니다. 요약하면, 레코드는 보통 복수형이 아닌 단수 명사이며, XRPC 메서드는 보통 "동사명사(verbNoun)" 형태를 취합니다.

### 향후 변경 가능성

NSID 구문이 마지막 세그먼트에 유니코드 문자를 허용하도록 완화될 가능성이 있습니다.

"글로브(glob)" 구문 변형은 단일 레벨과 중첩 매칭의 구분을 보다 명확하게 하기 위해 확장될 수 있습니다.

"프래그먼트(fragment)" 구문 변형은 향후 중첩 참조를 허용하도록 완화될 수 있습니다.

현재 "도메인 권한"의 제어를 검증하는 자동화된 메커니즘은 존재하지 않습니다. 또한, 주어진 NSID에 대한 Lexicon 스키마를 가져오거나 기본 도메인의 모든 NSID를 열거하는 자동화된 메커니즘도 존재하지 않습니다.


---
atproto/src/app/[locale]/specs/nsid/page.tsx
---
export const metadata = {
  title: 'Namespaced Identifiers (NSIDs)',
  description: 'A specification for global semantic IDs.',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/sync/en.mdx
---
export const metadata = {
  title: 'Sync',
  description:
    'Firehose and other data synchronization mechanisms.',
}

# Data Synchronization

One of the main design goals of atproto (the "Authenticated Transfer Protocol") is to reliably distribute public content between independent network services. This data transfer should be trustworthy (cryptographicly authenticated) and relatively low-latency even at large scale. It is also important that new participants can join the network at any time and “backfill” prior content.

This section describes the major data synchronization features in atproto. The primary real-time data synchronization mechanism is repository event streams, commonly referred to as "firehoses". The primary batch data transfer mechanism is repository exports as CAR files. These two mechanisms can be combined in a "bootstrapping" process which result in a live-synchronized copy of the network.

<Note>
An updated version of the Synchronization protocol is being rolled out to the live network in early 2025. There is a ["Sync v1.1" proposal document](https://github.com/bluesky-social/proposals/tree/main/0006-sync-iteration) and an [update blog post](https://docs.bsky.app/blog/relay-sync-updates) with deployment details. The written specifications will be updated soon.
</Note>

## Synchronization Primitives

As described in the repository spec, each commit to a repository has a *revision* number, in TID syntax. The revision number must always increase between commits for the same account, even if the account is migrated between hosts or has a period of inactivity in the network. Revision numbers can be used as a logical clock to aid synchronization of individual accounts. To keep this simple, it is recommended to use the current time as a TID for each commit, including the initial commit when creating a new account. Services should reject or ignore revision numbers corresponding to future timestamps (beyond a short fuzzy time drift window). Network services can track the commit revision for every account they have seen, and use this to verify synchronization progress. Services which synchronize data can include the most-recently-processed revision in HTTP responses to API requests from the account in question, in the `Atproto-Repo-Rev` HTTP response header. This allows clients (and users) to detect if the response is up-to-date with the actual repository, and detect any problems with synchronization.

## Firehose

The repository event stream (`com.atproto.sync.subscribeRepos`, also called the "firehose") is an [Event Stream](/specs/event-stream) which broadcasts updates to repositories (`#commit` events), handles and DID documents (`#identity`), and account hosting status (`#account`). PDS hosts provide a single stream with updates for all locally-hosted accounts. "Relays" are network services which subscribe to one or more repo streams (eg, multiple PDS instances) and aggregate them in to a single combined repo stream. The combined stream has the same structure and event types. A Relay which aggregates nearly all accounts from nearly all PDS instances in the network (possibly through intermediate relays) outputs a “full-network” firehose.  Relays often mirror and can re-distribute the repository contents, though their core functionality is to verify content and output a unified firehose.

In most cases the repository data synchronized over the firehose is self-certifying (contains verifiable signatures), and consumers can verify content without making additional requests directly to account PDS instances. It is possible for services to redact events from the firehose, such that downstream services would not be aware of new content.

Identity and account information is *not* self-certifying, and services may need need to verify independently. This usually means independent DID and [handle resolution](/specs/handle). Account hosting status might also be checked at account PDS hosts, to disambiguate hosting status at different pieces of infrastructure.

The event message types are declared in the `com.atproto.sync.subscribeRepos` Lexicon schema, and are summarized below. A few fields are the same for all event types (except for `repo` vs `did` for `#commit` events):

- `seq` (integer, required): used to ensure reliable consumption, as described in Event Streams
- `did` / `repo`(string with DID syntax, required): the account/identity associated with the event
- `time` (string with datetime syntax, required): an informal and non-authoritative estimate of when event was received. Intermediate services may decide to pass this field through as-is, or update to the current time

### `#identity` Events

Indicates that there *may* have been a change to the indicated identity (meaning the DID document or handle), and optionally what the current handle is. Does not indicate what changed, or reliably indicate what the current state of the identity is.

Event fields:

- `seq` (integer, required): same for all event types
- `did` (string with DID syntax, required): same for all event types
- `time` (string with datetime syntax, required): same for all event types
- `handle` (string with handle syntax, optional): the current handle for this identity. May be `handle.invalid` if the handle does not currently resolve correctly.

Presence or absence of the `handle` field does not indicate that it is the handle which has changed.

The semantics and expected behavior are that downstream services should update any cached identity metadata (including DID document and handle) for the indicated DID. They might mark caches as stale, immediately purge cached data, or attempt to re-resolve metadata.

Identity events are emitted on a "best-effort" basis. It is possible for the DID document or handle resolution status to change without any atproto service detecting the change, in which case an event would not be emitted. It is also possible for the event to be emitted redundantly, when nothing has actually changed.

Intermediate services (eg, relays) may chose to modify or pass through identity events:

- they may replace the handle with the result of their own resolution; or always remove the handle field; or always pass it through unaltered
- they may filter out identity events if they observe that identity has not actually changed
- they may emit identity events based on changes they became aware of independently (eg, via periodic re-validation of handles)

### `#account` Events

Indicates that there may have been a change in [Account Hosting status](/specs/account) at the service which emits the event, and what the new status is. For example, it could be the result of creation, deletion, or temporary suspension of an account. The event describes the current hosting status, not what changed.

Event Fields:

- `seq` (integer, required): same for all event types
- `did` (string with DID syntax, required): same for all event types
- `time` (string with datetime syntax, required): same for all event types
- `active` (boolean, required): whether the repository is currently available and can be redistributed
- `status` (string, optional): string status code which describes the account state in more detail. Known values include:
    - `takendown`: indefinite removal of the repository by a service provider, due to a terms or policy violation
    - `suspended`: temporary or time-limited variant of `takedown`
    - `deleted`: account has been deactivated, possibly permanently.
    - `deactivated`: temporary or indefinite removal of all public data by the account themselves.

When coming from any service which redistributes account data, the event describes what the new status is *at that service*, and is authoritative in that context. In other words, the event is hop-by-hop for repository hosts and mirrors.

See the Account Hosting specification for more details.

### `#commit` Events

This event indicates that there has been a new repository commit for the indicated account. The event usually contains the "diff" of repository data, in the form of a CAR slice. See the [Repository specification](/specs/repository) for details on "diffs" and the CAR file format.

See the Repository specification for more details around repo diffs.

Event Fields:

- `seq` (integer, required): same for all event types
- `repo` (string with DID syntax, required): the same as `did` for all other event types
- `time` (string with datetime syntax, required): same for all event types
- `rev` (string with TID syntax, required): the revision of the commit. Must match the `rev` in the commit block itself.
- `since` (string with TID syntax, nullable): indicates the `rev` of a preceding commit, which the the repo diff contains differences from
- `commit` (cid-link, required): CID of the commit object (in `blocks`)
- `tooBig` (boolean, required): if true, indicates that the repo diff was too large, and that `blocks`, `ops`, and complete `blobs` are not all included
- `blocks` (bytes, required): CAR "slice" for the corresponding repo diff. The commit object must always be included.
- `ops` (array of objects, required): list of record-level operations in this commit: specific records created, updated, deleted
- `blobs` (array of cid-link, required): set of new blobs (by CID) referenced by records in this commit

Commit events are broadcast when the account repository changes. Commits can be "empty", meaning no actual record content changed, and only the `rev` was incremented. They can contain a single record update, or multiple updates. Only the commit object, record blocks, and MST tree nodes are authenticated (signed): the `since`, `ops`, `blobs`, and `tooBig` fields are not self-certifying, and could in theory be manipulated, or otherwise be incorrect or incomplete.

If `since` is not included, the commit should include the full repo tree, or set the `tooBig` flag.

If the `tooBig` flag is set, then the amount of updated data was too much to be serialized in a single stream event message. Downstream services which want to maintain complete synchronized copies for the repo need to fetch the diff separately, as discussed below.

### Firehose Validation Best Practices

A service which does full validation of upstream events has a number of properties to track and check. For example, Relay instances should fully validate content from PDS instances before re-broadcasting.

Here is a summary of validation rules and behaviors:

- services should independently resolve identity data for each DID. They should ignore `#commit` events for accounts which do not have a functioning atproto identity (eg, lacking a signing key, or lacking a PDS service entry, or for which the DID has been tombstoned)
- services which subscribe directly to PDS instances should keep track of which PDS is authoritative for each DID. They should remember the host each subscription (WebSocket) is connected to, and reject `#commit` events for accounts if they come from a stream which does not correspond to the current account for that DID
- services should track account hosting status for each DID, and ignore `#commit` events for events which are not `active`
- services should verify commit signatures for each `#commit` event, using the current identity data. If the signature initially fails to verify, the service should refresh the identity metadata in case it had recently changed. Events with conclusively invalid signatures should be rejected.
- services should reject any event messages which exceed reasonable limits. A reasonable upper bound for producers is 5 MBytes (for any event stream message type). The `subscribeRepos` Lexicon also limits `blocks` to one million bytes, and `ops` to 200 entries. Commits with too much data must use the `tooBig` mechanism, though such commits should generally be avoided in the first place by breaking them up in to multiple smaller commits.
- services should verify that repository data structures are valid against the specification. Missing fields, incorrect MST structure, or other protocol-layer violations should result in events being rejected.
- services may apply rate-limits to identity, account, and commit events, and suspend accounts or upstream services which violate those limits. Rate limits might also be applied to recovery modes such as invalid signatures resulting in an identity refresh, `tooBig` events, missing or out-of-order commits, etc.
- services should ignore commit events with a `rev` lower or equal to the most recent processed `rev` for that DID, and should reject commit events with a `rev` corresponding to a future timestamp (beyond a clock drift window of a few minutes)
- services should check the `since` value in commit events, and if it is not consistent with the previous seen `rev` for that DID (see discussion in "Reliable Synchronization"), mark the repo as out-of-sync (similar to a `tooBig` commit event)
- data limits on records specifically should be verified. Events containing corrupt or entirely invalid records may be rejected. for example, a record not being CBOR at all, or exceeding normal data size limits.
- more subtle data validation of records may be enforced, or may be ignored, depending on the service. For example, unsupported CID hash types embedded in records should probably be ignored by Relays (even if they violate the atproto data model), but may result in the record or commit event being rejected by an AppView
- mirroring services, which retain a full copy of repository data, should verify that commit diffs leave the MST tree in a complete and valid state (eg, no missing records, no invalid MST nodes, commit CID would be reproducible if the MST structure was re-generated from scratch)
- Relays (specifically) should not validate records against Lexicons

## Reliable Synchronization

This section describes some details on how to reliably subscribe to the firehose and maintain an existing synchronized mirror of the network.

Services should generally maintain a few pieces of state for all accounts they are tracking data from:

- track the most recent commit `rev` they have successfully processed
- keep cached identity data, and use cache expiration to ensure periodic re-validation of that data
- track account status

Identity caches should be purged any time an `#identity` event is received. Additionally, identity resolution should be refreshed if a commit signature fails to verify, in case the signing key was updated but the identity cache has not been updated yet.

When `tooBig` events are emitted on the firehose, downstream services will need to fetch the diff out-of-band. This usually means an API request to the `com.atproto.sync.getRepo` endpoint on the current PDS host for the account, with the `since` field included. The `since` value should be the most recently processed `rev` value for the account, which may or may not match the `since` field in the commit event message.

If a `#commit` is received with a `since` that does not match the most recently processed `rev` for the account, and is “later” (higher value) than the most recent commit `rev` the service has processed for that account, the service may need to do the same kind of out-of-band fetch as for a `tooBig` event.

Services should keep track of the `seq` number of their upstream subscriptions. This should be stored separately per-upstream, even if there is only a single Relay connection, in case a different Relay is subscribed to in the future (which will have different `seq` numbers).

Events can be processed concurrently, but they should be processed sequentially in-order for any given account. This can be accomplished by partitioning multiple workers using the repo DID as a partitioning key.

Services can confirm that they are consuming content reliably by fetching a snapshot of repository DIDs and `rev` numbers from other services, including PDS hosts and Relay instances. After a short delay, these can be compared against the current state of the service to identify any accounts which have lower than expected `rev` numbers. These repos can then be updated out-of-band.

## Bootstrapping a Live Mirror

The firehose can be used to follow new data updates, and repo exports can be used for snapshots. Actually combining the two to bootstrap a complete live-updating mirror can be a bit tricky. One approach is described below.

Keep a sync status table for all accounts (DIDs) encountered. The status can be:

- `dirty`: there is either no local repo data for this account, or it has gotten out of sync
- `in-process`: the repo is "dirty", but there is a background task in process to update it
- `synchronized`: a complete copy of the repository has been processed

Start by subscribing to the full firehose. If there is no existing repository data for the account, mark the account as "dirty". When new events come in for a repo, the behavior depends on the repo state. If it is "dirty", the event is ignored. If the state is "synchronized", the event is immediately processed as an update to the repo. If the state is "in-process", the event is enqueued locally.

Have a set of background workers start processing "dirty" repos. First they mark the status as `in-process`, so that new events are enqueued locally. Then the full repo export (CAR file) is fetched from the PDS and processed in full. The commit `rev` of the repo export is noted. When the full repo import is complete, the worker can start processing any enqueued events, in order, skipping any with a `rev` lower than the existing repo processed `rev` (as is the usual behavior). When the queue for the account is fully processed, the state can be flipped to `synchronized`, and the worker can move on.

After some time, most of the known accounts will be marked as `synchronized`, though this will only represent the most recently active accounts in the network. Next a more complete set of repositories in the network can be fetched, for example using an API query against an existing large service. Any new identified accounts can be marked as `dirty` in the service, and the background workers can start processing them.

When all of the accounts are `synchronized`, the process is complete. At large scale it may be hard to get perfect synchronization: PDS instances may be down at various times, identities may fail to resolve, or invalid events, data, or signatures may end up in the network.

## Usage and Implementation Guidelines

Guidelines for specific firehose event sequencing during different account events are described in an [Account Lifecycle Best Practices guide](/guides/account-lifecycle).

## Security Concerns

General mitigations for resource exhaustion attacks are recommended: event rate-limits, data quotas per account, limits on data object sizes and deserialized data complexity, etc.

Care should always be taken when making network requests to unknown or untrusted hosts, especially when the network locators for those host from from untrusted input. This includes validating URLs to not connect to local or internal hosts (including via HTTP redirects), avoiding SSRF in browser contexts, etc.

To prevent traffic amplification attacks, outbound network requests should be rate-limited by host. For example, identity resolution requests when consuming from the firehose, including DNS TXT traffic volume and DID resolution requests.

## Future Work

The `subscribeRepos` Lexicon is likely to be tweaked, with deprecated fields removed, even if this breaks Lexicon evolution rules.

The event stream sequence/cursor scheme may be iterated on to support sharding, timestamp-based resumption, and easier failover between independent instances.

Alternatives to the full authenticated firehose may be added to the protocol. For example, simple JSON serialization, filtering by record collection type, omitting MST nodes, and other changes which would simplify development and reduce resource consumption for use-cases where full authentication is not necessary or desired.


---
atproto/src/app/[locale]/specs/sync/ko.mdx
---
export const metadata = {
  title: '동기화',
  description:
    'Firehose 및 기타 데이터 동기화 메커니즘.',
}

# 데이터 동기화

atproto(즉, "Authenticated Transfer Protocol")의 주요 설계 목표 중 하나는 독립적인 네트워크 서비스 간에 공개 콘텐츠를 안정적으로 분배하는 것입니다. 이 데이터 전송은 신뢰할 수 있어야 하며(암호학적으로 인증됨) 대규모 환경에서도 비교적 낮은 지연(latency)을 유지해야 합니다. 또한 새로운 참가자가 언제든지 네트워크에 참여하여 이전 콘텐츠를 “백필(backfill)”할 수 있어야 합니다.

이 섹션에서는 atproto의 주요 데이터 동기화 기능에 대해 설명합니다. 기본 실시간 데이터 동기화 메커니즘은 리포지토리 이벤트 스트림(일명 "firehose")이며, 기본 배치 데이터 전송 메커니즘은 CAR 파일 형태의 리포지토리 내보내기입니다. 이 두 메커니즘은 결합되어 네트워크의 실시간 동기화 미러(복제본)를 부트스트래핑하는 과정으로 활용될 수 있습니다.

## 동기화 기본 요소

레포지토리 명세에 설명된 바와 같이, 각 리포지토리 커밋에는 TID 형식의 *revision* 번호가 있습니다. 동일 계정에 대한 커밋 간에는, 계정이 호스트 간에 마이그레이션되거나 네트워크에서 활동이 없더라도 revision 번호는 항상 증가해야 합니다. revision 번호는 개별 계정의 동기화를 돕기 위한 논리적 시계로 사용할 수 있습니다. 이를 단순하게 유지하기 위해, 새로운 계정을 생성할 때의 초기 커밋을 포함하여 각 커밋에 대해 현재 시간을 TID로 사용하는 것이 권장됩니다. 서비스는 짧은 오차 범위를 넘어서는 미래 타임스탬프에 해당하는 revision 번호를 거부하거나 무시해야 합니다. 네트워크 서비스는 자신이 관찰한 모든 계정의 커밋 revision을 추적하여 동기화 진행 상황을 검증할 수 있습니다. 데이터를 동기화하는 서비스는 관련 계정의 API 응답 헤더(`Atproto-Repo-Rev`)에 가장 최근에 처리한 revision을 포함시켜, 클라이언트(및 사용자)가 응답이 실제 리포지토리와 최신 상태인지, 동기화에 문제가 없는지를 확인할 수 있도록 할 수 있습니다.

## Firehose

리포지토리 이벤트 스트림(`com.atproto.sync.subscribeRepos`, 일명 "firehose")은 리포지토리 업데이트(`#commit` 이벤트), 핸들 및 DID 문서(`#identity`), 그리고 계정 호스팅 상태(`#account`)를 방송하는 [이벤트 스트림](/specs/event-stream)입니다. PDS 호스트는 로컬에 호스팅된 모든 계정에 대한 업데이트를 포함하는 단일 스트림을 제공합니다. "릴레이"는 하나 이상의 리포 스트림(예: 여러 PDS 인스턴스)을 구독하여 이를 단일 통합 리포 스트림으로 집계하는 네트워크 서비스입니다. 통합 스트림은 동일한 구조와 이벤트 타입을 유지합니다. 네트워크 내 거의 모든 PDS 인스턴스(중간 릴레이를 통해서도 가능)로부터 거의 모든 계정을 집계하는 릴레이는 “전체 네트워크” firehose를 출력합니다. 릴레이는 종종 리포지토리 내용을 미러링하거나 재배포할 수 있지만, 그 핵심 기능은 콘텐츠를 검증하고 통합된 firehose를 제공하는 것입니다.

대부분의 경우 firehose를 통해 동기화되는 리포지토리 데이터는 자기인증(self-certifying)되어(검증 가능한 서명을 포함) 소비자가 별도의 계정 PDS 인스턴스에 추가 요청 없이도 콘텐츠를 검증할 수 있습니다. 다만, 신원(identity) 및 계정 정보는 자기인증되지 않으므로 서비스가 별도로 검증해야 할 수도 있습니다. 이는 보통 독립적인 DID 및 [핸들 확인](/specs/handle)을 의미하며, 계정 호스팅 상태 또한 인프라의 서로 다른 구성 요소 간에 혼동이 없도록 계정 PDS 호스트에서 확인할 수 있습니다.

이벤트 메시지 타입은 `com.atproto.sync.subscribeRepos` Lexicon 스키마에 선언되어 있으며, 아래에 요약되어 있습니다. 몇몇 필드는 모든 이벤트 타입에서 동일합니다 (단, `#commit` 이벤트의 경우 `repo`와 `did`가 다릅니다):

- `seq` (정수, 필수): 이벤트 스트림에 설명된 대로 안정적인 소비를 보장하기 위해 사용됨
- `did` / `repo` (DID 문법의 문자열, 필수): 이벤트와 관련된 계정/신원
- `time` (날짜시간 문자열, 필수): 이벤트가 수신된 대략적인 시각(비공식적이며 권위 있는 정보는 아님). 중간 서비스는 이 필드를 그대로 전달하거나 현재 시각으로 업데이트할 수 있음

### `#identity` 이벤트

지정된 신원(즉, DID 문서나 핸들)에 변경이 *있었을 수도 있음*을 나타내며, 선택적으로 현재 핸들이 무엇인지 포함할 수 있습니다. 단, 어떤 부분이 변경되었는지 또는 신원의 현재 상태가 무엇인지를 확실하게 나타내지는 않습니다.

**이벤트 필드:**

- `seq` (정수, 필수): 모든 이벤트 타입에서 동일
- `did` (DID 문법의 문자열, 필수): 모든 이벤트 타입에서 동일
- `time` (날짜시간 문자열, 필수): 모든 이벤트 타입에서 동일
- `handle` (핸들 문법의 문자열, 선택): 해당 신원의 현재 핸들. 만약 핸들이 올바르게 확인되지 않을 경우 `handle.invalid`일 수 있음

`handle` 필드의 존재 여부는 해당 핸들이 변경되었음을 나타내지 않습니다.

의미와 예상 동작은 다운스트림 서비스가 해당 DID에 대해 캐시된 신원 메타데이터(예: DID 문서 및 핸들)를 업데이트해야 한다는 것입니다. 이들은 캐시를 만료 상태로 표시하거나 즉시 삭제하거나 메타데이터를 재확인할 수 있습니다.

신원 이벤트는 "best-effort" 방식으로 발행됩니다. 어떤 atproto 서비스도 변경을 감지하지 못한 채 DID 문서나 핸들 확인 상태가 변경될 수 있으며, 이 경우 이벤트가 발행되지 않을 수 있습니다. 또한 실제로 아무 것도 변경되지 않았더라도 중복으로 이벤트가 발행될 수 있습니다.

중간 서비스(예: 릴레이)는 다음과 같이 신원 이벤트를 수정하거나 그대로 전달할 수 있습니다:

- 자체 확인 결과로 핸들을 대체하거나, 항상 핸들 필드를 제거하거나, 변경 없이 그대로 전달
- 신원이 실제로 변경되지 않았음을 확인하면 신원 이벤트를 필터링
- 독립적으로 인지한 변경 사항(예: 핸들의 주기적 재검증)을 바탕으로 신원 이벤트를 발행

### `#account` 이벤트

이 이벤트는 이벤트를 발행한 서비스에서 [계정 호스팅 상태](/specs/account)에 변화가 있었을 수 있으며, 새로운 상태가 무엇인지를 나타냅니다. 예를 들어, 계정의 생성, 삭제 또는 일시 중단의 결과일 수 있습니다. 이벤트는 변경된 내용을 설명하는 것이 아니라 현재의 호스팅 상태를 나타냅니다.

**이벤트 필드:**

- `seq` (정수, 필수): 모든 이벤트 타입에서 동일
- `did` (DID 문법의 문자열, 필수): 모든 이벤트 타입에서 동일
- `time` (날짜시간 문자열, 필수): 모든 이벤트 타입에서 동일
- `active` (불리언, 필수): 리포지토리가 현재 사용 가능하며 재배포 가능한지 여부
- `status` (문자열, 선택): 계정 상태를 좀 더 자세히 설명하는 상태 코드. 알려진 값:
  - `takendown`: 서비스 제공자가 약관 또는 정책 위반으로 인해 리포지토리를 무기한 제거함
  - `suspended`: `takendown`의 일시적 또는 시간 제한 변형
  - `deleted`: 계정이 비활성화되었으며, 영구적일 수 있음
  - `deactivated`: 계정 자체에 의해 모든 공개 데이터가 일시적 또는 무기한 제거됨

계정 데이터를 재배포하는 서비스에서 발행된 경우, 이벤트는 해당 서비스에서의 새로운 상태를 나타내며 그 맥락에서 권위적입니다. 즉, 이 이벤트는 리포지토리 호스트 및 미러 간에 홉 단위로 전달됩니다.

자세한 내용은 [계정 호스팅 명세](/specs/account)를 참조하세요.

### `#commit` 이벤트

이 이벤트는 지정된 계정에 대해 새로운 리포지토리 커밋이 있었음을 나타냅니다. 이벤트는 보통 CAR 슬라이스 형식의 리포지토리 데이터 "diff"를 포함합니다. "diff"와 CAR 파일 형식에 대한 자세한 내용은 [레포지토리 명세](/specs/repository)를 참조하세요.

**이벤트 필드:**

- `seq` (정수, 필수): 모든 이벤트 타입에서 동일
- `repo` (DID 문법의 문자열, 필수): 다른 이벤트 타입의 `did`와 동일
- `time` (날짜시간 문자열, 필수): 모든 이벤트 타입에서 동일
- `rev` (TID 형식의 문자열, 필수): 커밋의 revision (커밋 블록 내의 `rev`와 일치해야 함)
- `since` (TID 형식의 문자열, null 허용): 리포 diff가 차이를 포함하는 이전 커밋의 `rev`를 나타냄
- `commit` (CID 링크, 필수): 커밋 객체의 CID (`blocks` 내)
- `tooBig` (불리언, 필수): true인 경우, 리포 diff가 너무 커서 `blocks`, `ops`, 완전한 `blobs`가 모두 포함되지 않았음을 나타냄
- `blocks` (바이트, 필수): 해당 리포 diff에 대한 CAR "slice". 커밋 객체는 반드시 포함되어야 함
- `ops` (객체 배열, 필수): 이 커밋에서 발생한 레코드 수준 작업 목록 (생성, 업데이트, 삭제된 특정 레코드)
- `blobs` (CID 링크 배열, 필수): 이 커밋의 레코드에서 참조하는 새로운 blob들의 집합

계정 리포지토리에 변경이 발생하면 커밋 이벤트가 방송됩니다. 커밋은 "empty"일 수 있는데, 이는 실제 레코드 콘텐츠가 변경되지 않고 오직 `rev`만 증가했음을 의미합니다. 하나의 레코드 업데이트 또는 여러 업데이트를 포함할 수 있습니다. 인증(서명)되는 것은 커밋 객체, 레코드 블록, MST 트리 노드뿐이며, `since`, `ops`, `blobs`, `tooBig` 필드는 자기인증되지 않아 이론적으로 조작되거나 부정확 또는 불완전할 수 있습니다.

만약 `since`가 포함되지 않은 경우, 커밋은 전체 리포 트리를 포함하거나 `tooBig` 플래그를 설정해야 합니다.  
`tooBig` 플래그가 설정되면 업데이트된 데이터 양이 단일 스트림 이벤트 메시지에 직렬화하기에는 너무 많음을 의미하므로, 리포지토리의 완전한 동기화 복사본을 유지하려는 다운스트림 서비스는 diff를 별도로 가져와야 합니다.

### Firehose 검증 모범 사례

업스트림 이벤트를 완전히 검증하는 서비스는 여러 속성을 추적 및 확인해야 합니다. 예를 들어, 릴레이 인스턴스는 PDS 인스턴스로부터 받은 콘텐츠를 재방송하기 전에 완전히 검증해야 합니다.

다음은 검증 규칙 및 권장 동작에 대한 요약입니다:

- 서비스는 각 DID에 대해 신원 데이터를 독립적으로 확인해야 하며, 작동하는 atproto 신원이 없는 계정(예: 서명 키가 없거나, PDS 서비스 항목이 없거나, DID가 폐기된 경우)의 `#commit` 이벤트는 무시해야 합니다.
- PDS 인스턴스에 직접 구독하는 서비스는 각 DID에 대해 권한 있는 PDS를 추적해야 하며, 각 구독(WebSocket)이 연결된 호스트를 기억한 후, 해당 DID의 현재 계정과 일치하지 않는 스트림의 `#commit` 이벤트는 거부해야 합니다.
- 서비스는 각 DID의 계정 호스팅 상태를 추적하고, `active`가 아닌 이벤트의 `#commit`은 무시해야 합니다.
- 서비스는 각 `#commit` 이벤트에 대해 현재 신원 데이터를 사용하여 커밋 서명을 검증해야 합니다. 만약 서명 검증에 실패하면, 신원 메타데이터를 갱신하여 최근 변경되었는지 확인하고, 명백히 잘못된 서명의 이벤트는 거부해야 합니다.
- 서비스는 합리적인 한계를 초과하는 이벤트 메시지를 거부해야 합니다. 생산자에 대한 합리적 상한은 5MB이며, `subscribeRepos` Lexicon은 `blocks`를 1,000,000 바이트, `ops`를 200 항목으로 제한합니다. 데이터가 너무 많은 커밋은 `tooBig` 메커니즘을 사용해야 하며, 이러한 커밋은 가능하면 여러 개의 작은 커밋으로 분할해야 합니다.
- 서비스는 리포지토리 데이터 구조가 명세에 부합하는지 검증해야 하며, 누락된 필드, 잘못된 MST 구조, 또는 기타 프로토콜 위반 시 이벤트를 거부해야 합니다.
- 서비스는 신원, 계정, 커밋 이벤트에 대해 속도 제한을 적용하고, 제한을 위반하는 계정이나 업스트림 서비스를 일시 중단할 수 있습니다. 속도 제한은 잘못된 서명, `tooBig` 이벤트, 누락되거나 순서가 맞지 않는 커밋 등 복구 모드에도 적용될 수 있습니다.
- 서비스는 해당 DID에 대해 가장 최근에 처리한 `rev`보다 낮거나 같은 커밋 이벤트는 무시하고, 몇 분 정도의 시계 오차 범위를 벗어난 미래 타임스탬프의 `rev`를 가진 커밋 이벤트는 거부해야 합니다.
- 서비스는 커밋 이벤트의 `since` 값을 확인하여, 해당 DID에 대해 이전에 본 `rev`와 일치하지 않으면 리포지토리를 동기화되지 않은 상태로 표시해야 합니다(이는 `tooBig` 이벤트와 유사).
- 레코드에 대한 데이터 한계를 검증해야 하며, 손상되었거나 완전히 잘못된 레코드를 포함하는 이벤트는 거부될 수 있습니다 (예: 레코드가 CBOR 형식이 아니거나 정상적인 데이터 크기 제한을 초과하는 경우).
- 서비스에 따라 레코드의 미묘한 데이터 검증을 강제하거나 무시할 수 있습니다. 예를 들어, 레코드에 포함된 지원되지 않는 CID 해시 유형은 릴레이에서는 무시해야 하지만, AppView에서는 레코드나 커밋 이벤트를 거부할 수 있습니다.
- 리포지토리 데이터의 전체 복사본을 보유하는 미러링 서비스는 커밋 diff가 MST 트리를 완전하고 유효한 상태로 남기는지(예: 누락된 레코드 없음, 잘못된 MST 노드 없음, MST 구조를 재생성 시 커밋 CID가 재현 가능함)를 검증해야 합니다.
- 특히 릴레이는 Lexicon에 대해 레코드를 검증해서는 안 됩니다.

## 신뢰할 수 있는 동기화

이 섹션에서는 firehose를 안정적으로 구독하고 네트워크의 동기화된 미러를 유지하는 방법에 대해 설명합니다.

서비스는 일반적으로 데이터를 추적하는 모든 계정에 대해 다음과 같은 상태 정보를 유지해야 합니다:

- 가장 최근에 성공적으로 처리한 커밋 `rev`를 추적
- 캐시된 신원 데이터를 유지하고, 캐시 만료를 통해 주기적으로 해당 데이터를 재검증
- 계정 상태를 추적

어떤 `#identity` 이벤트가 수신될 때마다 신원 캐시는 삭제되어야 합니다. 또한, 커밋 서명 검증에 실패할 경우(예: 서명 키가 업데이트되었으나 캐시가 갱신되지 않은 경우) 신원 확인을 재실시해야 합니다.

firehose에서 `tooBig` 이벤트가 발행되면, 다운스트림 서비스는 대역 외에서 diff를 가져와야 합니다. 이는 보통 해당 계정의 현재 PDS 호스트의 `com.atproto.sync.getRepo` 엔드포인트에 `since` 필드를 포함한 API 요청을 의미합니다. 이때 `since` 값은 해당 계정에 대해 가장 최근에 처리한 `rev` 값이어야 하며, 이는 커밋 이벤트 메시지의 `since` 필드와 일치할 수도, 그렇지 않을 수도 있습니다.

만약 커밋 이벤트의 `since` 값이 해당 계정의 가장 최근 처리 `rev`와 일치하지 않고, 동시에 그 값이 현재 처리한 가장 최신 커밋 `rev`보다 “이후”(더 높은 값)라면, 서비스는 `tooBig` 이벤트와 동일하게 대역 외에서 diff를 가져와야 할 수도 있습니다.

서비스는 업스트림 구독의 `seq` 번호를 추적해야 하며, 이는 단일 릴레이 연결만 있더라도 업스트림별로 별도로 저장되어야 합니다(향후 다른 릴레이로 구독이 변경될 수 있으므로).

이벤트는 병렬로 처리될 수 있으나, 각 계정에 대해서는 순차적으로 순서에 맞게 처리되어야 합니다. 이는 repo DID를 파티션 키로 사용하여 여러 작업자를 분할함으로써 달성할 수 있습니다.

서비스는 PDS 호스트 및 릴레이 인스턴스 등 다른 서비스로부터 리포지토리 DID와 `rev` 번호의 스냅샷을 가져와 콘텐츠를 안정적으로 소비하고 있는지 확인할 수 있습니다. 짧은 지연 후, 이를 현재 상태와 비교하여 예상보다 낮은 `rev` 값을 가진 계정을 식별하고, 해당 리포지토리를 대역 외에서 업데이트할 수 있습니다.

## 실시간 미러 부트스트래핑

firehose는 새로운 데이터 업데이트를 실시간으로 반영하는 데, 리포지토리 내보내기는 스냅샷 용도로 사용할 수 있습니다. 이 둘을 결합하여 완전한 실시간 업데이트 미러를 부트스트래핑하는 것은 다소 까다로울 수 있습니다. 한 가지 접근 방식은 다음과 같습니다.

발견된 모든 계정(DID)에 대해 동기화 상태 테이블을 유지합니다. 상태는 다음과 같이 분류할 수 있습니다:

- `dirty`: 해당 계정에 로컬 리포지토리 데이터가 없거나 동기화가 깨진 상태
- `in-process`: 리포지토리가 "dirty"하지만, 이를 업데이트하기 위한 백그라운드 작업이 진행 중인 상태
- `synchronized`: 리포지토리의 완전한 복사본이 처리된 상태

전체 firehose에 구독을 시작합니다. 만약 계정에 기존 리포지토리 데이터가 없다면 해당 계정을 `dirty`로 표시합니다. 이후, 해당 리포지토리에 새로운 이벤트가 수신되면 계정의 상태에 따라 다음과 같이 처리합니다:

- 상태가 `dirty`인 경우: 이벤트를 무시
- 상태가 `synchronized`인 경우: 이벤트를 즉시 리포 업데이트로 처리
- 상태가 `in-process`인 경우: 이벤트를 로컬 큐에 저장

백그라운드 작업자 집합이 `dirty` 상태의 리포지토리를 처리하기 시작합니다. 우선 해당 계정의 상태를 `in-process`로 전환하여 새로운 이벤트가 큐에 저장되도록 한 후, PDS로부터 전체 리포 내보내기(CAR 파일)를 가져와 전체적으로 처리합니다. 이때 리포 내보내기의 커밋 `rev`를 기록합니다. 전체 리포 가져오기가 완료되면, 작업자는 큐에 저장된 이벤트를 순서대로 처리하며, 기존에 처리된 `rev`보다 낮은 이벤트는 건너뜁니다(일반적인 동작 방식). 해당 계정의 큐가 모두 처리되면 상태를 `synchronized`로 전환하고, 작업자는 다음 작업으로 넘어갑니다.

잠시 후, 대부분의 알려진 계정이 `synchronized`로 표시되겠지만, 이는 네트워크 내에서 가장 최근에 활동한 계정만을 나타낼 수 있습니다. 이후, 예를 들어 기존의 대규모 서비스를 대상으로 API 쿼리를 사용하여 네트워크 내의 보다 완전한 리포지토리 집합을 가져올 수 있습니다. 새로 확인된 계정은 `dirty`로 표시하고, 백그라운드 작업자가 이를 처리하도록 합니다.

모든 계정이 `synchronized` 상태가 되면 부트스트래핑 프로세스가 완료됩니다. 대규모 환경에서는 PDS 인스턴스 다운, 신원 확인 실패, 잘못된 이벤트나 데이터, 서명 등의 이유로 완벽한 동기화를 달성하기 어려울 수 있습니다.

## 사용 및 구현 가이드라인

계정 이벤트별 firehose 이벤트 시퀀싱에 대한 구체적인 가이드라인은 [계정 생애주기 모범 사례 가이드](/guides/account-lifecycle)에 설명되어 있습니다.

## 보안 우려사항

리소스 고갈 공격에 대한 완화 조치(예: 이벤트 속도 제한, 계정별 데이터 할당량, 데이터 객체 크기 및 역직렬화된 데이터 복잡도 제한 등)를 적용하는 것이 권장됩니다.

알 수 없거나 신뢰할 수 없는 호스트에 네트워크 요청을 보낼 때는 특히 주의해야 하며, 해당 호스트의 네트워크 위치가 신뢰할 수 없는 입력에서 온 경우(HTTP 리다이렉트를 포함하여) 로컬 또는 내부 호스트에 연결하지 않도록 URL을 검증하고, 브라우저 컨텍스트에서 SSRF를 피해야 합니다.

트래픽 증폭 공격을 방지하기 위해, 아웃바운드 네트워크 요청은 호스트별로 속도 제한되어야 합니다. 예를 들어, firehose에서 소비할 때 발생하는 신원 확인 요청(여기에는 DNS TXT 트래픽 및 DID 확인 요청이 포함됨)이 해당됩니다.

## 향후 작업

`subscribeRepos` Lexicon은 더 이상 사용되지 않는 필드를 제거하는 방식으로 조정될 가능성이 있으며, 이는 Lexicon 발전 규칙을 깨더라도 적용될 수 있습니다.

이벤트 스트림의 시퀀스/커서 체계는 샤딩, 타임스탬프 기반 재개, 그리고 독립 인스턴스 간의 더 쉬운 장애 조치를 지원하기 위해 개선될 수 있습니다.

전체 인증 firehose의 대안으로, 단순 JSON 직렬화, 레코드 컬렉션 타입에 따른 필터링, MST 노드 생략 등 전체 인증이 필요하지 않거나 원치 않는 사용 사례에서 개발을 단순화하고 리소스 소비를 줄이는 변경사항이 추가될 수 있습니다.


---
atproto/src/app/[locale]/specs/sync/page.tsx
---
export const metadata = {
  title: 'Sync',
  description: 'Firehose and other data synchronization mechanisms.',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/repository/en.mdx
---
export const metadata = {
  title: 'Repository',
  description:
    'Self-authenticating storage for public account content',
}

# Repository

*See the [Data Repositories Guide](../guides/data-repos) for a higher-level introduction.*

Public atproto content (**records**) is stored in per-account repositories (frequently shortened to **repo**). All currently active records are stored in the repository, and current repository contents are publicly available, but both content deletions and account deletions are fully supported. {{ className: 'lead' }}

The repository data structure is content-addressed (a [Merkle-tree](https://en.wikipedia.org/wiki/Merkle_tree)), and every mutation of repository contents (eg, addition, removal, and updates to records) results in a new commit `data` hash value (CID). Commits are cryptographically signed, with rotatable signing keys, which allows recursive validation of content as a whole or in part. {{ className: 'lead' }}

Repositories and their contents are canonically stored in binary [DAG-CBOR](https://ipld.io/docs/codecs/known/dag-cbor/) format, as a graph of data objects referencing each other by content hash (CID Links). Large binary blobs are not stored directly in repositories, though they are referenced by hash ([CID](https://github.com/multiformats/cid)). This includes images and other media objects. Repositories can be exported as [CAR](https://ipld.io/specs/transport/car/carv1/) files for offline backup, account migration, or other purposes. {{ className: 'lead' }}

In the atproto federation architecture, the authoritative location of an account's repository is the associated Personal Data Server (PDS). An account's current PDS location is authoritatively indicated in the DID Document. {{ className: 'lead' }}

In real-world use, it is expected that individual repositories will contain anywhere from dozens to millions of records. {{ className: 'lead' }}


## Repo Data Structure (v3)

This describes version `3` of the repository binary format.

Version `2` had a slightly different commit object schema, but is mostly compatible with `3`.

Version `1` had a different MST fanout configuration, and an incompatible schema for commits and repository metadata. Version `1` is deprecated, no repositories in this format exist in the network, and implementations do not need to support it.

At a high level, a repository is a key/value mapping where the keys are path names (as strings) and the values are records (DAG-CBOR objects).

A **Merkle Search Tree** (MST) is used to store this mapping. This content-addressed deterministic data structure stores data in key-sorted order. It is reasonably efficient for key lookups, key range scans, and appends (assuming sorted record paths). The properties of MSTs in general are described in this academic publication:

> Alex Auvolat, François Taïani. Merkle Search Trees: Efficient State-Based CRDTs in Open Networks. SRDS 2019 - 38th IEEE International Symposium on Reliable Distributed Systems, Oct 2019, Lyon, France. pp.1-10, ff10.1109/SRDS.2019.00032 ([pdf](https://inria.hal.science/hal-02303490/document))

The specific details of the MST as used in atproto repositories are described below.

Repo paths are strings, while MST keys are byte arrays. Neither may be empty (zero-length). While repo path strings are currently limited to a subset of ASCII (making encoding a no-op), the encoding is specified as UTF-8.

Repo paths currently have a fixed structure of `<collection>/<record-key>`. This means a valid, normalized [NSID](./nsid), followed by a `/`, followed by a valid [Record Key](./record-key). The path should not start with a leading `/`, and should always have exactly two path segments. The ASCII characters allowed in the entire path string are currently: letters (`A-Za-z`), digits (`0-9`), slash (`/`), period (`.`), hyphen (`-`), underscore (`_`), and tilde (`~`). The specific path segments `.` and `..` are not valid NSIDs or Record Keys, and will always be disallowed in repo paths.

Note that repo paths for all records in the same collection are sorted together in the MST, making enumeration (via key scan) and export efficient. Additionally, the TID Record Key scheme was intentionally selected to provide chronological sorting of MST keys within the scope of a collection. Appends are more efficient than random insertions/mutations within the tree, and when enumerating records within a collection they will be in chronological order (assuming that TID generation was done correctly, which cannot be relied on in general).


### Commit Objects

The top-level data object in a repository is a signed commit. The data fields are:

- `did` (string, required): the account DID associated with the repo, in strictly normalized form (eg, lowercase as appropriate)
- `version` (integer, required): fixed value of `3` for this repo format version
- `data` (CID link, required): pointer to the top of the repo contents tree structure (MST)
- `rev` (string, TID format, required): revision of the repo, used as a logical clock. Must increase monotonically. Recommend using current timestamp as TID; `rev` values in the "future" (beyond a fudge factor) should be ignored and not processed.
- `prev` (CID link, nullable): pointer (by hash) to a previous commit object for this repository. Could be used to create a chain of history, but largely unused (included for v2 backwards compatibility). In version `3` repos, this field must exist in the CBOR object, but is virtually always `null`. NOTE: previously specified as nullable and optional, but this caused interoperability issues.
- `sig` (byte array, required): cryptographic signature of this commit, as raw bytes

An UnsignedCommit data object has all the same fields except for `sig`. The process for signing a commit is to populate all the data fields, and then serialize the UnsignedCommit with DAG-CBOR. The output bytes are then hashed with SHA-256, and the binary hash output (without hex encoding) is then signed using the current "signing key" for the account. The signature is then stored as raw bytes in a commit object, along with all the other data fields.

The CID for a commit overall is generated by serializing a *signed* commit object as DAG-CBOR. See notes on the "blessed" CID format below, and in particular be sure to use the `dag-cbor` multicodec for CIDs linking to commit objects.

Note that neither the signature itself nor the signed commit indicate either the type of key used (curve type), or the specific public key used. That information must be fetched from the account's DID document. With key rotation, verification of older commit signatures can become ambiguous. The most recent commit should always be verifiable using the current DID document. This implies that a new repository commit should be created every time the signing key is rotated. Such a commit does not need to update the `data` CID link.


### MST Structure

At a high level, the repository MST is a key/value mapping where the keys are non-empty byte arrays, and the values are CID links to records. The MST data structure should be fully reproducible from such a mapping of bytestrings-to-CIDs, with exactly reproducible root CID hash (aka, the `data` field in commit object).

Every node in the tree structure contains a set of key/CID mappings, as well as links to other sub-tree nodes. The entries and links are in key-sorted order, with all of the keys of a linked sub-tree (recursively) falling in the range corresponding to the link location. The sort order is from **left** (lexically first) to **right** (lexically latter). Each key has a **depth** derived from the key itself, which determines which sub-tree it ends up in. The top node in the tree contains all of the keys with the highest depth value (which for a small tree may be all depth zero, so a single node). Links to the left or right of the entire node, or between any two keys in the node, point to a sub-tree node containing keys that fall in the corresponding key range.

An empty repository with no records is represented as a single MST node with an empty array of entries. This is the only situation in which a tree may contain an empty leaf node which does not either contain keys ("entries") or point to a sub-tree containing entries. The top of the tree must not be a an empty node which only points to a sub-tree. Empty intermediate nodes are allowed, as long as they point to a sub-tree which does contain entries. In other words, empty nodes must be pruned from the top and bottom of the tree, but empty intermediate nodes must be kept, such that sub-tree links do not skip a level of depth. The overall structure and shape of the MST is deterministic based on the current key/value content, regardless of the history of insertions and deletions that lead to the current contents.

For the atproto MST implementation, the hash algorithm used is SHA-256 (binary output), counting "prefix zeros" in 2-bit chunks, giving a fanout of 4. To compute the depth of a key:

- hash the key (a byte array) with SHA-256, with binary output
- count the number of leading binary zeros in the hash, and divide by two, rounding down
- the resulting positive integer is the depth of the key

Some examples, with the given ASCII strings mapping to byte arrays:

- `2653ae71`: depth "0"
- `blue`: depth "1"
- `app.bsky.feed.post/454397e440ec`: depth "4"
- `app.bsky.feed.post/9adeb165882c`: depth "8"

There are many MST nodes in repositories, so it is important that they have a compact binary representation, for storage efficiency. Within every node, keys (byte arrays) are compressed by eliding common prefixes, with each entry indicating how many bytes it shares with the previous key in the array. The first entry in the array for a given node must contain the full key, and a common prefix length of 0. This key compaction is internal to nodes, it does not extend across multiple nodes in the tree. The compaction scheme is mandatory, to ensure that the MST structure is deterministic across implementations.

The node data schema fields are:

- `l` ("left", CID link, nullable): link to sub-tree Node on a lower level and with all keys sorting before keys at this node
- `e` ("entries", array of objects, required): ordered list of TreeEntry objects
    - `p` ("prefixlen", integer, required): count of bytes shared with previous TreeEntry in this Node (if any)
    - `k` ("keysuffix", byte array, required): remainder of key for this TreeEntry, after "prefixlen" have been removed
    - `v` ("value", CID Link, required): link to the record data (CBOR) for this entry
    - `t` ("tree", CID Link, nullable): link to a sub-tree Node at a lower level which has keys sorting after this TreeEntry's key (to the "right"), but before the next TreeEntry's key in this Node (if any)

When parsing MST data structures, the depth and sort order of keys should be verified. This is particularly true for untrusted inputs, but is simplest to just verify every time. Additional checks on node size and other parameters of the tree structure also need to be limited; see the "Security Considerations" section of this document.

### CID Formats

The IPFS CID specification is very flexible, and supports a wide variety of hash types, a field indicating the type of content being linked to, and various string encoding options. These features are valuable to allow evolution of the repo format over time, but to maximize interoperability among implementations, only a specific "blessed" set of CID types are allowed.

The blessed format for commit objects and MST node objects, when linking to commit objects, MST nodes (aka, `data`, or MST internal links), or records (aka, MST leaf nodes to records), is:

- CIDv1
- Multibase: binary serialization within DAG-CBOR (or `base32` for JSON mappings)
- Multicodec: `dag-cbor` (0x71)
- Multihash: `sha-256` with 256 bits (0x12)

In the context of repositories, it is also desirable for the overall data structure to be reproducible given the contents, so the allowed CID types are strictly constrained and enforced. Commit objects with non-compliant `prev` or `data` links are considered invalid. MST Node objects with non-compliant links to other MST Node objects are considered invalid, and the entire MST data structure invalid.

More flexibility is allowed in processing the "leaf" links from MST to records, and implementations should retain the exact CID links used for these mappings, instead of normalizing. Implementations should strictly follow the CID blessed format when generating new CID Links to records.


## CAR File Serialization

The standard file format for storing data objects is Content Addressable aRchives (CAR). The standard repository export format for atproto repositories is [CAR v1](https://ipld.io/specs/transport/car/carv1/), which have file suffix `.car` and mimetype `application/vnd.ipld.car`.

The CARv1 format is very simple. It contains a small metadata header (which can indicate one or more "root" CID links), and then a series of binary "blocks", each of which is a data object. In the context of atproto repositories:

- The first element of the CAR `roots` metadata array must be the CID of the most relevant Commit object. for a generic export, this is the current (most recent) commit. additional CIDs may also be present in the `roots` array, with (for now) undefined meaning or order
- For full exports, the full repo structure must be included for the indicated commit, which includes all records and all MST nodes
- The order of blocks within the CAR file is not currently defined or restricted. implementations may have a "preferred" ordering, but should be tolerant of unexpected ordering
- Additional blocks, including records, may or may not be included in the CAR file

When importing CAR files, note that there may existing dangling CID references. For example, repositories may contain CID Links to blobs or records in other repositories, and the blocks corresponding to those blobs or references would likely not be included in the CAR file.

The CARv1 specification is agnostic about the same block appearing multiple times in the same file ("Duplicate Blocks)". Implementations should be robust to both duplication and de-duplication of blocks, and should also ignore any unnecessary or unlinked blocks.

## Repository Diffs

<Note>
An updated version of the Synchronization protocol is being rolled out to the live network in early 2025. There is a ["Sync v1.1" proposal document](https://github.com/bluesky-social/proposals/tree/main/0006-sync-iteration) and an [update blog post](https://docs.bsky.app/blog/relay-sync-updates) with deployment details. The written specifications will be updated soon.
</Note>

A concept which supports efficient synchronization of data between independent services is "diffs" of repository trees between different revisions. The basic principle is that a repository diff contains all the data (commit object, MST nodes, and records) that have changed between an older revision and the current revision of a repo. The diff can be "applied" to the older mirror of the repository, and the result will be the complete MST tree at the current (newer) commit revision.

Repo diffs can be serialized as CAR files, sometimes referred to as "CAR slices". Some details about diff CAR slices:

- same format, version, and atproto-specific constraints as full repo export CAR files
    - blocks "should" be de-duplicated by CID (only one copy included), though receiving implementations must be resilient to duplication
- the root CID indicated in the CAR header (the first element of `roots`) should point to the commit block (which must be included)
- any required blocks must be included even if they have appeared in the history of the repository previously. eg, if a record is created in rev C, deleted in rev F, and re-created in rev N, the diff "since F" must include the record block
- all "created" records must be included
- any records which have been "deleted" and do not exist in the current repo should not be included
- any records which have been "updated" should include the final version, and should not include the previous version
- all MST nodes in the current repo which didn't exist in the previous repo version must be included
- with the exception of removed record data, the diff may include additional blocks, which receivers should ignore.
  - however, diffs which intentionally contain a large amount of irrelevant block data to consume network or compute resources are considered a form of network abuse.

The diff is a partial Merkle tree, including a signed commit, and can be partially verified. This means that an observer which has successfully resolved the identity of the relevant account (including cryptographic public keys) can verify certain aspects of the data. The diff is a reliable "proof chain" for creation and updates of records: an observer can verify that the new or updated records have the specific record values in the overall repo as of the commit revision. If the observer knows of specific records (by repo path, or by full AT-URI) that have been deleted, they can verify that those records no longer exist in the repo as of the final commit revision.

However, an observer which does not know the full state of the repository at the "older" revision *can not* reliably enumerate all of the records that have been removed from the repository. Such an observer also can not see the previous values of deleted or updated records, either as full values or by CID. Note that the later is an intentional design goal for the diff concept: it is desired that content deletion happen rapidly and not "draw attention" to the content which has been deleted. It is technically possible for "archival" observers to track deletion events and lookup the previous content value, but this requires additional resources and effort.

Sometimes repo diffs are generated automatically. For example, every commit to a repo can result in a diff against the immediately preceding commit. In other contexts, diffs are generated on demand: a diff can be requested "since" an arbitrary previous revision. It is not expected that repo hosts support generating diffs between two arbitrary revisions, only "from" an arbitrary older revision and the current revision. Repo hosts are not required to maintain a complete history of prior commits/revisions, and in some cases (such as account migration) may never have had prior repo history. Some details about how to interpret and service requests for diffs "since" a prior revision:

- it is helpful to track internally the commit revision when a block (record or MST node) was created or re-created. This enables querying blocks "since" a point in time
- "since" revisions are not expected to be an exact match
    - for example, if a repo had a sequence of commits "333", "666", "999", and a "since" value of "444" was requested, the changes in "666" and "999" should be included, as if the "since" parameter was "666"..
- a host is allowed to include additional history, but is encouraged to return the minimal or most granular requested data
    - for example, a host may have "compacted" repo rev history to a smaller number of commits. If a repo had commit history "288", "300", "320", "340", "400", and got a request "since" 340, it might return all changes since 300. Hosts are encouraged to return the smallest diff when possible (eg, “since” 340), but clients should be resilient.
- if a host receives a “since” request earlier than the oldest available revision for a repository, it should return the full repository. This may happen if the host does not have the complete history of the repository.
    - for example, if a repository had revisions "140", "150", and "160", then migrated to a new PDS and revisions continued "161" and "170", if the new PDS is asked for a diff "since" 150, the new PDS would probably need to return the full repository, because the earliest revision it would be aware of was "160" or "161" (depending on how migration was implemented).

In the specific case of chained commit-to-commit diffs which appear on the firehose, diffs should be "minimal": they should not contain additional records or additional history.

## Security Considerations

Repositories are untrusted input: accounts have full control over repository contents, and PDS instances have full control over binary encoding. It is important to handle possible denial of service vectors from both hostile actors or accidental situations (eg, corrupted data or buggy implementations).

Generic precautions should be followed with CBOR decoding: a maximum serialized object size, a maximum recursion depth for nested fields, maximum memory budget for deserialized data, etc. Some CBOR libraries include these precautions by default, but others do not.

The efficiency of the MST data structure depends on key hashes being relatively randomly dispersed. Because accounts have control over Record Keys, they can mine for sets of record keys with particular depths and sorting order, which result in inefficient tree shapes, which can cause both large storage overhead, and network amplification in the context of federation streams. To protect against these attacks, implementations should limit the number of TreeEntries per Node to a statistically unlikely maximum length. It may also be necessary to limit the overall depth of the repo, or other parameters, to prevent more sophisticated key mining attacks.

When importing CAR files, the completeness of the repository structure should be verified. Additional unrelated blocks might be included in the CAR structure; care should be taken when injecting CAR contents directly in to backend block storage, to ensure resources are not wasted on un-referenced blocks. There may also be issues with cross-account contamination from CAR imports, for example previously-deleted records re-appearing via CAR import from an unrelated account.


## Possible Future Changes

An optional in-repo mechanism for storing multiple versions of the same record (by path) may be implemented. Eg, adding additional path field to indicate the version by CID, timestamp, or monotonically increasing version integer.

Mechanisms for storing metadata associated with each record are being considered, for example, generic label, re-use rights, or hashtag metadata. This would allow mutating the metadata without mutating the record itself, and make some metadata generic across lexicons.

Repo path restrictions may be relaxed in other ways, including fewer or additional path segments, more allowed characters (including non-ASCII), etc. Paths will always be valid Unicode strings, mapped to MST keys (byte arrays) by UTF-8 encoding.

At the overall atproto specification level, additional "blessed" cryptographic algorithms may be added over time. Likewise, additional CID formats to reference records and blobs may be added. Internal CID format changes would require a repo format version bump.

Repository CAR exports may include linked "blobs" (larger binary files). This might become the default, or a configurable option, or some another mechanism for blob export might be chosen (eg, `.tar` or `.zip` export).

Record content could conceivably be something other than DAG-CBOR some day. This would probably be a repo format version bump. Note that it is possible to efficiently wrap other data formats in a DAG-CBOR wrapper (via a byte array field), or to have a small DAG-CBOR record type that links to a blob in arbitrary format.

Repository CAR exports may end up with a preferred block ordering scheme specified.

The CARv2 file format, which includes optimizations for some use cases, may be adopted in some form.

Adding optional fields to commit and MST node objects may or may not result in a repo format version change. Changing the MST fanout, or any changes to the current MST fields, would be a full repo version change.


---
atproto/src/app/[locale]/specs/repository/ko.mdx
---
export const metadata = {
  title: '레포지토리',
  description: '공개 계정 콘텐츠를 위한 자가 인증 스토리지',
}

# 레포지토리

*상위 수준의 소개는 [데이터 레포지토리 가이드](../guides/data-repos)를 참조하세요.*

AT Protocol의 공개 atproto 콘텐츠(**레코드**)는 계정별 레포지토리(종종 **repo**로 축약됨)에 저장됩니다. 현재 활성화된 모든 레코드는 레포지토리에 저장되며, 현재의 레포지토리 내용은 공개적으로 접근 가능합니다. 다만, 콘텐츠 삭제와 계정 삭제 모두 완벽하게 지원됩니다. {{ className: 'lead' }}

레포지토리 데이터 구조는 콘텐츠 주소 지정 방식(머클 트리, [Merkle-tree](https://ko.wikipedia.org/wiki/Merkle_트리))을 따르며, 레포지토리 내용의 모든 변경(예: 레코드의 추가, 제거, 업데이트)은 새로운 커밋의 `data` 해시 값(CID)을 생성합니다. 커밋은 회전 가능한 서명 키로 암호화 서명되어, 전체 또는 일부 콘텐츠의 재귀적 검증이 가능합니다. {{ className: 'lead' }}

레포지토리와 그 내용은 이진 [DAG-CBOR](https://ipld.io/docs/codecs/known/dag-cbor/) 형식으로, 서로의 콘텐츠 해시(CID 링크)를 참조하는 데이터 객체의 그래프로 정규적으로 저장됩니다. 큰 이진 블롭은 레포지토리에 직접 저장되지 않고 해시([CID](https://github.com/multiformats/cid))로 참조됩니다. 여기에는 이미지 및 기타 미디어 객체가 포함됩니다. 레포지토리는 오프라인 백업, 계정 이전 또는 기타 목적을 위해 [CAR](https://ipld.io/specs/transport/car/carv1/) 파일로 내보낼 수 있습니다. {{ className: 'lead' }}

atproto 페더레이션 아키텍처에서는 계정 레포지토리의 공식적인 위치가 관련된 개인 데이터 서버(PDS)입니다. 계정의 현재 PDS 위치는 DID 문서에서 공식적으로 명시됩니다. {{ className: 'lead' }}

실제 사용에서는 개별 레포지토리가 수십 개에서 수백만 개의 레코드를 포함할 것으로 예상됩니다. {{ className: 'lead' }}


## 레포지토리 데이터 구조 (v3)

이 문서는 레포지토리 이진 형식의 버전 `3`에 대해 설명합니다.

버전 `2`는 약간 다른 커밋 객체 스키마를 가졌으나, 대부분 버전 `3`과 호환됩니다.

버전 `1`은 다른 MST 팬아웃 구성과, 커밋 및 레포지토리 메타데이터에 대해 호환되지 않는 스키마를 사용했습니다. 버전 `1`은 더 이상 사용되지 않으며, 네트워크 상에 이 형식의 레포지토리는 존재하지 않고 구현체도 이를 지원할 필요가 없습니다.

높은 수준에서, 레포지토리는 키/값 매핑이며, 키는 경로명(문자열)이고 값은 레코드(DAG-CBOR 객체)입니다.

**머클 검색 트리(Merkle Search Tree, MST)** 가 이 매핑을 저장하는 데 사용됩니다. 이 콘텐츠 주소 지정 방식의 결정적 데이터 구조는 키를 정렬된 순서로 저장하여, 키 조회, 범위 스캔, 그리고 (레코드 경로가 정렬되어 있다고 가정할 때) 추가 연산을 비교적 효율적으로 수행할 수 있습니다. 일반적인 MST의 속성은 아래의 학술 출판물에서 설명됩니다:

> Alex Auvolat, François Taïani. Merkle Search Trees: Efficient State-Based CRDTs in Open Networks. SRDS 2019 - 38th IEEE International Symposium on Reliable Distributed Systems, Oct 2019, Lyon, France. pp.1-10, ff10.1109/SRDS.2019.00032 ([pdf](https://inria.hal.science/hal-02303490/document))

atproto 레포지토리에서 사용되는 MST의 구체적인 세부 사항은 아래에 설명되어 있습니다.

레포지토리 경로는 문자열이며, MST 키는 바이트 배열입니다. 둘 다 빈 문자열(길이 0)이 될 수 없습니다. 레포지토리 경로 문자열은 현재 ASCII의 하위 집합으로 제한되어 있어 인코딩이 필요 없으나, 인코딩은 UTF-8로 지정되어 있습니다.

레포지토리 경로는 현재 `<collection>/<record-key>`의 고정 구조를 가집니다. 이는 유효하고 정규화된 [NSID](./nsid) 뒤에 `/`가 오고, 그 뒤에 유효한 [Record Key](./record-key)가 오는 형식입니다. 경로는 선행 슬래시(`/`)로 시작해서는 안되며, 항상 정확히 두 개의 경로 세그먼트로 구성되어야 합니다. 전체 경로 문자열에 허용되는 ASCII 문자는 현재 다음과 같습니다: 영문자(`A-Za-z`), 숫자(`0-9`), 슬래시(`/`), 점(`.`), 하이픈(`-`), 밑줄(`_`), 물결표(`~`). 특정 경로 세그먼트인 `.` 및 `..`는 유효한 NSID 또는 Record Key가 아니므로, 레포지토리 경로에서는 항상 허용되지 않습니다.

동일한 컬렉션 내의 모든 레코드의 레포지토리 경로는 MST에서 함께 정렬되므로, 열거(키 스캔) 및 내보내기가 효율적입니다. 또한, TID Record Key 체계는 컬렉션 내에서 MST 키의 연대순으로 정렬할 수 있도록 의도적으로 설계되었습니다. 이는 트리 내에서 무작위 삽입이나 수정보다 추가 작업이 효율적이며, 컬렉션 내 레코드를 열거할 때 시간 순서대로 나열됩니다(단, TID 생성이 올바르게 이루어졌다는 전제 하에 일반적으로 신뢰할 수 있습니다).


### 커밋 객체

레포지토리의 최상위 데이터 객체는 서명된 커밋입니다. 데이터 필드는 다음과 같습니다:

- `did` (문자열, 필수): 레포지토리에 연결된 계정의 DID로, 엄격하게 정규화된 형식(예: 소문자 등)이어야 합니다.
- `version` (정수, 필수): 이 레포지토리 형식 버전의 고정 값 `3`
- `data` (CID 링크, 필수): 레포지토리 내용 트리 구조(MST)의 최상위 노드를 가리키는 포인터
- `rev` (문자열, TID 형식, 필수): 레포지토리 개정판으로, 논리적 시계로 사용됩니다. 단조 증가해야 하며, 현재 타임스탬프를 TID로 사용하는 것이 권장됩니다. (약간의 오차 범위를 벗어난 미래의 `rev` 값은 무시되어야 합니다.)
- `prev` (CID 링크, nullable): 이 레포지토리의 이전 커밋 객체를 해시로 가리키는 포인터입니다. 이는 히스토리 체인을 구성할 수 있으나, 대부분 사용되지 않으며(버전 `2`와의 하위 호환성을 위해 포함됨), 버전 `3` 레포지토리에서는 이 필드가 CBOR 객체에 반드시 존재해야 하지만 사실상 항상 `null`입니다.  
  > **참고:** 이전에는 nullable 및 선택적이었으나, 이는 상호 운용성 문제를 일으켰습니다.
- `sig` (바이트 배열, 필수): 이 커밋의 암호화 서명을 원시 바이트 형태로 저장

UnsignedCommit 데이터 객체는 `sig`를 제외하고 동일한 필드를 가집니다. 커밋 서명의 절차는 모든 데이터 필드를 채운 후, DAG-CBOR로 UnsignedCommit을 직렬화하는 것입니다. 출력된 바이트는 SHA-256으로 해시되며, 이 바이너리 해시 결과(16진수 인코딩 없이)를 계정의 현재 "서명 키"로 서명합니다. 서명은 원시 바이트 형태로 커밋 객체에 저장됩니다.

서명된 커밋 객체의 CID는 해당 객체를 DAG-CBOR로 직렬화하여 생성됩니다. 아래 "blessed" CID 형식에 관한 주의사항을 참고하시고, 특히 커밋 객체에 링크할 때는 반드시 `dag-cbor` 멀티코덱을 사용해야 합니다.

서명 자체나 서명된 커밋은 사용된 키의 종류(곡선 유형) 또는 특정 공개 키 정보를 나타내지 않습니다. 해당 정보는 계정의 DID 문서에서 가져와야 합니다. 키 회전이 발생할 경우, 이전 커밋 서명의 검증이 모호해질 수 있으므로, 가장 최근의 커밋은 항상 현재의 DID 문서를 사용하여 검증되어야 합니다. 이는 서명 키 회전 시마다 새로운 레포지토리 커밋이 생성되어야 함을 의미합니다. (이 경우, `data` CID 링크는 업데이트할 필요가 없습니다.)


### MST 구조

높은 수준에서, 레포지토리 MST는 키/값 매핑으로, 키는 빈 문자열이 아닌 바이트 배열이며, 값은 레코드에 대한 CID 링크입니다. 이 MST 데이터 구조는 바이트 문자열-대-CID 매핑만으로도 완전히 재현 가능하며, 최상위 CID 해시(커밋 객체의 `data` 필드)는 내용에 따라 정확히 재생산됩니다.

트리 구조의 모든 노드는 키/CID 매핑 집합과 하위 트리 노드에 대한 링크를 포함합니다. 노드 내의 항목과 링크는 키의 정렬 순서대로 배열되며, 하위 트리 노드(재귀적으로)의 모든 키는 해당 링크 범위 내에 속합니다. 정렬 순서는 **왼쪽**(사전 순으로 가장 앞)부터 **오른쪽**(사전 순으로 뒤)까지입니다.

각 키는 해당 키 자체로부터 파생된 **깊이**를 가지며, 이 깊이가 키가 배치될 하위 트리를 결정합니다. 트리의 최상위 노드는 가장 높은 깊이 값을 가지는 모든 키를 포함합니다(작은 트리의 경우 깊이 0의 키만 존재하여 단일 노드일 수 있음). 노드 내에서 좌측 또는 우측에 존재하거나, 두 키 사이에 위치하는 링크들은 해당 범위에 속하는 키들을 포함하는 하위 트리 노드를 가리킵니다.

레코드가 없는 빈 레포지토리는 항목 배열이 비어있는 단일 MST 노드로 표현됩니다. 이것은 키("entries")나 하위 트리("sub-tree")에 대한 링크가 전혀 없는 유일한 경우입니다. 트리의 최상위는 단지 하위 트리 링크만 있는 빈 노드여서는 안 됩니다. 빈 중간 노드는 허용되며, 이 경우 해당 노드가 항목을 포함하는 하위 트리를 가리키기만 하면 됩니다. 즉, 빈 노드는 최상위와 최하위에서 제거되어야 하지만, 중간에 존재하는 빈 노드는 하위 트리 링크가 깊이 레벨을 건너뛰지 않도록 유지되어야 합니다. MST의 전체 구조와 형태는 현재의 키/값 내용에 따라 결정적이며, 삽입 및 삭제 이력과는 무관합니다.

atproto MST 구현에서는 해시 알고리즘으로 SHA-256(바이너리 출력)을 사용하며, 2비트 단위로 "접두사 0"의 개수를 세어 팬아웃을 4로 설정합니다. 키의 깊이를 계산하는 방법은 다음과 같습니다:

- 키(바이트 배열)를 SHA-256으로 해시하여 바이너리 결과를 얻습니다.
- 해시의 선행 바이너리 0의 개수를 세고, 이를 2로 나눈 후 내림합니다.
- 결과로 나온 양의 정수가 해당 키의 깊이가 됩니다.

다음은 주어진 ASCII 문자열이 바이트 배열로 매핑된 경우의 예시입니다:

- `2653ae71`: 깊이 "0"
- `blue`: 깊이 "1"
- `app.bsky.feed.post/454397e440ec`: 깊이 "4"
- `app.bsky.feed.post/9adeb165882c`: 깊이 "8"

레포지토리 내에는 많은 MST 노드가 존재하므로, 저장 효율성을 위해 이들이 압축된 이진 표현을 가지는 것이 중요합니다. 각 노드 내에서 키(바이트 배열)는 공통 접두사를 생략하여 압축되며, 각 항목은 바로 이전 키와 공유하는 바이트 수를 나타냅니다. 해당 노드의 첫 번째 항목은 전체 키를 포함해야 하며, 공통 접두사 길이는 0이어야 합니다. 이 키 압축은 노드 내부에서만 적용되며, 트리의 여러 노드에 걸쳐 확장되지는 않습니다. 이 압축 방식은 구현 간 MST 구조의 결정성을 보장하기 위해 필수적입니다.

노드 데이터 스키마 필드는 다음과 같습니다:

- `l` ("left", CID link, nullable): 현재 노드보다 낮은 레벨에 있으며, 이 노드의 키보다 작은 키들을 포함하는 하위 트리 노드에 대한 링크
- `e` ("entries", array of objects, required): 정렬된 TreeEntry 객체의 목록
  - `p` ("prefixlen", integer, required): 현재 노드 내에서 이전 TreeEntry와 공유하는 바이트 수(존재하는 경우)
  - `k` ("keysuffix", byte array, required): 이전 TreeEntry와 공유된 `p` 바이트 이후의 키 나머지
  - `v` ("value", CID Link, required): 해당 항목의 레코드 데이터(CBOR 객체)에 대한 링크
  - `t` ("tree", CID Link, nullable): 이 TreeEntry의 키 오른쪽에 해당하며, 노드 내 다음 TreeEntry의 키(존재하는 경우) 이전의 키들을 포함하는 하위 트리 노드에 대한 링크

MST 데이터 구조를 파싱할 때, 키의 깊이와 정렬 순서를 반드시 검증해야 합니다. 이는 신뢰할 수 없는 입력에 대해 특히 중요하며, 매번 검증하는 것이 가장 단순합니다. 노드 크기나 트리 구조의 기타 매개변수에 대한 추가 검사 역시 필요하지만, "보안 고려사항" 섹션을 참고하여 제한적으로 수행해야 합니다.

### CID 형식

IPFS CID 사양은 매우 유연하여 다양한 해시 유형, 링크되는 콘텐츠의 유형, 여러 문자열 인코딩 옵션을 지원합니다. 이러한 기능은 시간이 지나면서 레포지토리 형식이 발전할 수 있도록 하지만, 구현 간 상호 운용성을 극대화하기 위해서는 "blessed" CID 유형만 허용됩니다.

커밋 객체 및 MST 노드 객체(즉, 커밋 객체의 `data` 필드 또는 MST 내부 링크)와 레코드(즉, MST 리프 노드에서 레코드로의 링크)에 대해 승인된 형식은 다음과 같습니다:

- CIDv1
- 멀티베이스: DAG-CBOR 내 이진 직렬화(또는 JSON 매핑의 경우 `base32`)
- 멀티코덱: `dag-cbor` (0x71)
- 멀티해시: SHA-256 (256비트, 0x12)

레포지토리 맥락에서는 전체 데이터 구조가 내용에 따라 재현 가능해야 하므로, 허용된 CID 유형은 엄격히 제약되고 강제됩니다. 승인되지 않은 `prev` 또는 `data` 링크를 가진 커밋 객체는 유효하지 않은 것으로 간주됩니다. 또한, MST 노드 객체가 승인되지 않은 형식의 링크를 포함할 경우, 해당 MST 전체 구조가 유효하지 않은 것으로 판단됩니다.

레코드로의 "leaf" 링크에 대해서는 더 유연한 처리가 허용되며, 구현체는 이러한 매핑에 사용된 정확한 CID 링크를 보존해야 합니다. 새 레코드에 대한 CID 링크를 생성할 때는 반드시 승인된 CID 형식을 엄격히 따라야 합니다.


## CAR 파일 직렬화

데이터 객체를 저장하기 위한 표준 파일 형식은 Content Addressable aRchives(CAR) 입니다. atproto 레포지토리의 표준 내보내기 형식은 [CAR v1](https://ipld.io/specs/transport/car/carv1/)이며, 파일 확장자는 `.car`, mimetype은 `application/vnd.ipld.car`입니다.

CARv1 형식은 매우 단순합니다. 소규모 메타데이터 헤더(하나 이상의 "루트" CID 링크를 포함할 수 있음)와 일련의 이진 "블록"으로 구성됩니다. atproto 레포지토리의 경우:

- CAR 헤더의 `roots` 배열 첫 번째 요소는 가장 관련성이 높은 커밋 객체의 CID여야 합니다. 일반적인 내보내기에서는 이는 현재(최신) 커밋입니다. 추가 CID가 `roots` 배열에 포함될 수 있으나, 그 의미나 순서는 현재 정의되지 않았습니다.
- 전체 내보내기의 경우, 해당 커밋에 대한 전체 레포지토리 구조(모든 레코드와 모든 MST 노드)가 포함되어야 합니다.
- CAR 파일 내 블록의 순서는 현재 정의되거나 제한되지 않습니다. 구현체는 "선호하는" 순서를 가질 수 있으나, 예기치 않은 순서에 대해서도 유연해야 합니다.
- 추가 블록, 예를 들어 레코드 블록 등이 CAR 파일에 포함될 수 있습니다.

CAR 파일을 가져올 때, 연결이 끊긴 CID 참조가 존재할 수 있음을 유의해야 합니다. 예를 들어, 레포지토리가 다른 레포지토리의 블롭이나 레코드에 대한 CID 링크를 포함할 수 있으며, 해당 블록들은 CAR 파일에 포함되지 않을 수 있습니다.

CARv1 사양은 동일한 블록이 파일 내 여러 번 나타나는 경우(“중복 블록”)에 대해 특별한 규정을 두지 않습니다. 구현체는 중복 및 중복 제거에 대해 강건하게 처리해야 하며, 불필요하거나 연결되지 않은 블록은 무시해야 합니다.

## 레포지토리 차이(diff)

서로 독립적인 서비스 간 데이터를 효율적으로 동기화하기 위한 개념으로 "레포지토리 차이(diff)"가 있습니다. 기본 원리는 레포지토리 차이는 이전 개정판과 현재 개정판 사이에 변경된 모든 데이터(커밋 객체, MST 노드, 레코드)를 포함한다는 것입니다. 이 차이는 이전 버전의 레포지토리에 "적용"되어 최신 커밋 개정판의 전체 MST 트리를 구성하게 됩니다.

레포지토리 차이는 때때로 CAR 파일로 직렬화되며, 이를 "CAR slices"라고도 합니다. CAR slices에 관한 몇 가지 세부 사항은 다음과 같습니다:

- 전체 레포지토리 내보내기 CAR 파일과 동일한 형식, 버전, atproto 전용 제약 조건을 따릅니다.
  - 블록은 CID 기준으로 "중복 제거"되어야 하나(단, 하나의 사본만 포함), 수신 구현체는 중복에 대해 강건하게 처리해야 합니다.
- CAR 헤더의 `roots` 배열 첫 번째 요소는 커밋 블록을 가리켜야 하며, 해당 커밋 블록은 반드시 포함되어야 합니다.
- 필요한 모든 블록은, 설령 이전 레포지토리 히스토리에서 이미 나타난 블록이라 하더라도 반드시 포함되어야 합니다. 예를 들어, 레코드가 rev C에서 생성되어 rev F에서 삭제되고 rev N에서 재생성되었다면, "since F" 차이는 해당 레코드 블록을 포함해야 합니다.
- 생성된 모든 레코드는 반드시 포함되어야 합니다.
- 삭제되어 현재 레포지토리에 존재하지 않는 레코드는 포함되어서는 안 됩니다.
- 업데이트된 레코드는 최종 버전을 포함해야 하며, 이전 버전은 포함되지 않아야 합니다.
- 이전 레포지토리 버전에 존재하지 않았던 현재 레포지토리의 모든 MST 노드는 포함되어야 합니다.
- 삭제된 레코드 데이터 외에도, 차이는 수신자가 무시해야 할 추가 블록을 포함할 수 있습니다.
  - 다만, 네트워크나 계산 자원을 과도하게 소모하기 위해 불필요한 대량의 블록 데이터를 의도적으로 포함하는 차이는 네트워크 오용으로 간주됩니다.

차이는 부분적인 머클 트리로, 서명된 커밋을 포함하며, 일부 검증이 가능합니다. 이는 관련 계정(암호학적 공개 키 포함)의 신원을 확인한 관찰자가, 변경된 레코드가 최신 커밋 개정판에 포함되어 있음을 검증할 수 있음을 의미합니다. 또한, 관찰자가 특정 레코드(레포지토리 경로나 전체 AT-URI)를 알고 있다면, 해당 레코드가 최종 커밋 개정판에 더 이상 존재하지 않음을 검증할 수 있습니다.

하지만, 이전 개정판의 전체 레포지토리 상태를 모르는 관찰자는 삭제된 모든 레코드를 신뢰성 있게 열거하거나, 업데이트 이전의 레코드 값을 CID 혹은 전체 값으로 확인할 수 없습니다. 이는 콘텐츠 삭제가 신속하게 이루어지고, 삭제된 콘텐츠에 주목이 집중되지 않도록 하는 의도적인 설계입니다. 아카이브 목적으로 삭제 이벤트를 추적하고 이전 콘텐츠 값을 조회하는 것은 기술적으로 가능하나, 추가적인 자원과 노력이 필요합니다.

때때로 레포지토리 차이는 자동으로 생성됩니다. 예를 들어, 레포지토리의 모든 커밋은 바로 이전 커밋과의 차이를 생성할 수 있습니다. 다른 경우, 필요에 따라 "이전" 개정판 이후의 차이를 요청할 수 있습니다. 레포지토리 호스트는 임의의 두 개정판 간의 차이를 생성할 필요 없이, 임의의 이전 개정판 이후와 현재 개정판 사이의 차이만 제공하면 됩니다. 레포지토리 호스트는 이전 커밋/개정판의 전체 히스토리를 보관할 필요가 없으며, 일부 경우(예: 계정 이전)에는 이전 히스토리가 전혀 없을 수도 있습니다. "since" 요청에 따른 차이 해석 및 제공에 관한 몇 가지 세부 사항은 다음과 같습니다:

- 블록(레코드 또는 MST 노드)이 생성되거나 재생성된 개정판을 내부적으로 추적하면, 특정 시점 이후의 블록을 쿼리할 수 있습니다.
- "since" 개정판은 정확한 일치가 아닐 수 있습니다.
  - 예를 들어, 레포지토리 커밋이 "333", "666", "999"로 이루어졌을 때, "since" 값이 "444"로 요청되면 "666"과 "999"의 변경 사항이 포함되어야 하며, 마치 "since" 값이 "666"인 것처럼 처리되어야 합니다.
- 호스트는 추가 히스토리를 포함할 수 있으나, 가능한 한 최소한의 변경 사항만 반환하는 것이 권장됩니다.
  - 예를 들어, 레포지토리 커밋 히스토리가 "288", "300", "320", "340", "400"인 경우, "since" 값이 340이라면 340 이후의 모든 변경 사항이 반환되어야 합니다. 호스트는 가능한 한 최소한의 차이를 반환하도록 권장되지만, 클라이언트는 이에 대해 강건하게 처리해야 합니다.
- 만약 호스트가 보유한 레포지토리 히스토리 중 가장 오래된 개정판보다 이전의 "since" 요청을 받으면, 전체 레포지토리를 반환해야 합니다.
  - 예를 들어, 레포지토리 개정판이 "140", "150", "160"이었으나, 이후 새로운 PDS로 이전되어 개정판이 "161"과 "170"으로 이어진 경우, 새로운 PDS는 "since" 150 요청에 대해, 자신이 알고 있는 가장 오래된 개정판(예: "160" 또는 "161") 이후의 전체 레포지토리를 반환해야 합니다.

특히, firehose에 나타나는 연쇄적인 커밋 간 차이는 "최소화"되어야 하며, 추가 레코드나 히스토리를 포함해서는 안 됩니다.

## 보안 고려사항

레포지토리는 신뢰할 수 없는 입력입니다: 계정은 레포지토리 내용을 완전히 제어하며, PDS 인스턴스는 이진 인코딩을 완전히 제어합니다. 악의적 행위자나 우발적 상황(예: 손상된 데이터, 버그 있는 구현체)로 인한 서비스 거부(DoS) 벡터에 대비해야 합니다.

CBOR 디코딩 시에는 최대 직렬화 객체 크기, 중첩 필드의 최대 재귀 깊이, 디코딩된 데이터의 최대 메모리 사용량 등을 포함한 일반적인 예방책을 따라야 합니다. 일부 CBOR 라이브러리는 이러한 제한을 기본으로 제공하지만, 다른 라이브러리는 그렇지 않을 수 있습니다.

MST 데이터 구조의 효율성은 키 해시가 상대적으로 무작위 분포를 이루는 것에 의존합니다. 계정이 레코드 키를 제어할 수 있으므로, 특정 깊이와 정렬 순서를 갖는 레코드 키 집합을 채굴하여 비효율적인 트리 구조를 초래할 수 있으며, 이는 저장 공간의 과다 사용과 페더레이션 스트림에서의 네트워크 증폭을 유발할 수 있습니다. 이러한 공격을 방지하기 위해, 구현체는 노드당 TreeEntry의 개수를 통계적으로 있을 법한 최대 길이로 제한해야 합니다. 또한, 전체 레포지토리의 깊이나 기타 매개변수를 제한하여 보다 정교한 키 채굴 공격을 방지할 필요가 있습니다.

CAR 파일을 가져올 때에는, 레포지토리 구조의 완전성을 검증해야 합니다. CAR 구조에 관련 없는 추가 블록이 포함될 수 있으므로, CAR 내용을 백엔드 블록 저장소에 직접 주입할 때 불필요한 자원 소모를 방지해야 합니다. 또한, CAR 가져오기로 인해 이전에 삭제된 레코드가 다른 계정에서 다시 나타나는 등의 계정 간 오염 문제가 발생할 수 있습니다.


## 향후 변경 가능 사항

동일한 레코드(경로별)의 여러 버전을 저장하는 선택적 인레포지토리 메커니즘이 도입될 수 있습니다. 예를 들어, CID, 타임스탬프, 혹은 단조 증가하는 버전 정수를 나타내는 추가 경로 필드를 도입할 수 있습니다.

레코드와 관련된 메타데이터(예: 일반 라벨, 재사용 권한, 해시태그 메타데이터 등)를 저장하는 메커니즘이 고려되고 있습니다. 이는 레코드 자체를 변경하지 않고 메타데이터만 수정하여, 일부 메타데이터가 렉시콘 전반에 걸쳐 공통적으로 사용될 수 있도록 할 것입니다.

레포지토리 경로 제한이 완화되어, 경로 세그먼트의 수를 줄이거나 늘리거나, 허용 문자를 확장할 가능성이 있습니다(비 ASCII 포함). 경로는 항상 유효한 유니코드 문자열이어야 하며, MST 키(바이트 배열)로 변환 시 UTF-8 인코딩을 사용합니다.

atproto 전체 사양에서는 시간이 지남에 따라 추가적인 "blessed" 암호화 알고리즘이 도입될 수 있으며, 레코드와 블롭을 참조하기 위한 추가 CID 형식도 추가될 수 있습니다. 내부 CID 형식의 변경은 레포지토리 형식 버전 변경을 요구합니다.

레포지토리 CAR 내보내기에 연결된 "블롭"(대용량 이진 파일)이 포함될 수 있으며, 이는 기본 설정이거나 구성 가능한 옵션이 될 수 있습니다. 또는, `.tar`이나 `.zip`과 같은 다른 내보내기 메커니즘이 선택될 수도 있습니다.

레코드 콘텐츠가 언젠가는 DAG-CBOR 이외의 형식이 될 가능성도 있으며, 이 경우 레포지토리 형식 버전 변경이 필요할 것입니다. 단, 다른 데이터 형식을 DAG-CBOR 래퍼(바이트 배열 필드를 통해)로 감싸는 방식으로 효율적으로 처리할 수도 있습니다.

레포지토리 CAR 내보내기는 선호하는 블록 정렬 방식에 대한 명세가 추가될 수 있습니다.

커밋 및 MST 노드 객체에 선택적 필드를 추가하는 경우, 또는 MST 팬아웃이나 현재 MST 필드의 변경이 있는 경우, 이는 전체 레포지토리 형식 버전 변경을 초래할 수 있습니다.


---
atproto/src/app/[locale]/specs/repository/page.tsx
---
export const metadata = {
  title: 'Repository',
  description: 'Self-authenticating storage for public account content',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
atproto/src/app/[locale]/specs/lexicon/en.mdx
---
export const metadata = {
  title: 'Lexicon',
  description:
    'A schema definition language.',
}

# Lexicon

Lexicon is a schema definition language used to describe atproto records, HTTP endpoints (XRPC), and event stream messages. It builds on top of the atproto [Data Model](/specs/data-model). {{ className: 'lead' }}

The schema language is similar to [JSON Schema](http://json-schema.org/) and [OpenAPI](https://en.wikipedia.org/wiki/OpenAPI_Specification), but includes some atproto-specific features and semantics. {{ className: 'lead' }}

This specification describes version 1 of the Lexicon definition language. {{ className: 'lead' }}

## Overview of Types

| Lexicon Type | Data Model Type | Category |
| --- | --- | --- |
| `null` | Null | concrete |
| `boolean` | Boolean | concrete |
| `integer` | Integer | concrete |
| `string` | String | concrete |
| `bytes` | Bytes | concrete |
| `cid-link` | Link | concrete |
| `blob` | Blob | concrete |
| `array` | Array | container |
| `object` | Object | container |
| `params` |  | container |
| `token` |  | meta |
| `ref` |  | meta |
| `union` |  | meta |
| `unknown` |  | meta |
| `record` |  | primary |
| `query` |  | primary |
| `procedure` |  | primary |
| `subscription` |  | primary |

## Lexicon Files

Lexicons are JSON files associated with a single NSID. A file contains one or more definitions, each with a distinct short name. A definition with the name `main` optionally describes the "primary" definition for the entire file. A Lexicon with zero definitions is invalid.

A Lexicon JSON file is an object with the following fields:

- `lexicon` (integer, required): indicates Lexicon language version. In this version, a fixed value of `1`
- `id` (string, required): the NSID of the Lexicon
- `description` (string, optional): short overview of the Lexicon, usually one or two sentences
- `defs` (map of strings-to-objects, required): set of definitions, each with a distinct name (key)

Schema definitions under `defs` all have a `type` field to distinguish their type. A file can have at most one definition with one of the "primary" types. Primary types should always have the name `main`. It is possible for `main` to describe a non-primary type.

References to specific definitions within a Lexicon use fragment syntax, like `com.example.defs#someView`. If a `main` definition exists, it can be referenced without a fragment, just using the NSID. For references in the `$type` fields in data objects themselves (eg, records or contents of a union), this is a "must" (use of a `#main` suffix is invalid). For example, `com.example.record` not `com.example.record#main`.

Related Lexicons are often grouped together in the NSID hierarchy. As a convention, any definitions used by multiple Lexicons are defined in a dedicated `*.defs` Lexicon (eg, `com.atproto.server.defs`) within the group. A `*.defs` Lexicon should generally not include a definition named `main`, though it is not strictly invalid to do so.

## Primary Type Definitions

The primary types are:

- `query`: describes an XRPC Query (HTTP GET)
- `procedure`: describes an XRPC Procedure (HTTP POST)
- `subscription`: Event Stream (WebSocket)
- `record`: describes an object that can be stored in a repository record

Each primary definition schema object includes these fields:

- `type` (string, required): the type value (eg, `record` for records)
- `description` (string, optional): short, usually only a sentence or two

### Record

Type-specific fields:

- `key` (string, required): specifies the [Record Key type](/specs/record-key)
- `record` (object, required): a schema definition with type `object`, which specifies this type of record

### Query and Procedure (HTTP API)

Type-specific fields:

- `parameters` (object, optional): a schema definition with type `params`, describing the HTTP query parameters for this endpoint
- `output` (object, optional): describes the HTTP response body
    - `description` (string, optional): short description
    - `encoding` (string, required): MIME type for body contents. Use `application/json` for JSON responses.
    - `schema` (object, optional): schema definition, either an `object`, a `ref`, or a `union` of refs. Used to describe JSON encoded responses, though schema is optional even for JSON responses.
- `input` (object, optional, only for `procedure`): describes HTTP request body schema, with the same format as the `output` field
- `errors` (array of objects, optional): set of string error codes which might be returned
    - `name` (string, required): short name for the error type, with no whitespace
    - `description` (string, optional): short description, one or two sentences

### Subscription (Event Stream)

Type-specific fields:

- `parameters` (object, optional): same as Query and Procedure
- `message` (object, optional): specifies what messages can be
    - `description` (string, optional): short description
    - `schema` (object, required): schema definition, which must be a `union` of refs
- `errors` (array of objects, optional): same as Query and Procedure

Subscription schemas (referenced by the `schema` field under `message`) must be a `union` of refs, not an `object` type.

## Field Type Definitions

As with the primary definitions, every schema object includes these fields:

- `type` (string, required): fixed value for each type
- `description` (string, optional): short, usually only a sentence or two

### `null`

No additional fields.

### `boolean`

Type-specific fields:

- `default` (boolean, optional): a default value for this field
- `const` (boolean, optional): a fixed (constant) value for this field

When included as an HTTP query parameter, should be rendered as `true` or `false` (no quotes).

### `integer`

A signed integer number.

Type-specific fields:

- `minimum` (integer, optional): minimum acceptable value
- `maximum` (integer, optional): maximum acceptable value
- `enum` (array of integers, optional): a closed set of allowed values
- `default` (integer, optional): a default value for this field
- `const` (integer, optional): a fixed (constant) value for this field

### `string`

Type-specific fields:

- `format` (string, optional): string format restriction
- `maxLength` (integer, optional): maximum length of value, in UTF-8 bytes
- `minLength` (integer, optional): minimum length of value, in UTF-8 bytes
- `maxGraphemes` (integer, optional): maximum length of value, counted as Unicode Grapheme Clusters
- `minGraphemes` (integer, optional): minimum length of value, counted as Unicode Grapheme Clusters
- `knownValues` (array of strings, optional): a set of suggested or common values for this field. Values are not limited to this set (aka, not a closed enum).
- `enum` (array of strings, optional): a closed set of allowed values
- `default` (string, optional): a default value for this field
- `const` (string, optional): a fixed (constant) value for this field

Strings are Unicode. For non-Unicode encodings, use `bytes` instead. The basic `minLength`/`maxLength` validation constraints are counted as UTF-8 bytes. Note that Javascript stores strings with UTF-16 by default, and it is necessary to re-encode to count accurately. The `minGraphemes`/`maxGraphemes` validation constraints work with Grapheme Clusters, which have a complex technical and linguistic definition, but loosely correspond to "distinct visual characters" like Latin letters, CJK characters, punctuation, digits, or emoji (which might comprise multiple Unicode codepoints and many UTF-8 bytes).

`format` constrains the string format and provides additional semantic context. Refer to the Data Model specification for the available format types and their definitions.

`const` and `default` are mutually exclusive.

### `bytes`

Type-specific fields:

- `minLength` (integer, optional): minimum size of value, as raw bytes with no encoding
- `maxLength` (integer, optional): maximum size of value, as raw bytes with no encoding

In the JSON format, these fields are encoded as `{"$bytes":<base64-string>}`. In the CBOR format, these fields use the native bytes type.

### `cid-link`

No type-specific fields.

See [Data Model spec](/specs/data-model) for CID restrictions.

### `array`

Type-specific fields:

- `items` (object, required): describes the schema elements of this array
- `minLength` (integer, optional): minimum count of elements in array
- `maxLength` (integer, optional): maximum count of elements in array

In theory arrays have homogeneous types (meaning every element as the same type). However, with union types this restriction is meaningless, so implementations can not assume that all the elements have the same type.

### `object`

A generic object schema which can be nested inside other definitions by reference.

Type-specific fields:

- `properties` (map of strings-to-objects, required): defines the properties (fields) by name, each with their own schema
- `required` (array of strings, optional): indicates which properties are required
- `nullable` (array of strings, optional): indicates which properties can have `null` as a value

As described in the data model specification, there is a semantic difference in data between omitting a field; including the field with the value `null`; and including the field with a "false-y" value (`false`, `0`, empty array, etc).

### `blob`

Type-specific fields:

- `accept` (array of strings, optional): list of acceptable MIME types. Each may end in `*` as a glob pattern (eg, `image/*`). Use `*/*` to indicate that any MIME type is accepted.
- `maxSize` (integer, optional): maximum size in bytes

### `params`

This is a limited-scope type which is only ever used for the `parameters` field on `query`, `procedure`, and `subscription` primary types. These map to HTTP query parameters.

Type-specific fields:

- `required` (array of strings, optional): same semantics as field on `object`
- `properties`: similar to properties under `object`, but can only include the types `boolean`, `integer`, `string`, and `unknown`; or an `array` of one of these types

Note that unlike `object`, there is no `nullable` field on `params`.

### `token`

Tokens are empty data values which exist only to be referenced by name. They are used to define a set of values with specific meanings. The `description` field should clarify the meaning of the token. Tokens encode as string data, with the string being the fully-qualified reference to the token itself (NSID followed by an optional fragment).

Tokens are similar to the concept of a "symbol" in some programming languages, distinct from strings, variables, built-in keywords, or other identifiers.

For example, tokens could be defined to represent the state of an entity (in a state machine), or to enumerate a list of categories.

No type-specific fields.

### `ref`

Type-specific fields:

- `ref` (string, required): reference to another schema definition

Refs are a mechanism for re-using a schema definition in multiple places. The `ref` string can be a global reference to a Lexicon type definition (an NSID, optionally with a `#`-delimited name indicating a definition other than `main`), or can indicate a local definition within the same Lexicon file (a `#` followed by a name).

### `union`

Type-specific fields:

- `refs` (array of strings, required): references to schema definitions
- `closed` (boolean, optional): indicates if a union is "open" or "closed". defaults to `false` (open union)

Unions represent that multiple possible types could be present at this location in the schema. The references follow the same syntax as `ref`, allowing references to both global or local schema definitions. Actual data will validate against a single specific type: the union does not *combine* fields from multiple schemas, or define a new *hybrid* data type. The different types are referred to as **variants**.

By default unions are "open", meaning that future revisions of the schema could add more types to the list of refs (though can not remove types). This means that implementations should be permissive when validating, in case they do not have the most recent version of the Lexicon. The `closed` flag (boolean) can indicate that the set of types is fixed and can not be extended in the future.

A `union` schema definition with no `refs` is allowed and similar to `unknown`, as long as the `closed` flag is false (the default). The main difference is that the data would be required to have the `$type` field. An empty refs list with `closed` set to true is an invalid schema.

The schema definitions pointed to by a `union` are objects or types with a clear mapping to an object, like a `record`. All the variants must be represented by a CBOR map (or JSON Object) and must include a `$type` field indicating the variant type. Because the data must be an object, unions can not reference `token` (which would correspond to string data).

### `unknown`

Indicates than any data object could appear at this location, with no specific validation. The top-level data must be an object (not a string, boolean, etc). As with all other data types, the value `null` is not allowed unless the field is specifically marked as `nullable`.

The data object may contain a `$type` field indicating the schema of the data, but this is not currently required. The top-level data object must not have the structure of a compound data type, like blob (`$type: blob`) or CID link (`$link`).

The (nested) contents of the data object must still be valid under the atproto data model. For example, it should not contain floats. Nested compound types like blobs and CID links should be validated and transformed as expected.

Lexicon designers are strongly recommended to not use `unknown` fields in `record` objects for now.

No type-specific fields.

## String Formats

Strings can optionally be constrained to one of the following `format` types:

- `at-identifier`: either a [Handle](/specs/handle) or a [DID](/specs/did), details described below
- `at-uri`: [AT-URI](/specs/at-uri-scheme)
- `cid`: CID in string format, details specified in [Data Model](/specs/data-model)
- `datetime`: timestamp, details specified below
- `did`: generic [DID Identifier](/specs/did)
- `handle`: [Handle Identifier](/specs/handle)
- `nsid`: [Namespaced Identifier](/specs/nsid)
- `tid`: [Timestamp Identifier (TID)](/specs/tid)
- `record-key`: [Record Key](/specs/record-key), matching the general syntax ("any")
- `uri`: generic URI, details specified below
- `language`: language code, details specified below

For the various identifier formats, when doing Lexicon schema validation the most expansive identifier syntax format should be permitted. Problems with identifiers which do pass basic syntax validation should be reported as application errors, not lexicon data validation errors. For example, data with any kind of DID in a `did` format string field should pass Lexicon validation, with unsupported DID methods being raised separately as an application error.

### `at-identifier`

A string type which is either a DID (type: did) or a handle (handle). Mostly used in XRPC query parameters. It is unambiguous whether an at-identifier is a handle or a DID because a DID always starts with did:, and the colon character (:) is not allowed in handles.

### `datetime`

Full-precision date and time, with timezone information.

This format is intended for use with computer-generated timestamps in the modern computing era (eg, after the UNIX epoch). If you need to represent historical or ancient events, ambiguity, or far-future times, a different format is probably more appropriate. Datetimes before the Current Era (year zero) as specifically disallowed.

Datetime format standards are notoriously flexible and overlapping. Datetime strings in atproto should meet the [intersecting](https://ijmacd.github.io/rfc3339-iso8601/) requirements of the [RFC 3339](https://www.rfc-editor.org/rfc/rfc3339), [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601), and [WHATWG HTML](https://html.spec.whatwg.org/#dates-and-times) datetime standards.

The character separating "date" and "time" parts must be an upper-case `T`.

Timezone specification is required. It is *strongly* preferred to use the UTC timezone, and to represent the timezone with a simple capital `Z` suffix (lower-case is not allowed). While hour/minute suffix syntax (like `+01:00` or `-10:30`) is supported, "negative zero" (`-00:00`) is specifically disallowed (by ISO 8601).

Whole seconds precision is required, and arbitrary fractional precision digits are allowed. Best practice is to use at least millisecond precision, and to pad with zeros to the generated precision (eg, trailing `:12.340Z` instead of `:12.34Z`). Not all datetime formatting libraries support trailing zero formatting. Both millisecond and microsecond precision have reasonable cross-language support; nanosecond precision does not.

Implementations should be aware when round-tripping records containing datetimes of two ambiguities: loss-of-precision, and ambiguity with trailing fractional second zeros. If de-serializing Lexicon records into native types, and then re-serializing, the string representation may not be the same, which could result in broken hash references, sanity check failures, or repository update churn. A safer thing to do is to deserialize the datetime as a simple string, which ensures round-trip re-serialization.

Implementations "should" validate that the semantics of the datetime are valid. For example, a month or day `00` is invalid.

Valid examples:

```text
# preferred
1985-04-12T23:20:50.123Z
1985-04-12T23:20:50.123456Z
1985-04-12T23:20:50.120Z
1985-04-12T23:20:50.120000Z

# supported
1985-04-12T23:20:50.12345678912345Z
1985-04-12T23:20:50Z
1985-04-12T23:20:50.0Z
1985-04-12T23:20:50.123+00:00
1985-04-12T23:20:50.123-07:00
```

Invalid examples:

```text
1985-04-12
1985-04-12T23:20Z
1985-04-12T23:20:5Z
1985-04-12T23:20:50.123
+001985-04-12T23:20:50.123Z
23:20:50.123Z
-1985-04-12T23:20:50.123Z
1985-4-12T23:20:50.123Z
01985-04-12T23:20:50.123Z
1985-04-12T23:20:50.123+00
1985-04-12T23:20:50.123+0000

# ISO-8601 strict capitalization
1985-04-12t23:20:50.123Z
1985-04-12T23:20:50.123z

# RFC-3339, but not ISO-8601
1985-04-12T23:20:50.123-00:00
1985-04-12 23:20:50.123Z

# timezone is required
1985-04-12T23:20:50.123

# syntax looks ok, but datetime is not valid
1985-04-12T23:99:50.123Z
1985-00-12T23:20:50.123Z
```

### `uri`

Flexible to any URI schema, following the generic RFC-3986 on URIs. This includes, but isn’t limited to: `did`, `https`, `wss`, `ipfs` (for CIDs), `dns`, and of course `at`.
Maximum length in Lexicons is 8 KBytes.

### `language`

An [IETF Language Tag](https://en.wikipedia.org/wiki/IETF_language_tag) string, compliant with [BCP 47](https://www.rfc-editor.org/info/bcp47), defined in [RFC 5646](https://www.rfc-editor.org/rfc/rfc5646.txt) ("Tags for Identifying Languages"). This is the same standard used to identify languages in HTTP, HTML, and other web standards. The Lexicon string must validate as a "well-formed" language tag, as defined in the RFC. Clients should ignore language strings which are "well-formed" but not "valid" according to the RFC.

As specified in the RFC, ISO 639 two-character and three-character language codes can be used on their own, lower-cased, such as `ja` (Japanese) or `ban` (Balinese). Regional sub-tags can be added, like `pt-BR` (Brazilian Portuguese). Additional subtags can also be added, such as `hy-Latn-IT-arevela`.

Language codes generally need to be parsed, normalized, and matched semantically, not simply string-compared. For example, a search engine might simplify language tags to ISO 639 codes for indexing and filtering, while a client application (user agent) would retain the full language code for presentation (text rendering) locally.

## When to use `$type`

Data objects sometimes include a `$type` field which indicates their Lexicon type. The general principle is that this field needs to be included any time there could be ambiguity about the content type when validating data.

The specific rules are:

- `record` objects must always include `$type`. While the type is often known from context (eg, the collection part of the path for records stored in a repository), record objects can also be passed around outside of repositories and need to be self-describing
- `union` variants must always include `$type`, except at the top level of `subscription` messages

Note that `blob` objects always include `$type`, which allows generic processing.

As a reminder, `main` types must be referenced in `$type` fields as just the NSID, not including a `#main` suffix.

## Lexicon Evolution

Lexicons are allowed to change over time, within some bounds to ensure both forwards and backwards compatibility. The basic principle is that all old data must still be valid under the updated Lexicon, and new data must be valid under the old Lexicon.

- Any new fields must be optional
- Non-optional fields can not be removed. A best practice is to retain all fields in the Lexicon and mark them as deprecated if they are no longer used.
- Types can not change
- Fields can not be renamed

If larger breaking changes are necessary, a new Lexicon name must be used.

It can be ambiguous when a Lexicon has been published and becomes "set in stone". At a minimum, public adoption and implementation by a third party, even without explicit permission, indicates that the Lexicon has been released and should not break compatibility. A best practice is to clearly indicate in the Lexicon type name any experimental or development status. Eg, `com.corp.experimental.newRecord`.

## Authority and Control

The authority for a Lexicon is determined by the NSID, and rooted in DNS control of the domain authority. That authority has ultimate control over the Lexicon definition, and responsibility for maintenance and distribution of Lexicon schema definitions.

In a crisis, such as unintentional loss of DNS control to a bad actor, the protocol ecosystem could decide to disregard this chain of authority. This should only be done in exceptional circumstances, and not as a mechanism to subvert an active authority. The primary mechanism for resolving protocol disputes is to fork Lexicons in to a new namespace.

Protocol implementations should generally consider data which fails to validate against the Lexicon to be entirely invalid, and should not try to repair or do partial processing on the individual piece of data.

Unexpected fields in data which otherwise conforms to the Lexicon should be ignored. When doing schema validation, they should be treated at worst as warnings. This is necessary to allow evolution of the schema by the controlling authority, and to be robust in the case of out-of-date Lexicons.

Third parties can technically insert any additional fields they want into data. This is not the recommended way to extend applications, but it is not specifically disallowed. One danger with this is that the Lexicon may be updated to include fields with the same field names but different types, which would make existing data invalid.

## Lexicon Publication and Resolution

Lexicon schemas are published publicly as records in atproto repositories, using the `com.atproto.lexicon.schema` type. The domain name authority for [NSIDs](/specs/nsid) to specific atproto repositories (identified by [DID](/specs/did) is linked by a DNS TXT record (`_lexicon`), similar to but distinct from the [handle resolution](/specs/handle) system.

The `com.atproto.lexicon.schema` Lexicon itself is very minimal: it only requires the `lexicon` integer field, which must be `1` for this version of the Lexicon language. In practice, same fields as [Lexicon Files](#lexicon-files) should be included, along with `$type`. The record key is the NSID of the schema.

A summary of record fields:

- `$type`: must be `com.atproto.lexicon.schema` (as with all atproto records)
- `lexicon`: integer, indicates the overall version of the Lexicon (currently `1`)
- `id`: the NSID of this Lexicon. Must be a simple NSID (no fragment), and must match the record key
- `defs`: the schema definitions themselves, as a map-of-objects. Names should not include a `#` prefix.
- `description`: optional description of the overall schema; though descriptions are best included on individual defs, not the overall schema.

The `com.atproto.lexicon.schema` meta-schema is somewhat unlike other Lexicons, in that it is defined and governed as part of the protocol. Future versions of the language and protocol might not follow the evolution rules. It is an intentional decision to not express the Lexicon schema language itself recursively, using the schema language.

Authority for NSID namespaces is done at the "group" level, meaning that all NSIDs which differ only by the final "name" part are all published in the same repository. Lexicon resolution of NSIDs is not hierarchical: DNS TXT records must be created for each authority section, and resolvers should not recurse up or down the DNS hierarchy looking for TXT records.

As an example, the NSID `edu.university.dept.lab.blogging.getBlogPost` has a "name" `getBlogPost`. Removing the name and reversing the rest of the NSID gives an "authority domain name" of `blogging.lab.dept.university.edu`. To link the authority to a specific DID (say `did:plc:ewvi7nxzyoun6zhxrhs64oiz`), a DNS TXT record with the name `_lexicon.blogging.lab.dept.university.edu` and value `did=did:plc:ewvi7nxzyoun6zhxrhs64oiz` (note the `did=` prefix)  would be created. Then a record with collection `com.atproto.lexicon.schema` and record-key `edu.university.dept.lab.blogging.getBlogPost` would be created in that account's repository.

A resolving service would start with the NSID (`edu.university.dept.lab.blogging.getBlogPost`) and do a DNS TXT resolution for `_lexicon.blogging.lab.dept.university.edu`. Finding the DID, it would proceed with atproto DID resolution, look for a PDS, and then fetch the relevant record. The overall AT-URI for the record would be `at://did:plc:ewvi7nxzyoun6zhxrhs64oiz/com.atproto.lexicon.schema/edu.university.dept.lab.blogging.getBlogPost`.

If the DNS TXT resolution for `_lexicon.blogging.lab.dept.university.edu` failed, the resolving service would *NOT* try `_lexicon.lab.dept.university.edu` or `_lexicon.getBlogPost.blogging.lab.dept.university.edu` or `_lexicon.university.edu`, or any other domain name. The Lexicon resolution would simply fail.

If another NSID `edu.university.dept.lab.blogging.getBlogComments` was created, it would have the same authority name, and must be published in the same atproto repository (with a different record key). If a Lexicon for `edu.university.dept.lab.gallery.photo` was published, a new DNS TXT record would be required (`_lexicon.gallery.lab.dept.university.edu`; it could point at the same repository (DID), or a different repository.

As a simpler example, an NSID `app.toy.record` would resolve via `_lexicon.toy.app`.

A single repository can host Lexicons for multiple authority domains, possibly across multiple registered domains and TLDs. Resolution DNS records can change over time, moving schema resolution to different repositories, though it may take time for DNS and cache changes to propagate.

Note that Lexicon record operations are broadcast over repository event streams ("firehose"), but that DNS resolution changes do not (unlike handle changes). Resolving services should not cache DNS resolution results for long time periods.

## Usage and Implementation Guidelines

It should be possible to translate Lexicon schemas to JSON Schema or OpenAPI and use tools and libraries from those ecosystems to work with atproto data in JSON format.

Implementations which serialize and deserialize data from JSON or CBOR into structures derived from specific Lexicons should be aware of the risk of "clobbering" unexpected fields. For example, if a Lexicon is updated to add a new (optional) field, old implementations would not be aware of that field, and might accidentally strip the data when de-serializing and then re-serializing. Depending on the context, one way to avoid this problem is to retain any "extra" fields, or to pass-through the original data object instead of re-serializing it.

## Possible Future Changes

The validation rules for unexpected additional fields may change. For example, a mechanism for Lexicons to indicate that the schema is "closed" and unexpected fields are not allowed, or a convention around field name prefixes (`x-`) to indicate unofficial extension.


---
atproto/src/app/[locale]/specs/lexicon/ko.mdx
---
export const metadata = {
  title: 'Lexicon',
  description:
    '스키마 정의 언어입니다.',
}

# Lexicon

Lexicon은 atproto 레코드, HTTP 엔드포인트(XRPC) 및 이벤트 스트림 메시지를 설명하기 위해 사용되는 스키마 정의 언어입니다. 이는 atproto의 [Data Model](/specs/data-model)을 기반으로 합니다. {{ className: 'lead' }}

이 스키마 언어는 [JSON Schema](http://json-schema.org/) 및 [OpenAPI](https://en.wikipedia.org/wiki/OpenAPI_Specification)와 유사하지만, atproto 전용 기능과 의미론을 포함합니다. {{ className: 'lead' }}

이 명세는 Lexicon 정의 언어의 버전 1을 설명합니다. {{ className: 'lead' }}

## 타입 개요

| Lexicon Type | Data Model Type | Category |
| --- | --- | --- |
| `null` | Null | concrete |
| `boolean` | Boolean | concrete |
| `integer` | Integer | concrete |
| `string` | String | concrete |
| `bytes` | Bytes | concrete |
| `cid-link` | Link | concrete |
| `blob` | Blob | concrete |
| `array` | Array | container |
| `object` | Object | container |
| `params` |  | container |
| `token` |  | meta |
| `ref` |  | meta |
| `union` |  | meta |
| `unknown` |  | meta |
| `record` |  | primary |
| `query` |  | primary |
| `procedure` |  | primary |
| `subscription` |  | primary |

## Lexicon 파일

Lexicon은 단일 NSID와 연관된 JSON 파일입니다. 파일은 하나 이상의 정의를 포함하며, 각 정의는 고유한 짧은 이름을 갖습니다. 이름이 `main`인 정의는 파일 전체에 대한 "주요" 정의를 선택적으로 설명합니다. 정의가 하나도 없는 Lexicon은 유효하지 않습니다.

Lexicon JSON 파일은 다음 필드를 가진 객체입니다:

- `lexicon` (정수, 필수): Lexicon 언어 버전을 나타냅니다. 이 버전에서는 고정값 `1`
- `id` (문자열, 필수): Lexicon의 NSID
- `revision` (정수, 선택): 변경 사항이 발생한 경우 Lexicon의 버전을 나타냅니다
- `description` (문자열, 선택): 보통 한두 문장으로 구성된 Lexicon에 대한 간단한 개요
- `defs` (문자열-객체의 맵, 필수): 각기 다른 이름(키)을 가진 정의들의 집합

`defs` 아래의 스키마 정의들은 모두 자신들의 타입을 구분하기 위해 `type` 필드를 포함합니다. 파일은 "기본" 타입(definition) 중 하나를 가질 수 있으며, 이 경우 이름은 항상 `main`이어야 합니다. `main`이 기본 타입이 아닌 타입을 설명하는 것도 가능합니다.

Lexicon 내 특정 정의에 대한 참조는 조각(fragment) 구문(예: `com.example.defs#someView`)을 사용합니다. `main` 정의가 존재할 경우 NSID만으로 참조할 수 있으며, 조각(`#main`)을 붙여서는 안 됩니다. 예를 들어, 데이터 객체(레코드나 union 내의 값)의 `$type` 필드 내 참조는 반드시 NSID만 사용해야 하며, `#main` 접미사를 사용해서는 안 됩니다. 예: `com.example.record` (올바름) vs `com.example.record#main` (잘못됨).

`revision` 필드의 의미론은 아직 완전히 확립되지 않았으나, 이는 여러 버전이나 복사본 중 가장 최신의 Lexicon을 식별하는 데 도움이 될 의도로 마련되었습니다.

관련 Lexicon들은 종종 NSID 계층 내에서 함께 그룹화됩니다. 관례상 여러 Lexicon에서 사용되는 정의들은 전용 `*.defs` Lexicon(예: `com.atproto.server.defs`)에 정의됩니다. 일반적으로 `*.defs` Lexicon은 `main`이라는 이름의 정의를 포함하지 않아야 하지만, 반드시 그렇다는 규칙은 아닙니다.

## 기본 타입 정의

기본 타입은 다음과 같습니다:

- `query`: XRPC Query (HTTP GET)를 설명합니다.
- `procedure`: XRPC Procedure (HTTP POST)를 설명합니다.
- `subscription`: 이벤트 스트림(WebSocket)
- `record`: 저장소 레코드에 저장될 수 있는 객체를 설명합니다.

각 기본 정의 스키마 객체는 다음 필드를 포함합니다:

- `type` (문자열, 필수): 타입 값 (예: 레코드의 경우 `record`)
- `description` (문자열, 선택): 보통 한두 문장으로 구성된 간단한 설명

### Record

타입 별 추가 필드:

- `key` (문자열, 필수): [Record Key type](/specs/record-key)를 지정합니다.
- `record` (객체, 필수): 이 레코드 타입을 지정하는 `object` 타입의 스키마 정의

### Query 및 Procedure (HTTP API)

타입 별 추가 필드:

- `parameters` (객체, 선택): 이 엔드포인트의 HTTP 쿼리 매개변수를 설명하는 `params` 타입의 스키마 정의
- `output` (객체, 선택): HTTP 응답 본문을 설명합니다.
    - `description` (문자열, 선택): 간단한 설명
    - `encoding` (문자열, 필수): 본문 내용의 MIME 타입. JSON 응답의 경우 `application/json`을 사용합니다.
    - `schema` (객체, 선택): `object`, `ref` 또는 refs의 `union` 중 하나인 스키마 정의. JSON 인코딩 응답을 설명하는 데 사용되며, JSON 응답의 경우에도 스키마는 선택적입니다.
- `input` (객체, 선택, `procedure` 전용): HTTP 요청 본문 스키마를 설명하며, `output` 필드와 동일한 형식을 따릅니다.
- `errors` (객체 배열, 선택): 반환될 수 있는 문자열 오류 코드 집합
    - `name` (문자열, 필수): 공백 없이 오류 타입의 간단한 이름
    - `description` (문자열, 선택): 한두 문장 정도의 간단한 설명

### Subscription (이벤트 스트림)

타입 별 추가 필드:

- `parameters` (객체, 선택): Query 및 Procedure와 동일
- `message` (객체, 선택): 메시지의 유형을 지정합니다.
    - `description` (문자열, 선택): 간단한 설명
    - `schema` (객체, 필수): 스키마 정의. 반드시 refs의 `union` 타입이어야 합니다.
- `errors` (객체 배열, 선택): Query 및 Procedure와 동일

`message` 아래 참조되는 Subscription 스키마는 반드시 refs의 `union` 타입이어야 하며, `object` 타입은 허용되지 않습니다.

## 필드 타입 정의

기본 정의와 마찬가지로, 모든 스키마 객체는 다음 필드를 포함합니다:

- `type` (문자열, 필수): 각 타입에 대한 고정 값
- `description` (문자열, 선택): 보통 한두 문장 정도의 간단한 설명

### `null`

추가 필드 없음.

### `boolean`

타입 별 추가 필드:

- `default` (boolean, 선택): 이 필드의 기본값
- `const` (boolean, 선택): 이 필드의 고정(상수) 값

HTTP 쿼리 매개변수로 포함될 경우, `true` 또는 `false` (따옴표 없이)로 렌더링되어야 합니다.

### `integer`

부호가 있는 정수 숫자.

타입 별 추가 필드:

- `minimum` (정수, 선택): 허용되는 최소 값
- `maximum` (정수, 선택): 허용되는 최대 값
- `enum` (정수 배열, 선택): 허용되는 값들의 폐쇄 집합
- `default` (정수, 선택): 이 필드의 기본값
- `const` (정수, 선택): 이 필드의 고정(상수) 값

### `string`

타입 별 추가 필드:

- `format` (문자열, 선택): 문자열 형식 제한
- `maxLength` (정수, 선택): UTF-8 바이트 단위의 최대 길이
- `minLength` (정수, 선택): UTF-8 바이트 단위의 최소 길이
- `maxGraphemes` (정수, 선택): 유니코드 그래프 클러스터(시각적 문자) 기준 최대 길이
- `minGraphemes` (정수, 선택): 유니코드 그래프 클러스터 기준 최소 길이
- `knownValues` (문자열 배열, 선택): 이 필드에 대한 추천 또는 일반적인 값들의 집합 (폐쇄 enum은 아님)
- `enum` (문자열 배열, 선택): 허용되는 값들의 폐쇄 집합
- `default` (문자열, 선택): 이 필드의 기본값
- `const` (문자열, 선택): 이 필드의 고정(상수) 값

문자열은 유니코드입니다. 유니코드가 아닌 인코딩의 경우 `bytes`를 사용하십시오. 기본적인 `minLength`/`maxLength` 검증은 UTF-8 바이트 수를 기준으로 합니다. Javascript는 문자열을 기본적으로 UTF-16으로 저장하므로 정확한 바이트 수 계산을 위해 재인코딩이 필요합니다. `minGraphemes`/`maxGraphemes` 검증은 복잡한 기술적 및 언어학적 정의를 가진 그래프 클러스터를 기준으로 하지만, 대략적으로는 라틴 문자, CJK 문자, 구두점, 숫자, 또는 이모지(여러 유니코드 코드포인트 및 다수의 UTF-8 바이트로 구성될 수 있음)와 같은 "시각적 문자"에 해당합니다.

`format`은 문자열의 형식을 제한하고 추가 의미를 제공합니다. 사용 가능한 형식 및 그 정의에 대해서는 데이터 모델 명세를 참조하십시오.

`const`와 `default`는 서로 배타적입니다.

### `bytes`

타입 별 추가 필드:

- `minLength` (정수, 선택): 인코딩 없이 원시 바이트 기준 최소 크기
- `maxLength` (정수, 선택): 인코딩 없이 원시 바이트 기준 최대 크기

### `cid-link`

추가 필드 없음.

CID 제한에 대해서는 [Data Model spec](/specs/data-model)을 참조하십시오.

### `array`

타입 별 추가 필드:

- `items` (객체, 필수): 이 배열의 요소들을 설명하는 스키마
- `minLength` (정수, 선택): 배열 요소의 최소 개수
- `maxLength` (정수, 선택): 배열 요소의 최대 개수

이론상 배열은 동질적 타입(모든 요소가 동일한 타입)을 가지지만, union 타입의 경우 이러한 제한은 의미가 없으므로 구현체는 모든 요소가 동일한 타입임을 가정할 수 없습니다.

### `object`

다른 정의 내에서 참조될 수 있는 일반 객체 스키마.

타입 별 추가 필드:

- `properties` (문자열-객체의 맵, 필수): 각 필드의 이름과 해당 스키마를 정의합니다.
- `required` (문자열 배열, 선택): 필수 속성을 나타냅니다.
- `nullable` (문자열 배열, 선택): `null` 값을 가질 수 있는 속성을 나타냅니다.

데이터 모델 명세에서 설명된 바와 같이, 필드를 생략하는 것, 필드를 `null` 값으로 포함하는 것, 또는 "false-y" 값(예: `false`, `0`, 빈 배열 등)을 포함하는 것 사이에는 의미론적 차이가 있습니다.

### `blob`

타입 별 추가 필드:

- `accept` (문자열 배열, 선택): 허용되는 MIME 타입 목록. 각 타입은 glob 패턴(`image/*` 등)으로 끝날 수 있습니다. 모든 MIME 타입을 허용하려면 `*/*`를 사용합니다.
- `maxSize` (정수, 선택): 최대 크기 (바이트 단위)

### `params`

이 타입은 `query`, `procedure`, `subscription`의 `parameters` 필드에만 사용되는 제한된 범위의 타입입니다. 이는 HTTP 쿼리 매개변수에 매핑됩니다.

타입 별 추가 필드:

- `required` (문자열 배열, 선택): `object`의 `required`와 동일한 의미
- `properties`: `object`의 properties와 유사하지만, 포함할 수 있는 타입은 `boolean`, `integer`, `string`, `unknown` 또는 이들 중 하나의 `array`로 제한됩니다.

`object`와 달리 `params`에는 `nullable` 필드가 없습니다.

### `token`

토큰은 참조를 위해서만 존재하는 빈 데이터 값입니다. 이는 특정 의미를 가진 값 집합을 정의하는 데 사용됩니다. `description` 필드는 토큰의 의미를 명확히 해야 합니다. 토큰은 문자열 데이터로 인코딩되며, 문자열은 토큰 자체(NSID 뒤에 선택적 조각)의 완전한 참조를 나타냅니다.

토큰은 일부 프로그래밍 언어의 "심볼" 개념과 유사하며, 문자열, 변수, 내장 키워드 등과 구분됩니다.

예를 들어, 토큰은 엔티티의 상태(상태 머신)나 범주 목록을 열거하기 위해 정의될 수 있습니다.

추가 필드 없음.

### `ref`

타입 별 추가 필드:

- `ref` (문자열, 필수): 다른 스키마 정의에 대한 참조

ref는 여러 위치에서 스키마 정의를 재사용할 수 있도록 하는 메커니즘입니다. `ref` 문자열은 Lexicon 타입 정의(전역 NSID, 선택적 `#` 구분 이름)나 같은 Lexicon 파일 내의 로컬 정의(이름 앞에 `#`)를 참조할 수 있습니다.

### `union`

타입 별 추가 필드:

- `refs` (문자열 배열, 필수): 스키마 정의에 대한 참조
- `closed` (boolean, 선택): union이 "열림"인지 "닫힘"인지를 나타냅니다. 기본값은 `false` (열림)

union은 이 위치에서 여러 가능한 타입이 올 수 있음을 나타냅니다. 참조는 `ref`와 동일한 구문을 따르며, 전역 참조와 로컬 참조 모두 허용됩니다. 실제 데이터는 단일 구체 타입에 대해 검증됩니다: union은 여러 스키마의 필드를 *조합*하거나 새로운 *하이브리드* 데이터 타입을 정의하지 않습니다. 이때 각 다른 타입은 **variant**라고 부릅니다.

기본적으로 union은 "열림"으로, 이는 향후 명세의 개정 시 refs 목록에 새로운 타입이 추가될 수 있음을 의미합니다. 따라서 구현체는 최신 Lexicon이 아닐 수 있음을 감안하여 검증할 때 관대해야 합니다. `closed` 플래그(true)를 설정하면 타입 목록이 고정되었음을 나타냅니다.

`refs`가 없는 `union` 스키마 정의는 `closed` 플래그가 false(기본값)인 경우 `unknown`과 유사하게 허용됩니다. 주요 차이점은 데이터에 `$type` 필드가 필요하다는 점입니다. `closed`가 true인 빈 refs 목록은 유효하지 않은 스키마입니다.

`union`이 참조하는 스키마 정의들은 모두 객체 혹은 객체로 매핑이 명확한 타입이어야 하며, 각 variant는 CBOR map(또는 JSON Object)로 표현되고 variant 타입을 나타내는 `$type` 필드를 포함해야 합니다. 데이터는 객체여야 하므로 union은 `token`을 참조할 수 없습니다(토큰은 문자열 데이터에 해당).

### `unknown`

이 타입은 특정 검증 없이 어떤 데이터 객체든 올 수 있음을 나타냅니다. 최상위 데이터는 객체여야 하며(문자열, 불리언 등은 안 됨), 해당 위치에 `null`도 허용되지 않습니다(필드가 `nullable`로 명시되지 않은 한).

데이터 객체는 스키마를 나타내는 `$type` 필드를 포함할 수 있지만, 이는 필수가 아닙니다. 최상위 데이터 객체는 blob(`$type: blob`)이나 CID link(`$link`)와 같은 복합 데이터 타입의 구조를 가져서는 안 됩니다.

데이터 모델에 따라 데이터 객체의 (중첩된) 내용은 여전히 유효해야 합니다. 예를 들어, 부동소수점 수는 포함하면 안 됩니다. blob이나 CID link와 같은 중첩 복합 타입은 예상대로 검증 및 변환되어야 합니다.

Lexicon 디자이너는 당분간 `record` 객체 내에서 `unknown` 필드의 사용을 자제할 것을 강력히 권장합니다.

추가 필드 없음.

## 문자열 형식

문자열은 선택적으로 다음 `format` 타입 중 하나로 제한될 수 있습니다:

- `at-identifier`: [Handle](/specs/handle) 또는 [DID](/specs/did)를 의미하며, 아래에서 자세히 설명됩니다.
- `at-uri`: [AT-URI](/specs/at-uri-scheme)
- `cid`: [Data Model](/specs/data-model)에 지정된 형식의 CID 문자열
- `datetime`: 아래에 설명된 타임스탬프
- `did`: 일반적인 [DID Identifier](/specs/did)
- `handle`: [Handle Identifier](/specs/handle)
- `nsid`: [Namespaced Identifier](/specs/nsid)
- `tid`: [Timestamp Identifier (TID)](/specs/tid)
- `record-key`: [Record Key](/specs/record-key), 일반적인 구문("any")을 따름
- `uri`: 일반 URI, 아래에 설명됨
- `language`: 아래에 설명된 언어 코드

여러 식별자 형식의 경우, Lexicon 스키마 검증 시 가능한 한 포괄적인 식별자 구문을 허용해야 합니다. 기본 구문 검증을 통과하지 못하는 식별자에 대해서는 Lexicon 데이터 검증 오류가 아닌 애플리케이션 오류로 보고되어야 합니다. 예를 들어, `did` 형식 문자열 필드에 어떠한 DID가 포함되더라도 Lexicon 검증은 통과해야 하며, 지원되지 않는 DID 메서드는 별도의 애플리케이션 오류로 처리됩니다.

### `at-identifier`

DID(`did`) 또는 핸들(`handle`) 중 하나인 문자열 타입입니다. DID는 항상 `did:`로 시작하며, 콜론(`:`)은 핸들에 허용되지 않으므로 at-identifier가 핸들과 DID 중 어느 것인지 명확히 구분됩니다.

### `datetime`

타임존 정보를 포함한 정밀한 날짜 및 시간.

이 형식은 현대 컴퓨팅 시대(예: UNIX epoch 이후)의 컴퓨터 생성 타임스탬프에 사용하기 위한 것입니다. 만약 역사적 사건, 모호성 또는 먼 미래의 시간을 나타내야 한다면 다른 형식을 사용하는 것이 바람직합니다. 연대기 이전(기원전)은 명시적으로 허용되지 않습니다.

날짜 및 시간 형식 표준은 매우 유연하며 서로 겹칠 수 있습니다. atproto의 datetime 문자열은 [RFC 3339](https://www.rfc-editor.org/rfc/rfc3339), [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) 및 [WHATWG HTML](https://html.spec.whatwg.org/#dates-and-times) 표준의 교차 조건을 충족해야 합니다.

날짜와 시간 사이를 구분하는 문자는 반드시 대문자 `T`여야 합니다.

타임존 지정은 필수이며, UTC 타임존 사용과 대문자 `Z` 접미사(소문자 사용 불가)를 강력히 권장합니다. 시/분 형식(예: `+01:00` 또는 `-10:30`)도 지원되지만, ISO 8601에 따라 "음의 0"(`-00:00`)은 명시적으로 허용되지 않습니다.

전체 초(second) 정밀도가 필수이며, 임의의 소수점 이하 자릿수는 허용됩니다. 최소한 밀리초 정밀도를 사용하고 생성된 정밀도에 맞게 0으로 채워주는 것이 모범 사례입니다(예: `:12.340Z`는 `:12.34Z` 대신). 모든 datetime 포맷 라이브러리가 후행 0 포맷을 지원하는 것은 아닙니다. 밀리초와 마이크로초 정밀도는 언어 간에 reasonably 지원되지만, 나노초 정밀도는 그렇지 않습니다.

구현체는 정밀도 손실 및 후행 소수점 0의 모호성과 같은 datetime의 양방향 문제에 주의해야 합니다. 만약 Lexicon 레코드를 네이티브 타입으로 직렬화 및 역직렬화 한 후 재직렬화할 경우, 문자열 표현이 달라져 해시 참조, 무결성 검증 또는 저장소 업데이트에 문제가 생길 수 있습니다. 따라서 datetime을 단순 문자열로 역직렬화하여 원본 문자열을 유지하는 것이 안전합니다.

구현체는 datetime의 의미론적 유효성(예: 월이나 일이 `00`인 경우 등)도 검증해야 합니다.

유효한 예제:

```text
# 선호
1985-04-12T23:20:50.123Z
1985-04-12T23:20:50.123456Z
1985-04-12T23:20:50.120Z
1985-04-12T23:20:50.120000Z

# 지원
1985-04-12T23:20:50.12345678912345Z
1985-04-12T23:20:50Z
1985-04-12T23:20:50.0Z
1985-04-12T23:20:50.123+00:00
1985-04-12T23:20:50.123-07:00
```

잘못된 예제:

```text
1985-04-12
1985-04-12T23:20Z
1985-04-12T23:20:5Z
1985-04-12T23:20:50.123
+001985-04-12T23:20:50.123Z
23:20:50.123Z
-1985-04-12T23:20:50.123Z
1985-4-12T23:20:50.123Z
01985-04-12T23:20:50.123Z
1985-04-12T23:20:50.123+00
1985-04-12T23:20:50.123+0000

# ISO-8601 엄격한 대소문자 구분
1985-04-12t23:20:50.123Z
1985-04-12T23:20:50.123z

# RFC-3339는 맞으나, ISO-8601은 아님
1985-04-12T23:20:50.123-00:00
1985-04-12 23:20:50.123Z

# 타임존 지정은 필수
1985-04-12T23:20:50.123

# 구문은 올바르나, datetime 값이 유효하지 않음
1985-04-12T23:99:50.123Z
1985-00-12T23:20:50.123Z
```

### `uri`

RFC-3986에 따라 URI 스키마에 유연한 모든 URI 형식을 지원합니다. 여기에는 `did`, `https`, `wss`, CID를 위한 `ipfs`, `dns` 및 물론 `at` 등이 포함됩니다.
Lexicon에서 최대 길이는 8 KBytes입니다.

### `language`

[IETF Language Tag](https://en.wikipedia.org/wiki/IETF_language_tag) 문자열로, [BCP 47](https://www.rfc-editor.org/info/bcp47)을 준수하며, [RFC 5646](https://www.rfc-editor.org/rfc/rfc5646.txt) ("언어 식별을 위한 태그")에 정의되어 있습니다. 이는 HTTP, HTML 등 다른 웹 표준에서 언어를 식별하는 동일한 표준입니다. Lexicon 문자열은 RFC에 정의된 "well-formed" 언어 태그여야 하며, 클라이언트는 "well-formed"하지만 RFC에 따라 "유효하지 않은" 언어 문자열은 무시해야 합니다.

RFC에 명시된 대로, 소문자로 된 ISO 639의 2자 또는 3자 언어 코드를 단독으로 사용할 수 있으며, 예를 들어 `ja` (일본어)나 `ban` (발리어) 등이 이에 해당합니다. 지역 서브태그는 `pt-BR` (브라질 포르투갈어)와 같이 추가될 수 있습니다. 그 외에도 `hy-Latn-IT-arevela`와 같이 추가 서브태그가 붙을 수 있습니다.

언어 코드는 단순 문자열 비교가 아닌 파싱, 정규화 및 의미론적 매칭이 필요합니다. 예를 들어, 검색 엔진은 색인 및 필터링을 위해 언어 태그를 ISO 639 코드로 단순화할 수 있지만, 클라이언트 애플리케이션은 전체 언어 코드를 로컬에서 유지할 수 있어야 합니다.

## 언제 `$type`을 사용할까

데이터 객체는 가끔 `$type` 필드를 포함하여 Lexicon 타입을 나타냅니다. 이 필드는 데이터 검증 시 내용 타입에 대해 모호성이 있을 경우 반드시 포함되어야 합니다.

구체적인 규칙은 다음과 같습니다:

- `record` 객체는 항상 `$type`을 포함해야 합니다. 레코드가 저장소의 컬렉션 경로 등에서 타입이 추론될 수 있으나, 레코드 객체는 저장소 외부로 전달될 수 있으므로 자체적으로 타입을 명시해야 합니다.
- `union`의 variant들은 (subscription 메시지의 최상위 제외) 항상 `$type`을 포함해야 합니다.

또한, `blob` 객체는 항상 `$type`을 포함하여 일반적인 처리를 가능하게 합니다.

참고로, `main` 타입은 `$type` 필드에서 NSID만 사용해야 하며, `#main` 접미사는 사용하면 안 됩니다.

## Lexicon 진화

Lexicon은 전진 및 후진 호환성을 보장하는 범위 내에서 시간이 지남에 따라 변경될 수 있습니다. 기본 원칙은 기존의 모든 데이터는 업데이트된 Lexicon에서도 유효해야 하며, 새로운 데이터는 구버전 Lexicon에서도 유효해야 한다는 것입니다.

- 새로운 필드는 모두 선택적이어야 합니다.
- 필수 필드는 제거할 수 없습니다. 더 이상 사용되지 않는 필드는 사용 중단(deprecated)으로 표시하는 것이 모범 사례입니다.
- 타입은 변경될 수 없습니다.
- 필드 이름은 변경될 수 없습니다.

보다 큰 변경이 필요한 경우에는 새로운 Lexicon 이름을 사용해야 합니다.

Lexicon이 "확정"되었다는 것을 판단하는 것은 모호할 수 있습니다. 최소한 공개 채택 및 타 제3자의 구현이 이루어진 경우 해당 Lexicon은 출시된 것으로 간주되어 호환성을 깨뜨려서는 안 됩니다. 모범 사례로, 실험적 또는 개발 중임을 나타내기 위해 Lexicon 타입 이름에 이를 명시하는 것이 좋습니다. 예: `com.corp.experimental.newRecord`.

## 권한 및 제어

Lexicon의 권한은 NSID에 의해 결정되며, DNS의 도메인 권한에 근거합니다. 해당 권한은 Lexicon 정의에 대한 최종 통제권을 가지며, Lexicon 스키마 정의의 유지보수 및 배포에 책임을 집니다.

DNS 제어권을 잃어 부정적인 행위자가 나타나는 위기 상황에서는 프로토콜 생태계가 이 권한 체계를 무시할 수 있습니다. 이는 예외적인 상황에서만 적용되어야 하며, 활동 중인 권한을 전복시키기 위한 수단으로 사용되어서는 안 됩니다. 프로토콜 분쟁 해결의 기본 메커니즘은 Lexicon을 새로운 네임스페이스로 포크하는 것입니다.

프로토콜 구현체는 Lexicon 검증에 실패한 데이터를 완전히 유효하지 않은 것으로 간주하고, 개별 데이터에 대해 부분적인 처리나 복구를 시도해서는 안 됩니다.

Lexicon에 부합하는 나머지 데이터 내의 예상치 못한 필드는 무시되어야 합니다. 스키마 검증 시 이러한 필드는 경고 정도로 취급되어야 합니다. 이는 제어 권한에 의한 명세의 진화와 구버전 Lexicon을 고려한 강건함을 보장하기 위함입니다.

제3자는 기술적으로 데이터 내에 추가 필드를 삽입할 수 있으나, 이는 권장되는 확장 방식은 아닙니다. 다만, Lexicon이 나중에 동일한 이름의 필드를 다른 타입으로 추가할 경우 기존 데이터가 유효하지 않게 될 위험이 있습니다.

## 사용 및 구현 지침

Lexicon 스키마는 JSON Schema나 OpenAPI로 변환되어, 해당 생태계의 도구 및 라이브러리를 활용해 JSON 형식의 atproto 데이터를 처리할 수 있어야 합니다.

JSON 또는 CBOR에서 Lexicon 기반의 구조로 데이터를 직렬화/역직렬화하는 구현체는 예상치 못한 필드가 "덮어쓰기(clobber)"되는 위험에 주의해야 합니다. 예를 들어, Lexicon이 새로운(선택적) 필드를 추가할 경우, 구버전 구현체는 이를 인지하지 못해 역직렬화 후 재직렬화 시 해당 데이터가 누락될 수 있습니다. 이를 방지하기 위한 한 가지 방법은 모든 "추가" 필드를 유지하거나, 원본 데이터 객체를 그대로 전달하는 것입니다.

## 향후 변경 가능성

예상치 못한 추가 필드에 대한 검증 규칙은 변경될 수 있습니다. 예를 들어, 명세가 "닫힌(closed)" 스키마를 나타내어 예상치 못한 필드를 허용하지 않거나, 비공식 확장을 나타내기 위해 필드 이름 접두사(`x-`)에 관한 규약이 생길 수 있습니다.


---
atproto/src/app/[locale]/specs/lexicon/page.tsx
---
export const metadata = {
  title: 'Lexicon',
  description: 'A schema definition language.',
}

export default async function HomePage({ params }: any) {
  try {
    const Content = (await import(`./${params.locale}.mdx`)).default
    return <Content />
  } catch (error) {
    const Content = (await import(`./en.mdx`)).default
    return <Content />
  }
}


---
