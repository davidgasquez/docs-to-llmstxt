atproto/src/app/[locale]/guides/account-lifecycle/en.mdx
---
export const header = {
  title: 'Accounts and deletions',
  description:
    'How account lifecycles and deletions work in AT Protocol',
}

## Account lifecycles

This document complements the [Account Hosting](/specs/account) specification, which gives a high-level overview of account lifecycles. It summarizes the expected behaviors for a few common account lifecycle transitions, and what firehose events are expected in what order. Software is generally expected to be resilient to partial or incorrect event transmission.

Understanding account lifecycles also helps understand record deletions in AT Protocol.

## New Account Creation

When an account is registered on a PDS and a new identity (DID) is created:

- the PDS will generate or confirm the existence of the account's identity (DID and handle). Once the DID is in a confirmed state that can be resolved by other services in the network, and points to the current PDS instance, the PDS emits an `#identity` event. It is good, but not required, to wait until the handle is resolvable by third parties before emitting the event (especially, but not only, if the PDS is providing a handle for the account). The account status may or may not be `active` when this event is emitted.
- once the account creation has completed, and the PDS will respond with `active` to API requests for account status, an `#account` event can be emitted
- when the account's repository is initialized with a `rev` and `commit`, a `#commit` message can be emitted. The initial repo may be "empty" (no records), or may contain records.
- the specific order of events is not formally specified, but the recommended order is: `#identity`, `#account`, `#commit`
- downstream services process and pass through these events.

## Account Migration

The relevant events and behaviors when migrating an account to a new PDS:

- the new PDS will not emit any events on initial account creation. The account state will be `deactivated` on the new PDS (which will reply as such to API requests)
- when the identity is updated and confirmed by the new PDS, it should emit an `#identity` event
- when the account is switched to `active` on the new PDS, it should emit an `#account` event and a `#commit` event; the order is not formally required, but doing `#account` first is recommended. Ideally the `#commit` event will be empty (no new records), but signed with any new signing key, and have a new/incremented `rev`.
- when the account is deactivated on the old PDS, it should emit an `#account` event, indicating that the account is inactive and has status `deactivated`.
- Relays should ignore `#account` and `#commit` events which are not coming from the currently declared PDS instance for the identity: these should not be passed through to the output firehose. Further, they should ignore `#commit` events when the local account status is not `active`. Overall, this means that account migration should result in three events coming from the relay: an `#identity` (from new PDS), an `#account` (from new PDS), and a `#commit` (from new PDS). The `#account` from the old PDS is usually ignored.
- downstream services (eg, AppView) should update their identity cache, and increment the account's `rev` (when the `#commit` is received), but otherwise don't need to take any action.

## Account Deletion

- PDS emits an `#account` event, with `active` false and status `deleted`.
- Relay updates local account status for the repo, and passes through the `#account` event. If the Relay is a full mirror, it immediately stops serving `getRepo`, `getRecord`, and similar API requests for the account, indicating the reason in the response error. The Relay may fully delete repo content locally according to local policy. The firehose backfill window does not need to be immediately purged of commit events for the repo, as long as the backfill window is time-limited.
- the PDS should not emit `#commit` events for an account which is not "active". If any further `#commit` messages are emitted for the repo (eg, by accident or out-of-order processing or delivery), all downstream services should ignore the event and not pass it through
- downstream services (eg, AppView) **should** immediately stop serving/distributing content for the account. They *may* defer permanent data deletion according to local policy. Updating aggregations (eg, record counts) may also be deferred or processed in a background queue according to policy and implementation. Error status messages may indicate either that the content is "gone" (existed, but no longer available), or that content is "not found" (not revealing that content existed previously)

<Note>
The behavior of downstream services is a recurring theme throughout this guide. Due to the nature of the AT Protocol, downstream services may go out of sync with [firehose events](/guides/streaming-data), and may fail to process requests such as deletion requests. Records that have been requested to be deleted, or have been removed from a self-hosted PDS, may still exist elsewhere if they were processed by a Relay and ingested into another application or data model that does not follow the AT Proto spec ‚Äî i.e., doesn't resolve record requests back to the origin PDS. Full records are [deliberately not included](https://github.com/bluesky-social/indigo/issues/927) with delete events on the firehose to further mitigate this. As a rule, the PDS *should* be considered the source of truth for hosted accounts.
</Note>

Account takedowns work similarly to account deletion.

## Account Deactivation

- PDS emits an `#account` event, with `active` false and status `deactivated`.
- similar to deletion, Relay processes the event, stops redistributing content, and passes through the event. The Relay should not fully purge content locally, though it may eventually delete local copies if the deactivation status persists for a long time (according to local policy).
- similar to deletion, `#commit` events should not be emitted by the PDS, and should be ignored and not passed through if received by Relays
- downstream services (eg, AppViews) should make content unavailable, but do not need to delete data locally. They should indicate account/content status as "unavailable"; best practice is to specifically indicate that this is due to account deactivation.

Account suspension works similarly to deactivation.

## Account Reactivation

- PDS emits an `#account` event, with `active` status.
- Relay verifies that the account reactivation is valid, eg that it came from the current PDS instance for the identity. It updates local account status, and passes through the event.
- any downstream services (eg, AppViews) should update local account status for the account.
- any service which does not have any current repo content for the account (eg, because it was previously deleted) may fetch a repo [CAR](/specs/repository#car-file-serialization) export and process it as a background tasks. An ‚Äúupstream‚Äù host (like a relay) may have a repo copy, or the service might connect directly to the account's PDS host. They are not required to do so, and might instead wait for a `#commit` event.
- if the account was previously deleted or inactive for a long time, it is a best practice for the PDS to emit an empty `#commit` event after reactivation to ensure downstream services are synchronized.

## Further Reading and Resources

- [Reads and Writes](/guides/reads-and-writes)
- [Reading data](/guides/reading-data)
- [Writing data](/guides/writing-data)
- [Social graph](/guides/social-graph)
- [Record Key spec](/specs/record-key)
- [URI scheme](/specs/at-uri-scheme)

---
atproto/src/app/[locale]/guides/account-migration/en.mdx
---
export const header = {
  title: 'Account Migration',
  description:
    'Migrating an account to a new PDS',
}

This document complements the [Account Hosting](/specs/account) specification, which gives a high-level overview of account lifecycles. It breaks down the individual migration steps, both for an "easy" migration (when both PDS instances participate), and for account recovery scenarios. Note that these specific mechanisms are not a formal part of the protocol, and may evolve over time.

If you're just looking to migrate an account to a new PDS using existing tools, we recommend:
- [Migrating PDS Account with `goat`](https://whtwnd.com/bnewbold.net/3l5ii332pf32u)
- [PDS MOOver](https://pdsmoover.com/) ‚Äî a community-developed tool

For an in-depth explanation of the steps involved in a migration, read on.

## Creating a New Account

To create a PDS account with an existing identity, it is necessary to prove control the identity.

For an active account on another PDS, this is done by generating a service auth token (JWT) signed with the current atproto signing key indicated in the identity DID document. This can be requested from the previous PDS instance using the `com.atproto.server.getServiceAuth` endpoint (or an equivalent interface/API).

For an independently-controlled identity (eg, `did:web`, or a `did:plc` with old PDS offline or uncooperative), this may involve updating the identity to include a self-controlled atproto signing key, and generating the service auth token offline.

The service auth token is provided along with the existing DID when creating the account with `com.atproto.server.createAccount` (or an equivalent interface/API) on the new PDS.

The new account will be in a `deactivated` state. It should be possible to directly login and authenticate, but not to participate in the network. From the perspective of other services in the network, the old PDS account is still current, and the new PDS account is not yet active or valid. Functionality like OAuth or proxied requests using service auth will not yet work with the new PDS.

## Migrating Data

Some categories of data that are typically migrated are:

- public repository
- public blobs (media files)
- private preferences

At any stage of migration, the authenticated `com.atproto.server.checkAccountStatus` endpoint can be called on either the old or new PDS instance to check statistics about currently indexed data.

A copy of the repository can be fetched as a CAR file from the old PDS using the public `com.atproto.sync.getRepo` endpoint. If the old PDS is inaccessible, a mirror might be available from a public relay, or a local backup might be available. If not, the new account will still function with the same identity, but old content would be missing.

A CAR file can be imported to the new PDS using the authenticated `com.atproto.repo.importRepo` endpoint.

Blobs (media files) are download and re-uploaded one by one. They should not be uploaded to the new PDS until the repository has been imported and fully indexed, so that blobs can be linked to the records that refer to them, and won‚Äôt be garbage collected. A full list of relevant blobs (by CID) can be fetched either from the old PDS (`com.atproto.sync.listBlobs`), or the current list of "missing" blobs can be checked on the new PDS (`com.atproto.repo.listMissingBlobs`). If some blobs can not be found, the migration process can continue, and any recovered blobs can be uploaded later (if the blob CIDs match exactly).

Private account preferences can be exported from the old PDS using the authenticated like `app.bsky.actor.getPreferences` endpoint, then imported using `app.bsky.actor.putPreferences`. These are Bluesky app-specific endpoints, and other apps (Lexicons) may define their own preference APIs. Note that this will only include private state stored in the PDS; some preferences and state may exist in external services (eg, centralized chat/DM implementations).

## Updating Identity

Once content has been migrated, the identity (DID and handle) can be updated to indicate that the new PDS is the current host for the account.

"Recommended" DID document parameters can be fetched from the new PDS using the `com.atproto.identity.getRecommendedDidCredentials` endpoint. This will include the DID service hostname, local handle (as requested during account creation), a PDS-managed atproto signing key (public key), and (if relevant) PLC rotation keys (public keys).

For users who are able to securely manage a private cryptographic keypair (eg, store in a password manager or digital wallet), it is recommended to include a self-controlled PLC rotation key (public key) in the PLC operation.

For a self-controlled identity (eg, `did:web`, or `did:plc` with local rotation key), the identity update can be done directly by the user.

For an account with a `did:plc` managed by the old PDS, a PLC "operation" is signed by the old PDS, then submitted via the new PDS. The motivation for having the new PDS submit the PLC operation instead of having the user do so directly is to give the new PDS a chance to validate the operation and do safety check to prevent the account from getting in a broken state.

Because identity operations are sensitive, they require an additional security token as an additional "factor". The token can be requested via `com.atproto.identity.requestPlcOperationSignature` on the old PDS, and will be delivered by email to the verified account email by default.

This token is included as part of a call to `com.atproto.identity.signPlcOperation` on the old PDS, along with the requested DID fields (new signing key, rotation keys, PDS location, etc). The old PDS will validate the request, sign using the PDS-managed PLC rotation key, and return the signed PLC operation. The operation will not have been submitted to any PLC directory at this point in time.

The user is then recommended to submit the operation to the new PDS (using the `com.atproto.identity.submitPlcOperation` endpoint), which will validate that the changes are "safe" (aka, that they enable the PDS to help manage the identity and atproto account), and then submit it to the PLC directory.

With the identity successfully updated, the new PDS is now the "current" host for the account from the perspective of the entire network. This will be immediately apparent to new services which resolve the identity. Existing services that consume from the firehose will be alerted by the `#identity` event. Other services, which may have now-stale cached identity metadata for the account, will either refresh when the cache expires, or should refresh their cache when they encounter errors (such as invalid service auth signatures).

However, the new account is not yet "active".

## Finalizing Account Status

At this point, the user is still able to authenticate to both PDS instances. The new PDS knows that it is current for the account, but still has the account marked as "deactivated". The old PDS may not realize that it is no longer current.

It may be worth double-checking with `com.atproto.server.checkAccountStatus` on both PDS instances to confirm that all the expected content has been migrated.

The user can activate their account on the new PDS with a call to `com.atproto.server.activateAccount`, and deactivate their account on the old PDS with `com.atproto.server.deactivateAccount`.

At this point the migration is complete. New content can be published by writing to the repo, preferences can be updated, and inter-service auth and proxying should work as expected. It may be necessary to log out of any clients and log back in. In some cases, if services have aggressive identity caching and do not refresh on signature failure, service auth requests could fail for up to 24 hours.

It will still be possible to login and authenticate with the old PDS. The user may wish to fully terminated their old account eventually. This can be automated with the `deleteAfter` parameter to the `com.atproto.server.deactivateAccount` request. Note that the old PDS may be able to assist with PLC identity recovery during a fixed 72hr window, but only if the account was not fully deleted during that window.


---
atproto/src/app/[locale]/guides/applications/en.mdx
---
import Link from 'next/link'
import {FooterCTA} from "@/components/FooterCTA"
import AppBanner from './app-banner.png'
import AppLogin from './app-login.png'
import AppScreenshot from './app-screenshot.png'
import AppStatusHistory from './app-status-history.png'
import AppStatusOptions from './app-status-options.png'
import DiagramEventStream from './diagram-event-stream.png'
import DiagramInfoFlow from './diagram-info-flow.png'
import DiagramOauth from './diagram-oauth.png'
import DiagramOptimisticUpdate from './diagram-optimistic-update.png'
import DiagramRepo from './diagram-repo.png'

export const header = {
  title: 'Quick start guide to building applications on AT Protocol',
  description:
    'In this guide, we\'re going to build a simple multi-user app that publishes your current "status" as an emoji.',
}

# Quick start guide to building applications on AT Protocol

<Link href="https://github.com/bluesky-social/statusphere-example-app" className="not-prose flex items-center gap-2 bg-blue-100 dark:bg-blue-950 dark:text-white px-4 py-3 text-base rounded-lg hover:underline">
  <svg viewBox="0 0 20 20" aria-hidden="true" className="h-6 dark:text-white">
    <path
      fill="currentColor"
      fillRule="evenodd"
      clipRule="evenodd"
      d="M10 1.667c-4.605 0-8.334 3.823-8.334 8.544 0 3.78 2.385 6.974 5.698 8.106.417.075.573-.182.573-.406 0-.203-.011-.875-.011-1.592-2.093.397-2.635-.522-2.802-1.002-.094-.246-.5-1.005-.854-1.207-.291-.16-.708-.556-.01-.567.656-.01 1.124.62 1.281.876.75 1.292 1.948.93 2.427.705.073-.555.291-.93.531-1.143-1.854-.213-3.791-.95-3.791-4.218 0-.929.322-1.698.854-2.296-.083-.214-.375-1.09.083-2.265 0 0 .698-.224 2.292.876a7.576 7.576 0 0 1 2.083-.288c.709 0 1.417.096 2.084.288 1.593-1.11 2.291-.875 2.291-.875.459 1.174.167 2.05.084 2.263.53.599.854 1.357.854 2.297 0 3.278-1.948 4.005-3.802 4.219.302.266.563.78.563 1.58 0 1.143-.011 2.061-.011 2.35 0 .224.156.491.573.405a8.365 8.365 0 0 0 4.11-3.116 8.707 8.707 0 0 0 1.567-4.99c0-4.721-3.73-8.545-8.334-8.545Z"
    />
  </svg>
  <span>
    Find the source code for the example application on GitHub.
  </span>
</Link>

In this guide, we're going to build a simple multi-user app that publishes your current "status" as an emoji. Our application will look like this: {{className: 'lead'}}

<Image alt="A screenshot of our example application" src={AppScreenshot} />

We will cover how to: {{className: 'lead'}}

- Signin via OAuth {{className: 'lead'}}
- Fetch information about users (profiles) {{className: 'lead'}}
- Listen to the network firehose for new data {{className: 'lead'}}
- Publish data on the user's account using a custom schema {{className: 'lead'}}

We're going to keep this light so you can quickly wrap your head around ATProto. There will be links with more information about each step. {{className: 'lead'}}

## Introduction

Data in the Atmosphere is stored on users' personal repos. It's almost like each user has their own website. Our goal is to aggregate data from the users into our SQLite DB. 

Think of our app like a Google. If Google's job was to say which emoji each website had under `/status.json`, then it would show something like:

- `nytimes.com` is feeling üì∞ according to `https://nytimes.com/status.json`
- `bsky.app` is feeling ü¶ã according to `https://bsky.app/status.json`
- `reddit.com` is feeling ü§ì according to `https://reddit.com/status.json`

The Atmosphere works the same way, except we're going to check `at://` instead of `https://`. Each user has a data repo under an `at://` URL. We'll crawl all the user data repos in the Atmosphere for all the  "status.json" records and aggregate them into our SQLite database.

> `at://` is the URL scheme of the AT Protocol. Under the hood it uses common tech like HTTP and DNS, but it adds all of the features we'll be using in this tutorial.

## Step 1. Starting with our ExpressJS app

Start by cloning the repo and installing packages.

```bash
git clone https://github.com/bluesky-social/statusphere-example-app.git
cd statusphere-example-app
cp .env.template .env
npm install
npm run dev
# Navigate to http://localhost:8080
```

Our repo is a regular Web app. We're rendering our HTML server-side like it's 1999. We also have a SQLite database that we're managing with [Kysely](https://kysely.dev/).

Our starting stack:

- Typescript
- NodeJS web server ([express](https://expressjs.com/))
- SQLite database ([Kysely](https://kysely.dev/))
- Server-side rendering ([uhtml](https://www.npmjs.com/package/uhtml))

With each step we'll explain how our Web app taps into the Atmosphere. Refer to the codebase for more detailed code &mdash; again, this tutorial is going to keep it light and quick to digest.

## Step 2. Signing in with OAuth

When somebody logs into our app, they'll give us read & write access to their personal `at://` repo. We'll use that to write the status json record.

We're going to accomplish this using OAuth ([spec](https://github.com/bluesky-social/proposals/tree/main/0004-oauth)). Most of the OAuth flows are going to be handled for us using the [@atproto/oauth-client-node](https://github.com/bluesky-social/atproto/tree/main/packages/oauth/oauth-client-node) library. This is the arrangement we're aiming toward:

<Image alt="A diagram of the OAuth elements" src={DiagramOauth} />

When the user logs in, the OAuth client will create a new session with their repo server and give us read/write access along with basic user info.

<Image alt="A screenshot of the login UI" src={AppLogin} />

Our login page just asks the user for their "handle," which is the domain name associated with their account. For [Bluesky](https://bsky.app) users, these tend to look like `alice.bsky.social`, but they can be any kind of domain (eg `alice.com`).

```html
<!-- src/pages/login.ts -->
<form action="/login" method="post" class="login-form">
  <input
    type="text"
    name="handle"
    placeholder="Enter your handle (eg alice.bsky.social)"
    required
  />
  <button type="submit">Log in</button>
</form>
```

When they submit the form, we tell our OAuth client to initiate the authorization flow and then redirect the user to their server to complete the process.

```typescript
/** src/routes.ts **/
// Login handler
router.post(
  '/login',
  handler(async (req, res) => {
    // Initiate the OAuth flow
    const handle = req.body?.handle
    const url = await oauthClient.authorize(handle, {
      scope: 'atproto transition:generic',
    })
    return res.redirect(url.toString())
  })
)
```

This is the same kind of SSO flow that Google or GitHub uses. The user will be asked for their password, then asked to confirm the session with your application.

When that finishes, the user will be sent back to `/oauth/callback` on our Web app. The OAuth client will store the access tokens for the user's server, and then we attach their account's [DID](https://atproto.com/specs/did) to the cookie-session.

```typescript
/** src/routes.ts **/
// OAuth callback to complete session creation
router.get(
  '/oauth/callback',
  handler(async (req, res) => {
    // Store the credentials
    const { session } = await oauthClient.callback(params)

    // Attach the account DID to our user via a cookie
    const cookieSession = await getIronSession(req, res)
    cookieSession.did = session.did
    await cookieSession.save()

    // Send them back to the app
    return res.redirect('/')
  })
)
```

With that, we're in business! We now have a session with the user's repo server and can use that to access their data.

## Step 3. Fetching the user's profile

Why don't we learn something about our user? In [Bluesky](https://bsky.app), users publish a "profile" record which looks like this:

```typescript
interface ProfileRecord {
  displayName?: string // a human friendly name
  description?: string // a short bio
  avatar?: BlobRef     // small profile picture
  banner?: BlobRef     // banner image to put on profiles
  createdAt?: string   // declared time this profile data was added
  // ...
}
```

You can examine this record directly using [atproto-browser.vercel.app](https://atproto-browser.vercel.app). For instance, [this is the profile record for @bsky.app](https://atproto-browser.vercel.app/at?u=at://did:plc:z72i7hdynmk6r22z27h6tvur/app.bsky.actor.profile/self).

We're going to use the [Agent](https://github.com/bluesky-social/atproto/tree/main/packages/api) associated with the user's OAuth session to fetch this record.

```typescript
await agent.com.atproto.repo.getRecord({
  repo: agent.assertDid,                // The user
  collection: 'app.bsky.actor.profile', // The collection
  rkey: 'self',                         // The record key
})
```

When asking for a record, we provide three pieces of information.

- **repo** The [DID](https://atproto.com/specs/did) which identifies the user,
- **collection** The collection name, and
- **rkey** The record key

We'll explain the collection name shortly. Record keys are strings with [some restrictions](https://atproto.com/specs/record-key#record-key-syntax) and a couple of common patterns. The `"self"` pattern is used when a collection is expected to only contain one record which describes the user.

Let's update our homepage to fetch this profile record:

```typescript
/** src/routes.ts **/
// Homepage
router.get(
  '/',
  handler(async (req, res) => {
    // If the user is signed in, get an agent which communicates with their server
    const agent = await getSessionAgent(req, res, ctx)

    if (!agent) {
      // Serve the logged-out view
      return res.type('html').send(page(home()))
    }

    // Fetch additional information about the logged-in user
    const { data: profileRecord } = await agent.com.atproto.repo.getRecord({
      repo: agent.assertDid,                // our user's repo
      collection: 'app.bsky.actor.profile', // the bluesky profile record type
      rkey: 'self',                         // the record's key
    })

    // Serve the logged-in view
    return res
      .type('html')
      .send(page(home({ profile: profileRecord.value || {} })))
  })
)
```

With that data, we can give a nice personalized welcome banner for our user:

<Image alt="A screenshot of the banner image" src={AppBanner} />

```html
<!-- pages/home.ts -->
<div class="card">
  ${profile
    ? html`<form action="/logout" method="post" class="session-form">
        <div>
          Hi, <strong>${profile.displayName || 'friend'}</strong>.
          What's your status today?
        </div>
        <div>
          <button type="submit">Log out</button>
        </div>
      </form>`
    : html`<div class="session-form">
        <div><a href="/login">Log in</a> to set your status!</div>
        <div>
          <a href="/login" class="button">Log in</a>
        </div>
      </div>`}
</div>
```

## Step 4. Reading & writing records

You can think of the user repositories as collections of JSON records:

<Image alt="A diagram of a repository" src={DiagramRepo} />

Let's look again at how we read the "profile" record:

```typescript
await agent.com.atproto.repo.getRecord({
  repo: agent.assertDid,                // The user
  collection: 'app.bsky.actor.profile', // The collection
  rkey: 'self',                         // The record key
})
```

We write records using a similar API. Since our goal is to write "status" records, let's look at how that will happen:

```typescript
// Generate a time-based key for our record
const rkey = TID.nextStr()

// Write the 
await agent.com.atproto.repo.putRecord({
  repo: agent.assertDid,                 // The user
  collection: 'xyz.statusphere.status',  // The collection
  rkey,                                  // The record key
  record: {                              // The record value
    status: "üëç",
    createdAt: new Date().toISOString()
  }
})
```

Our `POST /status` route is going to use this API to publish the user's status to their repo.

```typescript
/** src/routes.ts **/
// "Set status" handler
router.post(
  '/status',
  handler(async (req, res) => {
    // If the user is signed in, get an agent which communicates with their server
    const agent = await getSessionAgent(req, res, ctx)
    if (!agent) {
      return res.status(401).type('html').send('<h1>Error: Session required</h1>')
    }

    // Construct their status record
    const record = {
      $type: 'xyz.statusphere.status',
      status: req.body?.status,
      createdAt: new Date().toISOString(),
    }

    try {
      // Write the status record to the user's repository
      await agent.com.atproto.repo.putRecord({
        repo: agent.assertDid, 
        collection: 'xyz.statusphere.status',
        rkey: TID.nextStr(),
        record,
      })
    } catch (err) {
      logger.warn({ err }, 'failed to write record')
      return res.status(500).type('html').send('<h1>Error: Failed to write record</h1>')
    }

    res.status(200).json({})
  })
)
```

Now in our homepage we can list out the status buttons:

```html
<!-- src/pages/home.ts -->
<form action="/status" method="post" class="status-options">
  ${STATUS_OPTIONS.map(status => html`
    <button class="status-option" name="status" value="${status}">
      ${status}
    </button>
  `)}
</form>
```

And here we are!

<Image alt="A screenshot of the app's status options" src={AppStatusOptions} />

## Step 5. Creating a custom "status" schema

Repo collections are typed, meaning that they have a defined schema. The `app.bsky.actor.profile` type definition [can be found here](https://github.com/bluesky-social/atproto/blob/main/lexicons/app/bsky/actor/profile.json).

Anybody can create a new schema using the [Lexicon](https://atproto.com/specs/lexicon) language, which is very similar to [JSON-Schema](http://json-schema.org/). The schemas use [reverse-DNS IDs](https://atproto.com/specs/nsid) which indicate ownership. In this demo app we're going to use `xyz.statusphere` which we registered specifically for this project (aka statusphere.xyz).

> ### Why create a schema?
>
> Schemas help other applications understand the data your app is creating. By publishing your schemas, you make it easier for other application authors to publish data in a format your app will recognize and handle.

Let's create our schema in the `/lexicons` folder of our codebase. You can [read more about how to define schemas here](https://atproto.com/guides/lexicon).

```json
/** lexicons/status.json **/
{
  "lexicon": 1,
  "id": "xyz.statusphere.status",
  "defs": {
    "main": {
      "type": "record",
      "key": "tid",
      "record": {
        "type": "object",
        "required": ["status", "createdAt"],
        "properties": {
          "status": {
            "type": "string",
            "minLength": 1,
            "maxGraphemes": 1,
            "maxLength": 32
          },
          "createdAt": {
            "type": "string",
            "format": "datetime"
          }
        }
      }
    }
  }
}
```

Now let's run some code-generation using our schema:

```bash
./node_modules/.bin/lex gen-server ./src/lexicon ./lexicons/*
```

This will produce Typescript interfaces as well as runtime validation functions that we can use in our app. Here's what that generated code looks like:

```typescript
/** src/lexicon/types/xyz/statusphere/status.ts **/
export interface Record {
  status: string
  createdAt: string
  [k: string]: unknown
}

export function isRecord(v: unknown): v is Record {
  return (
    isObj(v) &&
    hasProp(v, '$type') &&
    (v.$type === 'xyz.statusphere.status#main' || v.$type === 'xyz.statusphere.status')
  )
}

export function validateRecord(v: unknown): ValidationResult {
  return lexicons.validate('xyz.statusphere.status#main', v)
}
```

Let's use that code to improve the `POST /status` route:

```typescript
/** src/routes.ts **/
import * as Status from '#/lexicon/types/xyz/statusphere/status'
// ...
// "Set status" handler
router.post(
  '/status',
  handler(async (req, res) => {
    // ...

    // Construct & validate their status record
    const record = {
      $type: 'xyz.statusphere.status',
      status: req.body?.status,
      createdAt: new Date().toISOString(),
    }
    if (!Status.validateRecord(record).success) {
      return res.status(400).json({ error: 'Invalid status' })
    }

    // ...
  })
)
```

## Step 6. Listening to the firehose

So far, we have:

- Logged in via OAuth
- Created a custom schema
- Read & written records for the logged in user

Now we want to fetch the status records from other users.

Remember how we referred to our app as being like Google, crawling around the repos to get their records? One advantage we have in the AT Protocol is that each repo publishes an event log of their updates.

<Image alt="A diagram of the event stream" src={DiagramEventStream} />

Using a [Relay service](https://docs.bsky.app/docs/advanced-guides/federation-architecture#relay) we can listen to an aggregated firehose of these events across all users in the network. In our case what we're looking for are valid `xyz.statusphere.status` records.

```typescript
/** src/ingester.ts **/
import { Firehose } from '@atproto/sync'
import * as Status from '#/lexicon/types/xyz/statusphere/status'
// ...

new Firehose({
  filterCollections: ['xyz.statusphere.status'],
  handleEvent: async (evt) => {
    // Watch for write events
    if (evt.event === 'create' || evt.event === 'update') {
      const record = evt.record

      // If the write is a valid status update
      if (
        evt.collection === 'xyz.statusphere.status' &&
        Status.isRecord(record) &&
        Status.validateRecord(record).success
      ) {
        // Store the status
        // TODO
      }
    }
  },
})
```

Let's create a SQLite table to store these statuses:

```typescript
/** src/db.ts **/
// Create our statuses table
await db.schema
  .createTable('status')
  .addColumn('uri', 'varchar', (col) => col.primaryKey())
  .addColumn('authorDid', 'varchar', (col) => col.notNull())
  .addColumn('status', 'varchar', (col) => col.notNull())
  .addColumn('createdAt', 'varchar', (col) => col.notNull())
  .addColumn('indexedAt', 'varchar', (col) => col.notNull())
  .execute()
```

Now we can write these statuses into our database as they arrive from the firehose:

```typescript
/** src/ingester.ts **/
// If the write is a valid status update
if (
  evt.collection === 'xyz.statusphere.status' &&
  Status.isRecord(record) &&
  Status.validateRecord(record).success
) {
  // Store the status in our SQLite
  await db
    .insertInto('status')
    .values({
      uri: evt.uri.toString(),
      authorDid: evt.author,
      status: record.status,
      createdAt: record.createdAt,
      indexedAt: new Date().toISOString(),
    })
    .onConflict((oc) =>
      oc.column('uri').doUpdateSet({
        status: record.status,
        indexedAt: new Date().toISOString(),
      })
    )
    .execute()
}
```

You can almost think of information flowing in a loop:

<Image alt="A diagram of the flow of information" src={DiagramInfoFlow} />

Applications write to the repo. The write events are then emitted on the firehose where they're caught by the apps and ingested into their databases.

Why sync from the event log like this? Because there are other apps in the network that will write the records we're interested in. By subscribing to the event log, we ensure that we catch all the data we're interested in &mdash; including data published by other apps!

## Step 7. Listing the latest statuses

Now that we have statuses populating our SQLite, we can produce a timeline of status updates by users. We also use a [DID](https://atproto.com/specs/did)-to-handle resolver so we can show a nice username with the statuses:

```typescript
/** src/routes.ts **/
// Homepage
router.get(
  '/',
  handler(async (req, res) => {
    // ...

    // Fetch data stored in our SQLite
    const statuses = await db
      .selectFrom('status')
      .selectAll()
      .orderBy('indexedAt', 'desc')
      .limit(10)
      .execute()

    // Map user DIDs to their domain-name handles
    const didHandleMap = await resolver.resolveDidsToHandles(
      statuses.map((s) => s.authorDid)
    )

    // ...
  })
)
```

Our HTML can now list these status records:

```html
<!-- src/pages/home.ts -->
${statuses.map((status, i) => {
  const handle = didHandleMap[status.authorDid] || status.authorDid
  return html`
    <div class="status-line">
      <div>
        <div class="status">${status.status}</div>
      </div>
      <div class="desc">
        <a class="author" href="https://bsky.app/profile/${handle}">@${handle}</a>
        was feeling ${status.status} on ${status.indexedAt}.
      </div>
    </div>
  `
})}
```

<Image alt="A screenshot of the app status timeline" src={AppStatusHistory} />

## Step 8. Optimistic updates

As a final optimization, let's introduce "optimistic updates."

Remember the information flow loop with the repo write and the event log?

<Image alt="A diagram of the flow of information" src={DiagramInfoFlow} />

Since we're updating our users' repos locally, we can short-circuit that flow to our own database:

<Image alt="A diagram illustrating optimistic updates" src={DiagramOptimisticUpdate} />

This is an important optimization to make, because it ensures that the user sees their own changes while using your app. When the event eventually arrives from the firehose, we just discard it since we already have it saved locally.

To do this, we just update `POST /status` to include an additional write to our SQLite DB:

```typescript
/** src/routes.ts **/
// "Set status" handler
router.post(
  '/status',
  handler(async (req, res) => {
    // ...

    let uri
    try {
      // Write the status record to the user's repository
      const res = await agent.com.atproto.repo.putRecord({
        repo: agent.assertDid, 
        collection: 'xyz.statusphere.status',
        rkey: TID.nextStr(),
        record,
      })
      uri = res.uri
    } catch (err) {
      logger.warn({ err }, 'failed to write record')
      return res.status(500).json({ error: 'Failed to write record' })
    }

    try {
      // Optimistically update our SQLite <-- HERE!
      await db
        .insertInto('status')
        .values({
          uri,
          authorDid: agent.assertDid, 
          status: record.status,
          createdAt: record.createdAt,
          indexedAt: new Date().toISOString(),
        })
        .execute()
    } catch (err) {
      logger.warn(
        { err },
        'failed to update computed view; ignoring as it should be caught by the firehose'
      )
    }

    res.status(200).json({})
  })
)
```

You'll notice this code looks almost exactly like what we're doing in `ingester.ts`.

## Thinking in AT Proto

In this tutorial we've covered the key steps to building an atproto app. Data is published in its canonical form on users' `at://` repos and then aggregated into apps' databases to produce views of the network.

When building your app, think in these four key steps:

- Design the [Lexicon](https://atproto.com/guides/lexicon) schemas for the records you'll publish into the Atmosphere.
- Create a database for aggregating the records into useful views.
- Build your application to write the records on your users' repos.
- Listen to the firehose to aggregate data across the network.

Remember this flow of information throughout:

<Image alt="A diagram of the flow of information" src={DiagramInfoFlow} />

This is how every app in the Atmosphere works, including the [Bluesky social app](https://bsky.app).

## Next steps

If you want to practice what you've learned, here are some additional challenges you could try:

- Sync the profile records of all users so that you can show their display names instead of their handles.
- Count the number of each status used and display the total counts.
- Fetch the authed user's `app.bsky.graph.follow` follows and show statuses from them.
- Create a different kind of schema, like a way to post links to websites and rate them 1 through 4 stars.

<FooterCTA href="/" title="Ready to learn more?" description="Specs, guides, and SDKs can be found here." />

---
atproto/src/app/[locale]/guides/auth/en.mdx
---
import {Container} from "@/components/Container"

export const header = {
  title: 'Auth',
  description: 'Auth for Atproto application developers',
}

## Getting Started

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
    You can log in using any [OAuth](/guides/auth) provider, or with [PasswordSession](https://github.com/bluesky-social/atproto/tree/main/packages/lex/lex-password-session) from our SDK if you are using password auth.

    Begin by installing the Atproto packages:

    ```bash
    npm install @atproto/lex @atproto/lex-password-session
    ```
    </TabPanel>
    <TabPanel value="go">
    You can log in using any [OAuth](/guides/auth) provider, or with password authentication from our SDK.

    Begin by installing the Atproto packages:

    ```bash
    go get github.com/bluesky-social/indigo
    ```
    </TabPanel>
  </TabPanels>
</TabGroup>

Next, authenticate with an [SDK](/guides/sdk-auth).

## About authorization

**Auth is the first part of our docs for a reason!** You don't need to be authed to [read records](/reading-data#unauthenticated-reads) or even to [stream data from the firehose](/guides/streaming-data). But if you're building an app that integrates with the Atmosphere, you'll begin with auth.

You might start out by just using Atproto identities as an OAuth source ‚Äî though you'll find that there's lots more you can do by building on top of the Atproto [social graph](/guides/social-graph).

This section is aimed at developers building applications on Atproto. Applications with their own end-user login flow should implement [OAuth](#what-is-oauth) for authentication. Single-purpose applications such as bots or command line tools may use [password authentication](/guides/sdk-auth) instead.

<Note>
We also support the use of [App Passwords](/specs/xrpc#app-passwords). Accounts can create and revoke app passwords separate from their primary password. This way, you do not need to supply your primary password to applications that use password auth.
</Note>

To implement OAuth in your application, refer to [OAuth patterns](/guides/oauth-patterns) for common use cases, and the [Scopes](/guides/scopes) guide for understanding permission models. If you're building your own protocol implementation or OAuth Client SDK, refer to the [OAuth spec](/specs/oauth).

## What *is* OAuth?

OAuth is an authorization framework that lets developers request access to an account without requiring users to hand over their password. Without OAuth, if someone wanted to authorize a 3rd party app to access their account, their only option would be to type their username and password into that app. This is bad for all sorts of reasons, the first being that a 3rd party app gets to see, and likely save, passwords. And once an app has a username and password, the app would have _full_ access to their account.

OAuth aims to solve these problems with a family of open specifications for managing secure authorization without requiring a person's username and password. If you've ever used a "Sign in with..." button or link on the web, you've used OAuth.

As a developer, you can request limited access to a person's account, meaning if you only need to post a message to someone's public timeline, you don't also need to request access to read their private messages. This is great for your app because you don't have to worry about managing data you don't need, and it's great for the person using your app because they don't need to worry about their DMs being compromised.

## How is OAuth Different in AT Protocol?

Atproto [specifies](/specs/oauth) a particular "profile" of the OAuth standards, using [OAuth 2.1](https://www.ietf.org/archive/id/draft-ietf-oauth-v2-1-13.html) as the foundation.

There are a few details that might catch you off guard if you're used to using other OAuth systems.

- **Atproto is distributed**: Usually, when an app has a "sign in with..." button, it provides a choice of which authorities it allows to authenticate users (usually one of a few big corporations). With ATProto OAuth, the app has no prior relationship with the authentication provider: a user's [PDS](/guides/the-at-stack#pds). This is also the reason why Atproto OAuth is not compatible with OIDC, which requires a pre-established relationship.

- **Migration**: Atproto users can migrate their accounts between servers ([PDSes](/guides/glossary#pds-personal-data-server)) over time. To facilitate this Atproto has a flexible [Identity](/guides/identity) layer, which allows usernames ([handles](/specs/handle)) to be resolved to a static user ID ([DID](/specs/did)), which in turn can be resolved to locate the user's PDS. When a user logs in to an app, the OAuth client dynamically resolves these relationships.

- **Client IDs**: In other OAuth ecosystems, it is often necessary for client apps to pre-register themselves with the resource server. This is not viable in a decentralized system (with many clients and many resource servers), so Atproto addresses this using [Client ID Metadata Documents](/specs/oauth#client-id-metadata-document).

## Further Reading and Resources

Begin by reading [SDK authentication](/guides/sdk-auth). To implement OAuth in your application, refer to [OAuth patterns](/guides/oauth-patterns) for common use cases, and the [Scopes](/guides/scopes) guide for understanding permission models.

- [SDK authentication](/guides/sdk-auth)
- [OAuth patterns](/guides/oauth-patterns)
- [Scopes](/guides/scopes)
- [Permission requests](/guides/permission-sets)
- [OAuth spec](/specs/oauth)
- [Permissions spec](/specs/permission)

---
atproto/src/app/[locale]/guides/backfilling/en.mdx
---
export const header = {
  title: 'Backfilling',
  description: 'Replicating the network',
}

## About backfilling

Backfilling is the process of syncing all the data in the network from scratch. You may want to do this if you're running a service that requires a complete copy of the data in the network. This is not generally necessary for running feed generators, labelers, or bots, as most of the time they are fine handling live data off of the firehose. However, backfilling may be of interest if you want to perform large-scale data analysis.

For the entire network to be backfillable by third parties at all is a novel concept for AT. Other, monolithic social networks generally only offer an large-scale event stream (like our firehose) from the current date and time, making it difficult to perform longitudinal data analysis without additional data vendors. With the AT Protocol and adequate resources, you can *always* backfill the entire network on your own. This, in turn, benefits researchers and other forms of data analysis ‚Äî if you can provision enough storage, you can have your own local copy of the entire Atmosphere.

When backfilling, you generally need to maintain 'up to date' replica of the data, which requires a cutover to streaming firehose data once the backfill is complete. We created `tap` to streamline this process.

## Using tap

`tap` simplifies AT sync by handling the firehose connection, verification, backfill, and filtering. Your application connects to a Tap and receives simple JSON events for only the repos and collections you care about, no need to worry about binary formats for validating cryptographic signatures.

Tap can be run from the command line:

```shell
# Run tap
go run ./cmd/tap run --disable-acks=true
# By default, the service uses SQLite at `./tap.db` and binds to port `:2480`.

# In a separate terminal, connect to receive events:
websocat ws://localhost:2480/channel

#  Add a repo to track
curl -X POST http://localhost:2480/repos/add \
  -H "Content-Type: application/json" \
  -d '{"dids": ["did:plc:ewvi7nxzyoun6zhxrhs64oiz"]}' # @atproto.com repo
```

When a repo is added, tap provides:

1. Historical backfill: Tap fetches the full repo from the account's PDS using com.atproto.sync.getRepo
2. Live event buffering: Any firehose events for this repo during backfill are held in memory
3. Ordering guarantee: Historical events (marked live: false) are delivered first
4. Cutover: After historical events complete, buffered live events are drained
5. Live streaming: New firehose events are delivered immediately (marked live: true)

We also provide a [TypeScript library](https://www.npmjs.com/package/@atproto/tap) for working with Tap. For more information, refer to [The AT Stack](/guides/the-at-stack#tap) and the [tap repository](https://github.com/bluesky-social/indigo/tree/main/cmd/tap).

## How backfilling works

If you are implementing backfilling on your own, the general process is:

- Given a DID, check your current 'revision' for that DID (Each change to a repo is tagged with a 'revision' or 'rev' string that is a lexicographically sortable timestamp).
- If you do not have a rev for that repo, download and process the users repo checkpoint from the `com.atproto.sync.getRepo` endpoint.
- While you are doing that, buffer any events for the repo to go through after the checkpoint has been processed.
- The checkpoint will contain a rev value that you can use to skip any buffered events that have already been included in said checkpoint.
- For each buffered event, if the rev is less than the current rev you have, you can safely skip it.

Do the above process for each repo and you will end up with a complete replica of the network. To get a list of all the repos, you can use the `com.atproto.sync.listRepos` endpoint on the relay, or on each PDS.

- This is a fairly large amount of data (hundreds of GBs at the time of writing), and will be somewhat demanding in terms of resources.
- Be careful not to get rate limited. You will be making one call to `getRepo` per user. It is recommended to implement client side rate limiting to prevent your requests from getting blocked by firewalls on the PDS or relay you are requesting data from.

## Further Reading and Resources

- [Sync](/guides/sync)
- [Streaming data](/guides/streaming-data)
- [Feeds](/guides/feeds)
- [Repository spec](/specs/repository)
- [Event Stream spec](/specs/event-stream)
- [Sync spec](/specs/sync)
- The [Microcosm](https://www.microcosm.blue/) community project maintains tools for working with AT records at scale without local mirroring.

---
atproto/src/app/[locale]/guides/blob-lifecycle/en.mdx
---
export const header = {
  title: 'Blob Lifecycle',
  description: 'How blobs are uploaded, referenced, and deleted',
}

## Uploading blobs

Blobs must be uploaded to the PDS before a record can be created referencing that blob. Note that the server does not know the intended Lexicon when receiving an upload, so can only apply generic blob limits and restrictions at initial upload time, and then enforce Lexicon-defined limits later when the record is created.

Clients use the **`com.atproto.repo.uploadBlob`** endpoint on their PDS, which will return verified metadata in the form of a Lexicon blob object. Clients should set the HTTP **`Content-Type`** header and should set the **`Content-Length`** headers on the upload request. SDKs can handle this automatically:

```typescript
const image = 'data:image/png;base64,...'
const { data } = await agent.uploadBlob(convertDataURIToUint8Array(image), {
  encoding,
})
```

This `data` object could then be referenced in another record, e.g. as an `embed`.

Chunked transfer encoding may also be permitted for uploads. Servers *may* sniff the blob mimetype to validate against the declared **`Content-Type`** header, and either return a modified mimetype in the response, or reject the upload. See ["Security Considerations"](/specs/blob#security-considerations). If the actual blob upload size differs from the **`Content-Length`** header, the server should reject the upload.

## Garbage collecting

After a successful upload, blobs are placed in temporary storage. They are not accessible for download or distribution while in this state. Servers should "garbage collect" (delete) un-referenced temporary blobs after an appropriate time span (see [implementation guidelines](#web-accessibility)). Blobs which are in temporary storage should not be included in the **`listBlobs`** output.

## Referencing blobs

The upload blob can now be referenced from records by including the returned blob metadata in a record. When processing record creation, the server extracts the set of all referenced blobs, and checks that they are either already referenced, or are in temporary storage. Once the record creation succeeds, the server makes the blob publicly accessible.

The same blob can be referenced by multiple records in the same repository. Re-uploading a blob which has already been stored and referenced results in no change to the existing blobs or records.

Creation of new individual records which reference a blob which does not exist should be rejected at the time of creation (or update). However, it is possible for servers to host repository records which reference blobs which are not available locally. For example, during a bulk repository import or account migration; data loss; or content deletion/removal for policy reasons.

## Deleting blobs

When a record referencing blobs is deleted, the server checks if any other current records from the same repository reference the blob. If not, the blob is deleted along with the record.

When an account is deleted, all the hosted blobs are deleted, within some reasonable time frame. When an account is deactivated, takendown, or suspended, blobs should not be publicly accessible.

Servers may decide to make individual blobs inaccessible, separately from any account takedown or other account lifecycle events.

## Web accessibility

Original blobs can be fetched from the PDS using the **`com.atproto.sync.getBlob`** endpoint. The server should return appropriate **`Content-Type`** and **`Content-Length`** HTTP headers. It is not a recommended or required pattern to serve media directly from the PDS to end-user browsers, and servers do not need to support or facilitate this use case. See "Security Considerations" for more.

Servers may have their own generic limits and policies for blobs, separate from any Lexicon-defined constraints. They might implement account-wide quotas on data storage; maximum blob sizes; content policies; etc. Any of these restrictions might be enforced at the initial upload. Server operators should be aware that limits and other restrictions may impact functionality with existing and future applications. To maximize interoperability, operators are recommended to prefer limits on overall account resource consumption (e.g., "total blob size" quota, not "per blob" size limits).

Some applications may have a long delay between blob upload and reference from a record. To maximize interoperability, server implementations and operators are recommended to allow several hours of grace time before "garbage collecting", with at least one hour a firm lower bound.

## Related resources

- [Images and Video](/guides/images-and-video)
- [Blob security](/guides/blob-security)
- [Video handling](/guides/video-handling)
- [Blob Specs](/specs/blob)
- [Streamplace](https://stream.place/docs/) provides an implementation and related Lexicon for video streaming on Atproto.

---
atproto/src/app/[locale]/guides/blob-security/en.mdx
---
export const header = {
  title: 'Blob Security',
  description: 'Security considerations when serving blobs',
}

## HTTP requests

Serving arbitrary user-uploaded files from a web server raises many content security issues. For example, cross-site scripting (XSS) of scripts or SVG content form the same "origin" as other web pages. It is effectively mandatory to enable a [Content Security Policy](https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP) for the **`getBlob`** endpoint; it is effectively not supported to dynamically serve assets directly out of blob storage (the **`getBlob`** endpoint) directly to browsers and web applications. Applications must proxy blobs, files, and assets through an independent CDN, proxy, or other web service before serving to browsers and web agents, and such services are expected to implement security precautions.

An example set of content security headers for this endpoint is:

```jsx
Content-Security-Policy: default-src 'none'; sandbox
X-Content-Type-Options: nosniff
```

## Metadata

Some media types may contain sensitive metadata. For example, EXIF metadata in JPEG image files may contain GPS coordinates. Servers might take steps to prevent accidental leakage of such metadata, for example by stripping out this metadata before persisting a blob.

Parsing of media files is a notorious source of memory safety bugs and security vulnerabilities. Even content type detection (or "sniffing") can be a source of exploits. Servers are strongly recommended against parsing media files (image, video, audio, or any other non-trivial formats) directly, without the use of strong sandboxing mechanisms. In particular, PDS instances themselves should not directly implement media resizing or transcoding ‚Äî this should be handled in the application layer instead.

## Adversarial use

Richer media types raise the stakes for abusive and illegal content. Services should implement appropriate mechanisms to takedown such content when it is detected and reported.

Servers may need to take measures to prevent malicious resource consumption. For example, intentional exhaustion of disk space, network congestion, bandwidth utilization, etc. ‚Äî rate-limits, size limits, and quotas are recommended.

## Related resources

- [Images and Video](/guides/images-and-video)
- [Blob security](/guides/blob-security)
- [Video handling](/guides/video-handling)
- [Blob Specs](/specs/blob)
- [Streamplace](https://stream.place/docs/) provides an implementation and related Lexicon for video streaming on ATProto.

---
atproto/src/app/[locale]/guides/bot-tutorial/en.mdx
---
export const header = {
  title: 'Build an Agent',
  description: 'Create a bot that automatically posts on Bluesky',
}

Bots are accounts on the network that post automatically. Popular ones include bots that post the magnitude of recent earthquakes, photos from an archive on a regular schedule, etc.

In this tutorial, you'll create a bot that authenticates with an [app password](/guides/sdk-auth) to interact with the network.

## Prerequisites

This tutorial involves building a TypeScript application from scratch. You should have a working understanding of TypeScript and Node.js.

You should have installed:
- Node.js 18+

On platforms supported by [homebrew](https://brew.sh/), you can install it with:

```bash
brew install node
```

You should also have our `lex` CLI tool installed globally:

```bash
npm install -g @atproto/lex
```

Now, create a new Typescript project directory and initialize your project:

```bash
mkdir my-agent
cd my-agent
npm init -y
npm i -D typescript ts-node @types/node dotenv cron @atproto/lex @atproto/lex-password-session
npx tsc --init --verbatimModuleSyntax false
```

If you aren't using an existing account to post from, you should [create a new account](https://bsky.app/) for your bot. You should also generate an [app password](/guides/sdk-auth) for that account.

## Part 1: Lexicons 

[Lexicons](/guides/lexicon) define Atproto records. Your bot will create Bluesky posts, using the [app.bsky.feed.post](https://lexicon.garden/lexicon/did:plc:4v4y5r3lwsbtmsxhile2ljac/app.bsky.feed.post) Lexicon. You can use `lex` to download and build this Lexicon into your project:

```bash
lex install app.bsky.feed.post
```

If you want, you can view the downloaded Lexicon file at `lib/lexicons/app.bsky.post.json`. From here, you can run `lex build` to generate TypeScript types for all of your installed Lexicons:

```bash
lex build
```

The generated code will live in `src/lexicons`. Now you can create your bot script.

## Part 2: Create a Bot

Create a new file at `src/index.ts`. This file will contain the code for your bot.

First, add the imports that you'll need, and a call to `PasswordSession.create()` to authenticate your bot using an app password:

```ts
import { Client } from '@atproto/lex'
import { PasswordSession } from '@atproto/lex-password-session'
import * as dotenv from 'dotenv'
import { CronJob } from 'cron'
import * as process from 'process'
import * as app from './lexicons/app.js'

dotenv.config()

// Create a session
async function login() {
    const service = process.env.BOT_PDS!
    const identifier = process.env.BOT_HANDLE!
    const password = process.env.BOT_PASSWORD!
    const session = await PasswordSession.login({
        service, // eg 'https://bsky.social'
        identifier, // eg 'alice.bsky.social'
        password,
    })

    return session
}
```

Next, add a function that posts. This will instantiate a new `Client` using the authenticated session, and use it to create a post. The `app.bsky.feed.post` Lexicon requires two parameters: the text of a post ‚Äî you'll use "üôÇ" ‚Äî and a timestamp.

```ts
// Posting logic
async function makePost(session: PasswordSession) {
    const client = new Client(session)
    await client.create(app.bsky.feed.post, {
        text: 'üôÇ',
        createdAt: new Date().toISOString(),
    })
    console.log('Just posted!')
}

async function main() {
    const session = await login()
    await makePost(session)
}

main().catch(console.error)
```

Finally, add another code block inside of `main()` using the `cron` package to schedule your bot to post every three hours:

```ts
async function main() {
    const session = await login()
    await makePost(session)
    // Run this on a cron job
    const scheduleExpression = '0 */3 * * *'
    const job = new CronJob(scheduleExpression, () => makePost(session))
    job.start()
}
```

That's all the code you need for your bot! Next, you'll test it out.

## Part 3: Run the Bot

Save your bot's host (PDS), username, and password to an `.env` file.

```bash
BOT_PDS= # eg 'https://bsky.social' if you aren't self-hosting your PDS
BOT_HANDLE= # eg 'alice.bsky.social'
BOT_PASSWORD=
```

Now you can test the bot using `ts-node`.

```bash
ts-node src/index.ts
```

You can run this bot on your own machine, or deploy it on a platform like [Fly.io](http://fly.io/).

Keep in mind that bots should respect the network's rate limits. For instance, [Bluesky has its own set of rate limits](https://docs.bsky.app/docs/advanced-guides/rate-limits). 

## Part 4: From Bot to Agent

So far, you've created a bot that makes posts on a regular interval. Now, you can expand your bot into a more complex agent that interacts with other users!

In this section, you'll use the [Vercel AI SDK](https://www.npmjs.com/package/ai). This library provides a unified API to interact with model providers like Anthropic through the Vercel AI Gateway. You'll need to sign up for a free [Hobby project](https://vercel.com/) to get an API key.

Once you've got an API key, add the AI SDK to your project:

```bash
npm i ai
```

Then, add another environment variable to your `.env` file:

```bash
BOT_PDS=
BOT_HANDLE=
BOT_PASSWORD=
AI_GATEWAY_API_KEY=xxxxxxxxx
```

Finally, you'll need to install some additional Lexicons to get mentions. Run the following command to install and build the necessary Lexicons:

```bash
lex install app.bsky.notification.listNotifications app.bsky.feed.getPostThread
lex build --override
```

Almost there. Add the `ai` import to your `src/index.ts` file:

```ts
import { generateText } from 'ai'
```

Now you can replace everything after the `login` function in your script with new logic that uses the AI SDK to generate replies:

```ts
async function generateReply(mentionText: string): Promise<string> {
  const { text } = await generateText({
    model: 'anthropic/claude-sonnet-4-5',
    system: 'You are a mystical fortune teller bot on Bluesky named "The Oracle". Speak in a dramatic, mysterious tone with occasional emoji (üîÆ ‚ú® üåô ‚≠ê). If someone asks a yes/no question, give a cryptic but leaning answer. If they ask for general guidance, offer a brief horoscope-style prediction. Keep responses under 280 characters. Never break character.',
    prompt: mentionText,
  })
  return text
}

// Check for new mentions and reply
async function checkMentions(session: PasswordSession) {
  const client = new Client(session)
  const notifications = await client.call(app.bsky.notification.listNotifications, {
    limit: 20,
  })

  for (const notif of notifications.notifications) {
    // Only process unread mentions
    if (notif.reason !== 'mention' || notif.isRead) continue

    // Get the post that mentioned us
    const post = await client.call(app.bsky.feed.getPostThread, {
      uri: notif.uri,
    })

    const mentionText = post.thread.post.record.text
    const reply = await generateReply(mentionText)

    // Reply to the post
    await client.create(app.bsky.feed.post, {
      text: reply,
      createdAt: new Date().toISOString(),
      reply: {
        root: { uri: notif.uri, cid: post.thread.post.cid },
        parent: { uri: notif.uri, cid: post.thread.post.cid },
      },
    })

    console.log(`Replied to mention: ${mentionText}`)
  }
}

async function main() {
    const session = await login()
    const job = new CronJob('*/1 * * * *', () => checkMentions(session))
    job.start()

    console.log('Agent is running...')
}

main().catch(console.error)
```

There are a few new concepts here. You'll recognize the `app.bsky.feed.post` logic, and now you'll also implement `app.bsky.notification.listNotifications` and `app.bsky.feed.getPostThread`. The first Lexicon is used to get a list of notifications for, and the second fetches the full thread for a given post URI.

...and feel free to change our silly example prompt.

Redeploy your bot, and it will now reply to mentions!

<Note>
Automated bots that post to an account on an regular interval are welcome on the network. If your bot interacts with other users, please only interact (like, repost, reply, etc.) if the user has tagged the bot account. It should be an opt-in interaction, or your bot may be flagged for spam.
</Note>

## Conclusion

You've now built an automated Atproto app that creates records on the network!

You can find more guides and tutorials in our [Guides](/guides) section, and more example apps in the [Cookbook](https://github.com/bluesky-social/cookbook/) repository. Happy building!

---
atproto/src/app/[locale]/guides/creating-a-labeler/en.mdx
---
import {Container} from "@/components/Container"
import ozone from "./ozone.png"

export const header = {
  title: 'Creating a Labeler',
  description: 'Run your own modular moderation service',
}

## Declaring a labeler

Labelers publish an `/app.bsky.labeler.service/self` record to declare that they are a labeler and publish their policies. That record looks like this:

```json
{
  "$type": "app.bsky.labeler.service",
  "policies": {
    "labelValues": ["porn", "spider"],
    "labelValueDefinitions": [
      {
        "identifier": "spider",
        "severity": "alert",
        "blurs": "media",
        "defaultSetting": "warn",
        "locales": [
          {"lang": "en", "name": "Spider Warning", "description": "Spider!!!"}
        ]
      }
    ]
  },
  "subjectTypes": ["record"],
  "subjectCollections": ["app.bsky.feed.post", "app.bsky.actor.profile"],
  "reasonTypes": ["com.atproto.moderation.defs#reasonOther"],
  "createdAt": "2024-03-03T05:31:08.938Z"
}
```

The `labelValues` declares what to expect from the Labeler. It may include global and custom label values.

The `labelValueDefinitions` defines the custom labels. It includes the `locales` field for specifying human-readable copy in various languages. If the user's language is not found, it will use the first set of strings in the array.

`subjectTypes`, `subjectCollections`, and `reasonTypes` declare what type of moderation reports are reviewed by the Labeler. `subjectTypes` can include `record` for individual pieces of content, and `account` for overall accounts. `subjectCollections` is a list of NSIDs of record types; if not defined, any record type is allowed. `reasonTypes` is a list of report reason codes (Lexicon references).

## Ozone

[Ozone](https://github.com/bluesky-social/ozone) is our reference labeling service for Atmosphere apps, and includes a web interface for triaging and actioning moderation reports.

<Container>
  <Image src={ozone} alt="" className="w-full max-w-md mx-auto" />
</Container>

Self-hosting Ozone enables you to participate as a labeler in the AT stackable moderation architecture. The Ozone service consists of a web UI, a backend, and a Postgres database. 

Before setting up your Ozone service you should create a *new* account on the network, separate from your main account. This is the account that subscribers to your labeler will interact with: accounts for labelers will appear differently in app interfaces than normal accounts. Refer to the [Ozone User Guide](https://github.com/bluesky-social/ozone/blob/main/docs/userguide.md) for more.

## Further Reading and Resources

- [Moderation](/guides/moderation)
- [Labels](/guides/labels)
- [Subscriptions](/guides/subscriptions)
- [Using Ozone](/guides/using-ozone)
- [Label Specs](/specs/label)

---
atproto/src/app/[locale]/guides/custom-feed-tutorial/en.mdx
---
import {Container} from "@/components/Container"
import bsky from "./feeds-bsky.png"

export const header = {
  title: 'Write a Custom Feed',
  description: 'Host an algorithm that\'s accessible from apps like Bluesky.',
}

Custom feeds, or feed generators, are services that provide algorithms to users. This gives you lots of freedom: to choose your own timeline view, or to [create a embeddable feed for other users](https://graze.leaflet.pub/3m4yjmbnyec2c).

In this tutorial, you'll create a custom feed server by consuming posts from the [firehose](/guides/streaming-data), algorithmically sorting them, and [broadcasting them as an XRPC feed](/guides/feeds).

## Prerequisites

This tutorial involves building a TypeScript application from scratch. You should have a working understanding of TypeScript and Node.js.

You should have installed:
- Node.js 18+
- Go 1.25+

On platforms supported by [homebrew](https://brew.sh/), you can install them with:

```bash
brew install node go
```

You should also have our `lex` CLI tool installed globally:

```bash
npm install -g @atproto/lex
```

You'll also need to install [`tap`](/guides/the-at-stack#tap) to consume the firehose of posts from the network. 

```bash
go install github.com/bluesky-social/indigo/cmd/tap
```

Now, create a new Typescript project directory and initialize your project:

```bash
mkdir my-agent
cd my-agent
npm init -y
npm i -D typescript ts-node @types/node dotenv @atproto/lex @atproto/lex-server @atproto/tap
npx tsc --init --verbatimModuleSyntax false
```

You'll begin by adding Lexicons to your project.

## Part 1: Lexicons and Tap

[Lexicons](/guides/lexicon) define Atproto records. A baseline feed generator requires the `app.bsky.feed.describeFeedGenerator` and `app.bsky.feed.getFeedSkeleton` Lexicons. You can use `lex` to download and build these Lexicons into your project:

```bash
lex install app.bsky.feed.describeFeedGenerator app.bsky.feed.getFeedSkeleton
```

From here, you can run `lex build` to generate TypeScript types for all of your installed Lexicons:

```bash
lex build
```

The generated code will live in `src/lexicons`.

You'll also need to run `tap` in a separate terminal to index posts from the network. Start `tap` with:

```bash
tap run --disable-acks=true
```

Now you can start building your feed generator.

## Part 2: Feed Structure

The general flow of providing a custom algorithm to a user is as follows:

- A user requests a feed from the application backend (e.g. Bluesky)
- The application finds the DID doc of the requested feed to look up its hosting server
- The application sends a `getFeedSkeleton` request to the hosting server
- The feed's hosting server returns a list of post URLs
- The application hydrates the feed (user info, post contents, aggregates, etc.)
- The application returns the hydrated feed to the user

You'll start by capturing some configuration from environment variables. Much of this is configuration for the published feed (`FEED_PUBLISHER_DID`, `FEED_NAME`) but some details configure how the feed will behave (`FEED_MAX_POSTS`, `SEARCH_TERMS`).

For this example, you'll be creating our entire app in a single file at `src/index.ts`. Create that file now, and add these imports and config lines:

```ts
import { AtUriString, DidString, asDidString } from '@atproto/lex'
import { LexError, LexRouter, serviceAuth } from '@atproto/lex-server'
import { serve } from '@atproto/lex-server/nodejs'
import { Tap, SimpleIndexer } from '@atproto/tap'
import * as app from './lexicons/app.js'

// =============================================================================
// Configuration
// =============================================================================

interface FeedConfig {
    publisherDid: DidString
    feedName: string
    searchTerms: string[]
    maxPosts: number
    port: number
    tapUrl: string
    tapPassword: string
    initialRepos: string[]
}

const DEFAULT_REPO = 'did:plc:ragtjsm2j2vknwkz3zp4oxrd' // pfrazee.com

const config: FeedConfig = {
    publisherDid: asDidString(process.env.FEED_PUBLISHER_DID || 'did:example:alice'),
    feedName: process.env.FEED_NAME || 'whats-alf',
    searchTerms: (process.env.FEED_SEARCH_TERMS || 'alf').split(',').map(s => s.trim()),
    maxPosts: parseInt(process.env.FEED_MAX_POSTS || '1000', 10),
    port: parseInt(process.env.FEED_PORT || '3000', 10),
    tapUrl: process.env.TAP_URL || 'http://localhost:2480',
    tapPassword: process.env.TAP_PASSWORD || 'secret',
    initialRepos: (process.env.FEED_INITIAL_REPOS || DEFAULT_REPO).split(',').map(s => s.trim()),
}

const FEED_URI: AtUriString = `at://${config.publisherDid}/app.bsky.feed.generator/${config.feedName}` as AtUriString
```

There are a few things to note here. You've supplied a `DEFAULT_REPO` to fetch with `tap`. In this case, you're following a single user, [`pfrazee.com`](https://bsky.app/profile/pfrazee.com). This will keep the tutorial example manageable.

You're sorting this feed by searching for posts that contain certain terms. You can configure these terms with the `FEED_SEARCH_TERMS` environment variable. In this case, you're searching for posts that mention "alf".

Next, you'll set up the logic to consume posts from the firehose.

## Part 3: Consuming the Firehose

You'll store the posts you want to serve in an array.

As `tap` discovers posts from the network, this feed server will search their text for our configured search terms. If it finds a match, it'll add the post to your array. Add the following contents to `src/index.ts`:

```ts
// =============================================================================
// Post Index
// =============================================================================

interface IndexedPost {
    uri: AtUriString
    indexedAt: number
}

const postIndex: IndexedPost[] = []

// =============================================================================
// Tap Indexer
// =============================================================================

const tap = new Tap(config.tapUrl, { adminPassword: config.tapPassword })
const indexer = new SimpleIndexer()

const searchPattern = new RegExp(
    `\\b(${config.searchTerms.map(t => t.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')).join('|')})\\b`,
    'i'
)

indexer.record(async (evt) => {
    if (evt.collection !== 'app.bsky.feed.post') return

    const uri = `at://${evt.did}/${evt.collection}/${evt.rkey}` as AtUriString

    if (evt.action === 'delete') {
        const idx = postIndex.findIndex(p => p.uri === uri)
        if (idx !== -1) {
            postIndex.splice(idx, 1)
            console.log(`DELETE ${uri}`)
        }
        return
    }

    const text = (evt.record?.text as string) || ''
    if (!searchPattern.test(text)) return
    if (postIndex.some(p => p.uri === uri)) return

    postIndex.unshift({ uri, indexedAt: Date.now() })
    if (postIndex.length > config.maxPosts) postIndex.pop()

    const preview = text.substring(0, 60).replace(/\n/g, ' ')
    console.log(`${evt.action.toUpperCase()} ${uri}`)
    console.log(`  "${preview}${text.length > 60 ? '...' : ''}"`)
    console.log(`  ‚≠ê Added to index (${postIndex.length} total)`)
})

indexer.identity(async (evt) => {
    if (evt.status === 'active') return
    // Remove posts from disabled/deleted identities
    const removed = postIndex.filter(p => p.uri.includes(evt.did)).length
    if (removed > 0) {
        postIndex.splice(0, postIndex.length, ...postIndex.filter(p => !p.uri.includes(evt.did)))
        console.log(`Identity ${evt.did} (${evt.status}): removed ${removed} posts`)
    }
})

indexer.error((err) => console.error('Indexer error:', err))

const channel = tap.channel(indexer)
```

Here, you connect to `tap`, and use the `SimpleIndexer` function provided by the `@atproto/tap` library to process incoming records from the `app.bsky.feed.post` Lexicon ‚Äî i.e., Bluesky posts.

When a new post is discovered, you check if its text matches your search terms. If it does, you add it to your `postIndex` array. You'll also handle deletions and identity status changes to keep your index accurate.

Essentially, you're building an in-memory search index of posts that match your criteria. You'll then serve this index as a feed.

## Part 4: Serving the Feed

Now, you'll add [XRPC](/guides/lexicon#http-api-methods) routes to serve our feed. Your server will be handling two kinds of requests:

```
GET /xrpc/app.bsky.feed.describeFeedGenerator
GET /xrpc/app.bsky.feed.getFeedSkeleton
```

Add the Feed Generator server logic to `src/index.ts`:

```ts
// =============================================================================
// Feed Generator Server
// =============================================================================

// Auth is optional for this demo since we only log the requester's DID.
// In production, you may want to use credentials to personalize the feed.
const auth = serviceAuth({
    audience: config.publisherDid,
    unique: async () => true,
})

const router = new LexRouter()

router.add(app.bsky.feed.describeFeedGenerator, {
    auth,
    handler: (ctx) => {
        console.log('describeFeedGenerator from', ctx.credentials?.did)
        return {
            body: {
                did: config.publisherDid,
                feeds: [app.bsky.feed.describeFeedGenerator.feed.$build({ uri: FEED_URI })],
                links: {
                    privacyPolicy: 'https://example.com/privacy',
                    termsOfService: 'https://example.com/tos',
                },
            },
        }
    },
})

router.add(app.bsky.feed.getFeedSkeleton, {
    auth,
    handler: (ctx) => {
        if (ctx.params.feed !== FEED_URI) {
            throw new LexError('InvalidRequest', 'Feed not found')
        }
        console.log('getFeedSkeleton from', ctx.credentials?.did)

        const limit = Math.min(ctx.params.limit ?? 50, 100)
        const cursor = ctx.params.cursor as string | undefined

        let startIdx = 0
        if (cursor) {
            const cursorTime = parseInt(cursor, 10)
            startIdx = postIndex.findIndex(p => p.indexedAt < cursorTime)
            if (startIdx === -1) startIdx = postIndex.length
        }

        const slice = postIndex.slice(startIdx, startIdx + limit)
        const feed = slice.map(p => app.bsky.feed.defs.skeletonFeedPost.$build({ post: p.uri }))
        const lastPost = slice.at(-1)
        const nextCursor = lastPost && slice.length === limit && startIdx + limit < postIndex.length
            ? lastPost.indexedAt.toString()
            : undefined

        return { body: { feed, cursor: nextCursor } }
    },
})
```

Here, you're using `LexRouter` from the `@atproto/lex-server` package to define your XRPC routes. You'll notice that the `getFeedSkeleton` method returns a `cursor` in its response and takes a `cursor` param as input. This allows clients to paginate through your feed.

<Note>
This cursor is treated as an opaque value and fully at the Feed Generator's discretion. It is passed through the AppView directly to and from the client. The cursor be _unique per feed item_ to prevent unexpected behavior in pagination. For instance, a compound cursor with a timestamp + a CID:
`1683654690921::bafyreia3tbsfxe3cc75xrxyyn6qc42oupi73fxiox76prlyi5bpx7hr72u`
</Note>

Finally, you'll add the runtime logic to start the server.

## Part 5: Running the Server

You'll connect to tap with `channel.start()` then initiate your server. Add the runtime logic to `src/index.ts`:

```ts
// =============================================================================
// Start
// =============================================================================

channel.start()
console.log('Indexer connected to Tap server')

if (config.initialRepos.length > 0) {
    tap.addRepos(config.initialRepos).then(() => {
        console.log(`Added ${config.initialRepos.length} repo(s) to follow\n`)
    })
}

serve(router, { port: config.port }).then((server) => {
    const feedParam = encodeURIComponent(FEED_URI)

    console.log(`
Feed Generator Running

Server: http://localhost:${config.port}
Feed: ${config.feedName}
Terms: ${config.searchTerms.join(', ')}
Tap: ${config.tapUrl}
Repos: ${config.initialRepos.length}

To test (generate a JWT with goat):
goat account service-auth --aud ${config.publisherDid}

Then:
curl -H "Authorization: Bearer <jwt>" "http://localhost:${config.port}/xrpc/app.bsky.feed.getFeedSkeleton?feed=${feedParam}"

Listening for posts matching: ${config.searchTerms.join(', ')}
`)

    const shutdown = async () => {
        console.log('Shutting down...')
        await channel.destroy()
        await server.terminate()
        process.exit(0)
    }

    process.on('SIGINT', shutdown)
    process.on('SIGTERM', shutdown)
})
```

This kicks off both your connection to `tap` (your feed input) and your XRPC server (your feed output). It also provides some logging to help you test your feed generator.

Run it with `ts-node src/index.ts`. You should see some initial posts being indexed:

```
Indexer connected to Tap server

Feed Generator Running

Server: http://localhost:3000
Feed: whats-alf
Terms: alf
Tap: http://localhost:2480
Repos: 1

To test (generate a JWT with goat):
goat account service-auth --aud did:example:alice

Then:
curl -H "Authorization: Bearer <jwt>" "http://localhost:3000/xrpc/app.bsky.feed.getFeedSkeleton?feed=at%3A%2F%2Fdid%3Aexample%3Aalice%2Fapp.bsky.feed.generator%2Fwhats-alf"

Listening for posts matching: alf

Added 1 repo(s) to follow

CREATE at://did:plc:ragtjsm2j2vknwkz3zp4oxrd/app.bsky.feed.post/3jux6xlrdb42v
  "Run?? Alf was the date I struck out with ‚òπÔ∏è"
  ‚≠ê Added to index (1 total)
CREATE at://did:plc:ragtjsm2j2vknwkz3zp4oxrd/app.bsky.feed.post/3jux7x2uvip2v
  "god is that okay? I'm really confused right now, Alf broke m..."
  ‚≠ê Added to index (2 total)
```

<Note>
Remember that this example fetches only a single user's data repository. To fetch the entire network, stop and re-run `tap` with the `--signal-collection` flag:

```bash
tap run --collection-filters=app.bsky.feed.post \
  --signal-collection=app.bsky.feed.post
```
</Note>

## Conclusion

You've now built a custom Atproto feed that retrieves posts from the network, indexes them based on your own algorithm, and serves them via XRPC to clients like Bluesky.

Now that your feed server is running, you can customize it further, then publish it for others to use. To do that, you'll want to deploy it to a public server (e.g., AWS, DigitalOcean, etc.) and ensure it's accessible over HTTPS.

After that, you can clone down our original [feed-generator repo](https://github.com/bluesky-social/feed-generator) and run the [publishFeedGen.ts](https://github.com/bluesky-social/feed-generator/blob/main/scripts/publishFeedGen.ts) script interactively. This creates the necessary Atproto records to point to your feed server, and allow it to be discovered:

<Container>
  <Image src={bsky} alt="" className="w-full max-w-md mx-auto" />
</Container>

You can find more guides and tutorials in our [Guides](/guides) section, and more example apps in the [Cookbook](https://github.com/bluesky-social/cookbook/) repository. Happy building!

---
atproto/src/app/[locale]/guides/data-repos/en.mdx
---
import {DescriptionList, Description} from '@/components/DescriptionList'

export const header = {
  title: 'Personal Data Repositories',
  description:
    'A guide to the AT Protocol repo structure.',
}

# Data Repositories

A data repository is a collection of data published by a single user. Repositories are self-authenticating data structures, meaning each update is signed and can be verified by anyone.

They are described in more depth in the [Repository specification](/specs/repository).

## Data Layout

The content of a repository is laid out in a [Merkle Search Tree (MST)](https://hal.inria.fr/hal-02303490/document) which reduces the state to a single root hash. It can be visualized as the following layout:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Commit     ‚îÇ  (Signed Root)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Tree Nodes   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Record     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Every node is an [IPLD](https://ipld.io/) object ([dag-cbor](https://ipld.io/docs/codecs/known/dag-cbor/)) which is referenced by a [CID](https://github.com/multiformats/cid) hash. The arrows in the diagram above represent a CID reference.

This layout is reflected in the [AT URIs](/specs/at-uri-scheme):

```
Root       | at://alice.com
Collection | at://alice.com/app.bsky.feed.post
Record     | at://alice.com/app.bsky.feed.post/1234
```

A ‚Äúcommit‚Äù to a data repository is simply a keypair signature over a Root node‚Äôs CID. Each mutation to the repository produces a new Commit node.

## Identifier Types

Multiple types of identifiers are used within a Personal Data Repository.

<DescriptionList>
  <Description title="DIDs">[Decentralized IDs (DIDs)](https://w3c.github.io/did/) identify data repositories. They are broadly used as user IDs, but since every user has one data repository then a DID can be considered a reference to a data repository. The format of a DID varies by the ‚ÄúDID method‚Äù used but all DIDs ultimately resolve to a keypair and a list of service providers. This keypair can sign commits to the data repository.</Description>
  <Description title="CIDs">[Content IDs (CIDs)](https://github.com/multiformats/cid) identify content using a fingerprint hash. They are used throughout the repository to reference the objects (nodes) within it. When a node in the repository changes, its CID also changes. Parents which reference the node must then update their reference, which in turn changes the parent‚Äôs CID as well. This chains all the way to the Commit node, which is then signed.</Description>
  <Description title="NSIDs">[Namespaced Identifiers (NSIDs)](/specs/nsid) identify the Lexicon type for groups of records within a repository.</Description>
  <Description title="rkey">[Record Keys ("rkeys")](/specs/record-key) identify individual records within a collection in a given repository. The format is specified by the collection Lexicon, with some collections having only a single record with a key like "self", and other collections having many records, with keys using a base32-encoded timestamp called a Timestamp Identifier (TID).</Description>
</DescriptionList>

---
atproto/src/app/[locale]/guides/data-validation/en.mdx
---
export const metadata = {
  title: 'Data Validation',
  description:
    'Expectations around recommended data limits and validation',
}

# Data Validation

Some software does not need to worry much about Bluesky or atproto schema validation. For example, client apps and bots can generally assume that data they receive from the Bluesky API is valid, and when they try to create records the PDS is responsible for double-checking schema validation.

Software which consumes directly from the event stream (firehose) should be more careful. And developers designing new tools or clever hacks should be aware of some expectations and hard limits around data validation.

If an individual record fails to validate for any reason, the entire record should be ignored, but other records from the same repository should be processed. If there is a problem with the repository commit data or repository structure ("MST"), the repository should be marked as invalid, but old content does not need to be de-indexed or purged. As soon as a new valid commit is received, the repository should be marked valid again. This logic is mostly relevant to feed generators, AppViews, and moderation services, all of which may be consuming from the firehose.


## Recommended Data Limits

The following are informal guidelines, mostly to communicate order-of-magnitude expectations, and are likely to evolve over time. They are not part of the atproto specification, but if you are pushing the limits here, you may encounter interoperability issues now or in the future.

**CBOR Record Size:** try to keep individual records to a few dozen KBytes. If you need to store more data, even text data, consider using a blob instead. A reasonable maximum record size limit (`MAX_CBOR_RECORD_SIZE`) is 1 MiByte (1 mebibyte, or 1,048,576 bytes). Note that as of August 2025, the `subscribeRepos` Lexicon limits `#commit` message block size to 2,000,000 bytes (not mebibytes), so only a single 1 MiByte record can be created or updated per commit.

Note that event stream (firehose) "frames" may consist of multiple records, and larger limits are recommended for CBOR parsing in general, on the order of 4-5 MBytes.

**JSON Record Size:** the CBOR encoding is "canonical" for records, so focusing only on that encoding would make sense. But sometimes it is good to also have a limit on JSON encoding size. A reasonable limit (`MAX_JSON_RECORD_SIZE`) is 2 MiByte (2,097,152 bytes).

**General string length:** an overall length limit on strings within a record, including both those with and without Lexicon-specified string lengths. Measured as bytes (UTF-8 encoded). Should try to keep these to tens of KBytes at most. For an upper-bound limit (`MAX_RECORD_STRING_LEN`), reasonable to just rely on the overall CBOR record size limit. Notably, some early implementations had a 8 KByte (8192 bytes) limit.

**General `bytes` length:** same as the string limit, but for binary data (`MAX_RECORD_BYTES_LEN`). Recommend relying on the overall CBOR record size limit. As with record size overall, if more than a few dozen KBytes are needed, recommendation is to use blobs.

**CID binary encoding size:** recommend an overall limit (`MAX_CID_BYTES`) of 100 bytes.

**Container nesting depth:** for example, how many layers of map inside an array inside an array, etc. If your CBOR or JSON parsing library supports a limit, the default is probably fine. A reasonable limit (`MAX_CBOR_NESTED_LEVELS`) is 32 levels of nesting.

**Container element count:** for example, how many keys in a map, or elements in an array. If your CBOR or JSON parsing library supports a limit, the default is probably fine. A reasonable limit (`MAX_CBOR_CONTAINER_LEN`) is 128 x 1024 = 131,072 elements.

**Object key string length:** for example, how many bytes (UTF-8 encoded) are allowed in any key of an object. If your CBOR or JSON parsing library supports a limit, the default is probably fine. A reasonable limit (`MAX_CBOR_OBJECT_KEY_LEN`) is 8 KByte (8192).

**Integers:** as mentioned [in the atproto specification](https://atproto.com/specs/data-model#data-types), it is a strongly recommended best practice to keep integer values "64-bit float safe", meaning restricting them to 53 bits of precision. This ensures compatibility with JavaScript without loss of numeric precision. The specific values are `MAX_SAFE_INTEGER: 9007199254740991` and `MIN_SAFE_INTEGER: -9007199254740991`.


## Validation Without Schema

The [atproto Lexicon system](https://atproto.com/specs/lexicon) describes a data model and schema language for validating data against a known schema. But what if you are processing data where the schema isn't known or present?

Here are some guidelines, by data type:

- `integer`: should have values within safe limits (discussed in limits section)
- `string`: must be valid UTF-8 encoding. No particular Unicode normalization is expected or required. Empty strings are allowed, but it is preferred to take advantage of nullable or optional fields if possible.
- `bytes`: may be empty (length 0)
- `cid-link`: as discussed in the protocol specification: must be CIDv1; multibase should be raw in CBOR encoding (type `0x00`); multicodec should be `dag-cbor` (`0x71`) or `raw` (`0x55`); and multihash type SHA-256 is encouraged (but not strictly required)
- `array`: may have elements of heterogeneous type, because of the flexibility Lexicon unions provide.
- `object`: key must all be strings, and follow similar data requirements to `string` fields. Empty key strings are not allowed. Key names starting with `$` are reserved for protocol use (for example, `$bytes` and `$type`)
- `blob`: the `ref` must be a valid `cid-link`, and have `raw` multicodec (not `dag-cbor`). `size` may not be negative. `mimeType` can not be an empty string. The "legacy" blob format should be supported for reading, but new records created must be the regular blob format. Note that supprting additional reference types (eg, alternative multicodecs or hash types) are a likely evolution and extension point for the protocol in the future, but the current version of the repository and data model is narrowly defined.



---
atproto/src/app/[locale]/guides/faq/en.mdx
---
export const header = {
  title: 'FAQ',
  description:
    'Frequently Asked Questions about AT Protocol.',
}

# FAQ

Frequently Asked Questions about the Authenticated Transfer Protocol (Atproto). For FAQ about Bluesky, visit [here](https://bsky.social/about/faq).

## Is the AT Protocol a blockchain?

No. The AT Protocol is a [federated protocol](https://en.wikipedia.org/wiki/Federation_(information_technology)). It's not a blockchain nor does it use a blockchain.

## Why not use ActivityPub?

[ActivityPub](https://en.wikipedia.org/wiki/ActivityPub) is a federated social networking technology popularized by [Mastodon](https://joinmastodon.org/).

Account portability is a major reason why we chose to build a separate protocol. We consider portability to be crucial because it protects users from sudden bans, server shutdowns, and policy disagreements. Our solution for portability requires both [signed data repositories](/guides/data-repos) and [DIDs](/guides/identity), neither of which are easy to retrofit into ActivityPub. The migration tools for ActivityPub are comparatively limited; they require the original server to provide a redirect and cannot migrate the user's previous data.

Another major reason is scalability. ActivityPub depends heavily on delivering messages between a wide network of small-to-medium sized nodes, which can cause individual nodes to be flooded with traffic and generally struggles to provide global views of activity. The AT Protocol uses aggregating applications to merge activity from the users' hosts, reducing the overall traffic and dramatically reducing the load on individual hosts.

Other smaller differences include: a different viewpoint about how schemas should be handled, a preference for domain usernames over AP's double-@ email usernames, and the goal of having large scale search and algorithmic feeds.

## Why create Lexicon instead of using JSON-LD or RDF?

Atproto exchanges data and RPC commands across organizations. For the data and RPC to be useful, the software needs to correctly handle schemas created by separate teams. This is the purpose of [Lexicon](/guides/lexicon).

We want engineers to feel comfortable using and creating new schemas, and we want developers to enjoy the DX of the system. Lexicon helps us produce strongly typed APIs which are extremely familiar to developers and which provides a variety of runtime correctness checks.

[RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework) is intended for extremely general cases in which the systems share very little infrastructure. It's conceptually elegant but difficult to use, often adding a lot of syntax which devs don't understand. JSON-LD simplifies the task of consuming RDF vocabularies, but it does so by hiding the underlying concepts, not by making RDF more legible.

We looked very closely at using RDF but just didn't love the developer experience (DX) or the tooling it offered.

## What is ‚ÄúXRPC,‚Äù and why not use ___?

[XRPC](/specs/xrpc) is HTTP with some added conventions.

XRPC uses [Lexicons](/guides/lexicon) to describe HTTP calls and maps them to `/xrpc/{methodId}`. For example, this API call:

```typescript
await api.com.atproto.repo.listRecords({
  user: 'alice.com',
  collection: 'app.bsky.feed.post'
})
```

...maps to:

```text
GET /xrpc/com.atproto.repo.listRecords
  ?user=alice.com
  &collection=app.bsky.feed.post
```

Lexicons establish shared method id (`com.atproto.repo.listRecords`) and the expected query params, input body, and output body. By using Lexicons we get runtime checks on the inputs and outputs of the call, and can generate typed code like the API call example above.

---
atproto/src/app/[locale]/guides/feeds/en.mdx
---
import { TabGroup, TabList, Tab, TabPanels, TabPanel } from '@/components/mdx'
export const header = {
  title: 'Feeds',
  description: 'Creating and consuming custom feed generators',
}

## Custom Feeds

Custom feeds, or feed generators, are services that provide custom algorithms to users through the AT Protocol. This allows users to choose their own timelines, and for feed builders to [create and embed dedicated view](https://graze.leaflet.pub/3m4yjmbnyec2c) of AT records.

The way custom feeds work is straightforward: the server receives a request from a user's server and returns a list of post URIs with some optional metadata attached. Those posts are then hydrated into full views by the requesting server and sent back to the client.

Because Feeds will provide their own XRPC endpoints, they should use the server SDKs:

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
      Use the [lex-server](https://github.com/bluesky-social/atproto/tree/main/packages/lex/lex-server) package:

      ```bash
      npm install @atproto/lex-server
      lex install app.bsky.feed.getFeed ...
      lex build
      ```

      ```ts
      import { LexRouter, LexError } from '@atproto/lex-server'
      import { serve, upgradeWebSocket } from '@atproto/lex-server/nodejs'
      import * as app from './lexicons/app.js'

      const router = new LexRouter({ upgradeWebSocket })

      // Add handlers here

      const server = await serve(router, { port: 3000 })
      console.log('Server listening on port 3000')
      ```
    </TabPanel>
    <TabPanel value="go">
      In Go, you can implement HTTP endpoints manually:

      ```go
      type Feed interface {
          GetPage(ctx context.Context, feed string, userDID string, limit int64, cursor string) (
              feedPosts []*appbsky.FeedDefs_SkeletonFeedPost, newCursor *string, err error)
          Describe(ctx context.Context) ([]appbsky.FeedDescribeFeedGenerator_Feed, error)
      }

      ...

      router.GET("/.well-known/did.json", GetWellKnownDID)
      router.GET("/xrpc/app.bsky.feed.getFeedSkeleton", GetFeedSkeleton)
      router.GET("/xrpc/app.bsky.feed.describeFeedGenerator", DescribeFeeds)
      ```

      Refer to [https://github.com/jazware/go-bsky-feed-generator](https://github.com/jazware/go-bsky-feed-generator) for a community example.
    </TabPanel>
  </TabPanels>
</TabGroup>

A Feed Generator service can host one or more algorithms. The service itself is identified by DID, while each algorithm that it hosts is declared by a record in the repo of the account that created it. For instance, feeds offered by Bluesky will likely be declared in `@bsky.app`'s repo. Therefore, a given algorithm is identified by the at-uri of the declaration record. This declaration record includes a pointer to the service's DID along with some profile information for the feed.

The general flow of providing a custom algorithm to a user is as follows:

- A user requests a feed from the AppView using the at-uri of the declared feed
- The AppView resolves the at-uri and finds the DID doc of the Feed Generator
- The AppView sends a getFeedSkeleton request to the service endpoint declared in the Feed Generator's DID doc
  - This request is authenticated by a JWT signed by the user's repo signing key
- The Feed Generator returns a skeleton of the feed to the AppView
- The AppView hydrates the feed (user info, post contents, aggregates, etc.)
- The AppView returns the hydrated feed to the user

For users, this should feel like visiting a page in the app. Once they subscribe to a custom algorithm, it will appear in their home interface as one of their available feeds.

## Feed Generator Templates

We maintain a Feed Generator example in TypeScript. Refer to the [tutorial](/guides/custom-feed-tutorial) to get started. 

There are also community-maintained implementations in [Python](https://github.com/MarshalX/bluesky-feed-generator) and [Ruby](https://github.com/mackuba/bluesky-feeds-rb).

## Implementing Feeds

How a feed generator fulfills the `getFeedSkeleton` request is completely at their discretion. At the simplest end, a Feed Generator could supply a "feed" that only contains some hardcoded posts.

For most use cases, we recommend [subscribing to the firehose](/guides/streaming-data) at `com.atproto.sync.subscribeRepos`. This websocket will send you every record that is published on the network. Since Feed Generators do not need to provide hydrated posts, you can index as much or as little of the firehose as necessary. This is an important point ‚Äî needing to sync with the firehose to provide a feed *can* be resource-intensive, but does not have to be. Refer to the examples linked from this guide for more details.

Depending on your feed's algorithm, you likely do not need to keep posts around for long. Unless your feed is intended to provide "posts you missed" or something similar, you can likely garbage collect any data that is older than 48 hours from your feed.

## Language Handling

When making requests to `getFeedSkeleton`, clients are encouraged to populate the `Accept-Language` HTTP header with comma-separated BCP-47 language codes e.g. `en`,`pr-BR`. Feed generators can use this language context to filter or rank posts. If language filtering is applied, the feed generator should use the the `Content-Language` response header indicating the parsed language codes.

## Feed Feedback

Feed generators can also implement the `app.bsky.feed.sendInteractions` endpoint to receive input from users. These include likes, reposts, shares, and other interactions. See the [the app.bsky.feed Lexicon](https://github.com/bluesky-social/atproto/blob/main/lexicons/app/bsky/feed/defs.json#L247) for a complete list.

Implementing Feed Feedback is optional, but can help improve user experience.

## Example Feeds

- A community feed: Compile a list of DIDs within that community and filter the firehose for all posts from users within that list.
- A topical feed: Filter the algorithm for posts and pass the post text through some filtering mechanism (an LLM, a keyword matcher, etc.) that filters for the topic of your choice.
- [graze.social](https://www.graze.social/) provides an application model for Atproto feeds.
- You can find other examples of community-created feeds at [https://bsky.app/feeds](https://bsky.app/feeds).

## Further Reading and Resources

- [Sync](/guides/sync)
- [Backfilling](/guides/backfilling)
- [Streaming data](/guides/streaming-data)
- [Repository spec](/specs/repository)
- [Event Stream spec](/specs/event-stream)
- [Sync spec](/specs/sync)
- The [Microcosm](https://www.microcosm.blue/) community project maintains tools for working with AT records at scale without local mirroring.

---
atproto/src/app/[locale]/guides/glossary/en.mdx
---
export const header = {
  title: 'Glossary of terms',
  description:
    'A collection of terminology used in the AT Protocol and their definitions.',
}

# Glossary of terms

The AT Protocol uses a lot of terms that may not be immediately familiar. This page gives a quick reference to these terms and includes some links to more information. {{className: 'lead'}}

## Atmosphere

The "Atmosphere" is the term we use to describe the ecosystem around the [AT Protocol](#at-protocol).

## AT Protocol

The AT Protocol stands for "Authenticated Transfer Protocol," and is frequently shortened to "atproto." The name is in reference to the fact that all user-data is signed by the authoring users, which makes it possible to broadcast the data through many services and prove it's real without having to speak directly to the originating server.

The name is also a play on the "@" symbol, aka the "at" symbol, since atproto is designed for social systems.

## PDS (Personal Data Server)

A PDS, or Personal Data Server, is a server that hosts a user. A PDS will always store the user's [data repo](#data-repo) and signing keys. It may also assign the user a [handle](#handle) and a [DID](#did-decentralized-id). Many PDSes will host multiple users.

A PDS communicates with [AppViews](#app-view) to run applications. A PDS doesn't typically run any applications itself, though it will have general account management interfaces such as the OAuth login screen. PDSes actively sync their [data repos](#data-repo) with [Relays](#relay).

## AppView

An AppView is an application in the [Atmosphere](#atmosphere). It's called an "AppView" because it's just one view of the network. The canonical data lives in [data repos](#data-repo) which is hosted by [PDSes](#pds-personal-data-server), and that data can be viewed many different ways.

AppViews function a bit like search engines on the Web: they aggregate data from across the Atmosphere to produce their UIs. The difference is that AppViews also communicate with users' [PDSes](#pds) to publish information on their [repos](#data-repo), forming the full application model. This communication is established as a part of the OAuth login flow.

## Relay

A Relay is an aggregator of [data repos](#data-repo) from across the [Atmosphere](#atmosphere). They sync the repos from [PDSes](#pds) and produce a firehose of change events. [AppViews](#app-view) use a Relay to fetch user data.

Relays are an optimization and are not strictly necessary. An [AppView](#app-view) could communicate directly with [PDSes](#pds) (in fact, this is encouraged if needed). The Relay serves to reduce the number of connections that are needed in the network.

## Lexicon

Lexicon is a schema language. It's used in the [Atmosphere](#atmosphere) to describe [data records](#record) and HTTP APIs. Functionally it's very similar to [JSON-Schema](https://json-schema.org/) and [OpenAPI](https://www.openapis.org/).

Lexicon's sole purpose is to help developers build compatible software.

- [An introduction to Lexicon](/guides/lexicon)
- [Lexicon spec](/specs/lexicon)

## Data Repo

The "data repository" or "repo" is the public dataset which represents a user. It is comprised of [collections](#collection) of JSON [records](#record) and unstructured [blobs](#blob). Every repo is assigned a single permanent [DID](#did-decentralized-id) which identifies it. Repos may also have a single [domain handle](#handle) which act as human-readable names.

Data repositories are signed merkle trees. Their signatures can be verified against the key material published under the repo's [did](#did-decentralized-id).

- [An introduction to data repos](/guides/data-repos)
- [Repository spec](/specs/repository)

## Collection

The "collection" is a bucket of JSON [records](#record) in a [data repository](#data-repo). They support ordered list operations. Every collection is identified by an [NSID](#nsid-namespaced-id) which is expected to map to a [Lexicon](#lexicon) schema.

## Record

A "record" is a JSON document inside a [repo](#data-repo) [collection](#collection). The type of a record is identified by the `$type` field, which is expected to map to a [Lexicon](#lexicon) schema. The type is also expected to match the [collection](#collection) which contains it.

- [Record key spec](/specs/record-key)

## Blob

Blobs are unstructured data stored inside a [repo](#data-repo). They are most commonly used to store media such as images and video.

## Label

Labels are metadata objects which are attached to accounts ([DIDs](#did-decentralized-id)) and [records](#record). They are typically referenced by their values, such as "nudity" or "graphic-media," which identify the meaning of the label. Labels are primarily used by applications for moderation, but they can be used for other purposes.

- [Labels spec](/specs/label)

## Handle

Handles are domain names which are used to identify [data repos](#data-repo). More than one handle may be assigned to a repo. Handles may be used in `at://` URIs in the domain segment.

- [Handle spec](/specs/handle)
- [URI Scheme spec](/specs/at-uri-scheme)

## DID (Decentralized ID)

DIDs, or Decentralized IDentifiers, are universally-unique identifiers which represent [data repos](#data-repo). They are permanent and non-human-readable. DIDs are a [W3C specification](https://www.w3.org/TR/did-core/). The AT Protocol currently supports `did:web` and `did:plc`, two different DID methods.

DIDs resolve to documents which contain metadata about a [repo](#data-repo), including the address of the repo's [PDS](#pds), the repo's [handles](#handle), and the public signing keys.

- [DID spec](/specs/did)

## NSID (Namespaced ID)

NSIDs, or Namespaced IDentifiers, are an identifier format used in the [Atmosphere](#atmosphere) to identify [Lexicon](#lexicon) schemas. They follow a reverse DNS format such as `app.bsky.feed.post`. They were chosen because they give clear schema governance via the domain ownership. The reverse-DNS format was chosen to avoid confusion with domains in URIs.

- [NSID spec](/specs/nsid)

## TID (Timestamp ID)

TIDs, or Timestamp IDentifiers, are an identifier format used for [record](#record) keys. They are derived from the current time and designed to avoid collisions, maintain a lexicographic sort, and efficiently balance the [data repository's](#data-repo) internal data structures.

- [Record keys spec](/specs/record-key)

## CID (Content ID)

CIDs, or Content Identifiers, are cryptographic hashes of [records](#record). They are used to track specific versions of records.

## DAG-CBOR

DAG-CBOR is a serialization format used by [atproto](#at-protocol). It was chosen because it provides a reliable canonical form, which is important for cryptographic verification.

- [Data model spec](/specs/data-model)

## XRPC

XRPC is [atproto's](#at-protocol) HTTP API. It stands for "Cross-organizational Remote Procedure Calls" and is a thin wrapper around standard HTTPS. XRPC uses [Lexicon](#lexicon) schemas to define the valid parameters and responses for each API endpoint.

- [HTTP API spec](/specs/xrpc)


---
atproto/src/app/[locale]/guides/going-to-production/en.mdx
---
export const header = {
  title: 'Going to production',
  description:
    'Improving your Atmosphere stack deployment for production use at scale',
}

The Atmosphere is built to be distributed, which means anyone can and should feel free to host their own AT Protocol infrastructure. If you are self-hosting at scale, i.e. offering a service for many thousands of users, you will likely need to perform some additional ‚Äúhardening‚Äù of your hosted service, and make more complication decisions about provisioning your infrastructure.

The [code](https://github.com/bluesky-social/atproto) to power a fully distributed Atmosphere is constantly improving. Here's a guide with some lessons and best practices for managing your own services.

## PDS

We provide a [PDS install script](https://github.com/bluesky-social/pds) that is designed to get you up and running with your own PDS on a modestly resourced VPS. This will enable you to host your own data repositories, create and manage accounts, and serve your self-hosted data in the wider Atmosphere network or any other Atmosphere app. This section of the docs will help you take that PDS to production.

## Domain names

If you are running a non-Bluesky app in addition to a PDS deployment, **you will need separate domain names** for your PDS hosting and your app. This is because image and video blobs would otherwise be served from the same domain name as the OAuth and session management pages. This could lead to security issues such as credential theft, and means that using subdomains isn't a practical solution.

In other words, if you have an app at `https://greensky.app`, you should not use `https://pds.greensky.app` as a PDS hostname, you should use something like `https://greensky-pds.net`. If you used `https://pds.greensky.social` (which would make sense), it might rule out an OAuth client app at `https://chat.greensky.social` in the future. But you could do something like `https://chat.greensky.app`.

The PDS has restrictive CORS headers by default. 

It is difficult to change a PDS hostname once it has active accounts ‚Äî¬†every account needs to be migrated individually. This is absolutely possible using [PLC rotation keys](https://web.plc.directory/spec/v0.1/did-plc), but migrating is not turnkey. Also note that the PDS is the domain that users will enter their passwords on (and therefore remembered in password managers).

## Backups and Recovery

Our [official PDS install script](https://github.com/bluesky-social/pds) configures SQLite as a database backend. Specifically, it uses one SQLite DB for each user's [data repository](https://atproto.com/guides/data-repos), and a few others for PDS-wide data. This may be surprising, since SQLite doesn't have any mechanism for handling backups or replications out of the box, but SQLite has increasingly been found to [scale very well](https://fly.io/blog/sqlite-internals-rollback-journal/) with some additional tooling.

The secret sauce here is [Litestream](https://fly.io/blog/litestream-v050-is-here/), which does exactly that ‚Äî [Backups and Recovery for SQLite](https://github.com/benbjohnson/litestream). To get a recoverable, scalable PDS SQLite deployment with Litestream, we recommend:

- Setting `PDS_SQLITE_DISABLE_WAL_AUTO_CHECKPOINT` = `true` . This is in the [environment variable config](https://github.com/bluesky-social/atproto/blob/f8e56b387fcd3bc8405225c1bbdef66ca5dc1591/packages/pds/src/config/env.ts#L48) for your PDS. What this does is force the PDS SQLite client to keep write-ahead logging open and leave the actual checkpointing to Litestream.
- Use [Litestream with systemd](https://litestream.io/guides/systemd/) or another scheduler to find the most recently updated DBs and back them up. By default, individual user databases are located in `/pds/actors/`. Litestream will clean the write-ahead log and checkpoint automatically, so there's no risk of WAL growing out of control.
- If needed, ensure that this script also runs on SIGTERM when cleaning up containers or pods from your deployment.

In a disaster recovery scenario, you'd pull down all the most recent account database files (which might be missing a few hours of data) and the PDS-wide database files (which should be roughly complete). Then, run a recovery script which applies any repo operations from the PDS-level firehose output. This includes identity and account updates. Then, start the PDS back up.

## PLC key management

It's important to secure the extra PLC identity rotation key for hosted accounts ‚Äî e.g., in a [Key Management System](https://developer.hashicorp.com/vault/docs/secrets/key-management) or [Hardware Security Module](https://csrc.nist.gov/glossary/term/hardware_security_module_hsm). Pay attention to these two environment variables:

- `PDS_PLC_ROTATION_KEY_K256_PRIVATE_KEY_HEX`: used by the PDS service to perform operations on the PLC including recovering user accounts. If you are using AWS, this is configurable with the [AWS Key Management Service](https://aws.amazon.com/kms/), using `PDS_PLC_ROTATION_KEY_KMS_KEY_ID`.
- `PDS_RECOVERY_DID_KEY`: a public key that is added to the users DID. The corresponding private key **must be** stored in a safe place and can be used to recover user accounts in case of a catastrophic loss of data including the `PDS_PLC_ROTATION_KEY_K256_PRIVATE_KEY_HEX`.

## Object storage

By default, the PDS install script configures local disk storage for blobs (images, video, etc). This is fine for small deployments, but not recommended for production deployments.

Storage needs will vary, but we strongly recommend using S3-compatible storage (e.g. [Minio](https://github.com/minio/minio) or a block storage offering from a VPS provider) instead of storing images and video on local disk.

## Rate limits

The PDS implementation includes [repo operation limits](https://docs.bsky.app/docs/advanced-guides/rate-limits) by default. 

If you remove those limits, then the entire PDS might get limited at the [Relay](https://github.com/bluesky-social/indigo/tree/main/cmd/relay) (which itself has default event rate-limits for each PDS host), so be mindful of this if you are not also running your own Relay.

By default, our PDS install script will set up a single Node application process. These are stateless backend applications that run on top of SQLite. If you scale horizontally to run multiple Node processes, you'll also need to use [Redis](https://redis.io/) to allow them to share state. There are two environment variables used to configure Redis:

- `PDS_REDIS_SCRATCH_ADDRESS`  ‚Äî your Redis instance (e.g. localhost:6379)
- `PDS_REDIS_SCRATCH_PASSWORD` ‚Äî your Redis credentials.

This allows your rate limiting to scale gracefully.

If you are running multiple processes in parallel, you may also want to leverage [sidecar containers](https://litestream.io/guides/docker/#why-sidecar-containers-work) for your SQLite databases.

## Moderation

Moderation actions can be applied at both the AppView and the PDS level. However, if you are also running a Relay, you should be aware that the Relay admin web interface has forms for performing account-level and PDS-level actions if needed. There are also some moderation-related PDS endpoints not exposed via XRPC, which our [goat](https://github.com/bluesky-social/goat) CLI tool can interact with. Refer to [goat/pds_admin.go](https://github.com/bluesky-social/goat/blob/main/pds_admin.go) for examples.

[Ozone](https://github.com/bluesky-social/ozone) is our labeling service and moderation interface. If you are building your own AppView, AppViews can use the Ozone `subscribeLabel` "firehose" to receive updates. Ozone can also be granted permission to read additional PDS-level private information, such as the account email. This is helpful for things like account recovery.

You should generally run separate Ozone instances for moderating the PDS (since they can see private account info) and standalone AppViews, unless there is actually a single administrative org doing the moderation for both.

---
atproto/src/app/[locale]/guides/identity/en.mdx
---
import {DescriptionList, Description} from '@/components/DescriptionList'

export const header = {
  title: 'Identity',
  description:
    'How the AT Protocol handles user identity.',
}

The atproto identity system has a number of requirements:

* **ID provision.** Users should be able to create global IDs which are stable across services. These IDs should never change, to ensure that links to their content are stable.
* **Public key distribution.** Distributed systems rely on cryptography to prove the authenticity of data. The identity system must publish their public keys with strong security.
* **Key rotation.** Users must be able to rotate their key material without disrupting their identity.
* **Service discovery.** Applications must be able to discover the services in use by a given user.
* **Usability.** Users should have human-readable and memorable names.
* **Portability.** Identities should be portable across services. Changing a provider should not cause a user to lose their identity, social graph, or content.

Using the atproto identity system gives applications the tools for end-to-end encryption, signed user data, service sign-in, and general interoperation.

## Identifiers

We use two interrelated forms of identifiers: _handles_ and _DIDs_. Handles are DNS names while DIDs are a [W3C standard](https://www.w3.org/TR/did-core/) with multiple implementations which provide secure & stable IDs. AT Protocol supports the DID PLC and DID Web variants.

The following are all valid user identifiers:

```
alice.host.com
at://alice.host.com
did:plc:bv6ggog3tya2z3vxsub7hnal
```

The relationship between them can be visualized as:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ DNS name         ‚îú‚îÄ‚îÄresolves to‚îÄ‚îÄ‚Üí ‚îÇ DID           ‚îÇ
‚îÇ (alice.host.com) ‚îÇ                 ‚îÇ (did:plc:...) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üë                                   ‚îÇ
       ‚îÇ                               resolves to
       ‚îÇ                                   ‚îÇ
       ‚îÇ                                   ‚Üì
       ‚îÇ                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄreferences‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ DID Document  ‚îÇ
                                    ‚îÇ {"id":"..."}  ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

The DNS handle is a user-facing identifier ‚Äî it should be displayed in user interfaces and promoted as a way to find users. Applications resolve handles to DIDs and then use the DID as the canonical identifier for accounts. Any DID can be rapidly resolved to a DID document which includes public keys and user services.

<DescriptionList>
  <Description title="Handles">Handles are DNS names. They are resolved using DNS TXT records or an HTTP well-known endpoint, and must be confirmed by a matching entry in the DID document. Details in the <a href="/specs/handle">Handle specification</a>.</Description>
  <Description title="DIDs">DIDs are a <a href="https://www.w3.org/TR/did-core/">W3C standard</a> for providing stable & secure IDs. They are used as stable, canonical IDs of users. Details of how they are used in AT Protocol in the <a href="/specs/did">DID specification</a>.</Description>
  <Description title="DID Documents">
    DID Documents are standardized JSON objects which are returned by the DID resolution process. They include the following information:
    <ul>
      <li>The handle associated with the DID.</li>
      <li>The signing key.</li>
      <li>The URL of the user‚Äôs PDS.</li>
    </ul>
  </Description>
</DescriptionList>


## DID Methods

The [DID standard](https://www.w3.org/TR/did-core/) describes a framework for different "methods" of publishing and resolving DIDs to the [DID Document](https://www.w3.org/TR/did-core/#core-properties), instead of specifying a single mechanism. A variety of existing methods [have been registered](https://w3c.github.io/did-spec-registries/#did-methods), with different features and properties. We established the following criteria for use with atproto:

- **Strong consistency.** For a given DID, a resolution query should produce only one valid document at any time. (In some networks, this may be subject to probabilistic transaction finality.)
- **High availability**. Resolution queries must succeed reliably.
- **Online API**. Clients must be able to publish new DID documents through a standard API.
- **Secure**. The network must protect against attacks from its operators, a Man-in-the-Middle, and other users.
- **Low cost**. Creating and updating DID documents must be affordable to services and users.
- **Key rotation**. Users must be able to rotate keypairs without losing their identity.
- **Decentralized governance**. The network should not be governed by a single stakeholder; it must be an open network or a consortium of providers.

When we started the project, none of the existing DID methods met all of these criteria. Therefore, we chose to support both the existing [did-web](https://w3c-ccg.github.io/did-method-web/) method (which is simple), and a novel method we created called [DID PLC](https://github.com/bluesky-social/did-method-plc).

## Handle Resolution

Handles in atproto are domain names which resolve to a DID, which in turn resolves to a DID Document containing the user's signing key and hosting service.

Handle resolution uses either a DNS TXT record, or an HTTPS well-known endpoint. Details can be found in the [Handle specification](/specs/handle).


### Example: Hosting service

Consider a scenario where a hosting service is using PLC and is providing the handle for the user as a subdomain:

- The handle: `alice.pds.com`
- The DID: `did:plc:12345`
- The hosting service: `https://pds.com`

At first, all we know is `alice.pds.com`, so we look up the DNS TXT record `_atproto.alice.pds.com`. This tells us the DID: `did:plc:12345`.

Next we query the PLC directory for the DID, so that we can learn the hosting service's endpoint and the user's key material.

```typescript
await didPlc.resolve('did:plc:12345') /* => {
  id: 'did:plc:12345',
  alsoKnownAs: `https://alice.pds.com`,
  verificationMethod: [...],
  service: [{serviceEndpoint: 'https://pds.com', ...}]
}*/
```

We can now communicate with `https://pds.com` to access Alice's data.

### Example: Self-hosted

Let's consider a self-hosting scenario. If it's using `did:plc`, it would look something like:

- The handle: `alice.com`
- The DID: `did:plc:12345`
- The hosting service: `https://alice.com`

However, **if the self-hoster is confident they will retain ownership of the domain name**, they can use `did:web` instead of `did:plc`:

- The handle: `alice.com`
- The DID: `did:web:alice.com`
- The hosting service: `https://alice.com`

We can resolve the handle the same way, resolving `_atproto.alice.com`, which returns the DID: `did:web:alice.com`

Which we then resolve:

```typescript
await didWeb.resolve('did:web:alice.com') /* => {
  id: 'did:web:alice.com',
  alsoKnownAs: `https://alice.com`,
  verificationMethod: [...],
  service: [{serviceEndpoint: 'https://alice.com', ...}]
}*/
```


---
atproto/src/app/[locale]/guides/images-and-video/en.mdx
---
export const header = {
  title: 'Images and Video',
  description: 'Working with image and video blobs',
}

## Using blobs

"Blobs" are media files stored alongside an account's repository. They include images, video, and audio, but could also include any other file format. Blobs are referenced by individual records by the **`blob`** lexicon datatype, which includes a content hash (CID; `ref`) for the blob .

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
    ```typescript
    const imageBytes = fs.readFileSync('./photo.jpg')

    const uploadRes = await client.call(com.atproto.repo.uploadBlob, {
      encoding: 'image/jpeg',
      data: imageBytes,
    })

    const blob = uploadRes.data.blob
    console.log(JSON.stringify(blob, null, 2))
    ```

    ```
    {
      "$type": "blob",
      "ref": {
        "$link": "bafkreihdwdcefgh4dqkjv67uzcmw7ojee6xedzdetojuzjevtenxquvyku"
      },
      "mimeType": "image/jpeg",
      "size": 354028
    }
    ```

    Blob files are uploaded and distributed separately from records:

    ```typescript
    await client.create(app.bsky.feed.post, {
      text: 'Using a previously uploaded image',
      createdAt: new Date().toISOString(),
      embed: {
        $type: 'app.bsky.embed.images',
        images: [
          {
            alt: 'A photo uploaded earlier',
            image: blobRef,
            aspectRatio: { width: 1200, height: 800 },
          },
        ],
      },
    })
    ```
    </TabPanel>
    <TabPanel value="go">
    ```go
    imgBytes, err := os.ReadFile("./photo.jpg")
    if err != nil {
      panic(err)
    }

    uploadResp, err := comatproto.RepoUploadBlob(ctx, xrpcc, bytes.NewReader(imgBytes))
    if err != nil {
      panic(err)
    }

    fmt.Println(string(json.MarshalIndent(uploadResp, "", "  ")))
    ```

    ```
    {
      "blob": {
        "$type": "blob",
        "ref": {
          "$link": "bafkreihdwdcefgh4dqkjv67uzcmw7ojee6xedzdetojuzjevtenxquvyku"
        },
        "mimeType": "image/jpeg",
        "size": 354028
      }
    }
    ```

    Blob files are uploaded and distributed separately from records:

    ```go
    post := &bsky.FeedPost{
        Text:      "Using a previously uploaded image",
        CreatedAt: time.Now().UTC().Format(time.RFC3339),
        Embed: &bsky.FeedPost_Embed{
          EmbedImages: &bsky.EmbedImages{
            Images: []*bsky.EmbedImages_Image{
              {
                Alt: "A photo uploaded earlier",
                Image: &lexutil.LexBlob{
                  Ref:      uploadResp.Blob.Ref,
                  MimeType: http.DetectContentType(imgBytes),
                  Size:     uploadResp.Blob.Size,
                },
                AspectRatio: &bsky.EmbedDefs_AspectRatio{
                  Width:  1200,
                  Height: 800,
                },
              },
            },
          },
        },
      }
    ```
    </TabPanel>
  </TabPanels>
</TabGroup>

You can see what an existing Blob record looks like in [atproto.at](https://atproto.at/viewer?uri=did:plc:ewvi7nxzyoun6zhxrhs64oiz/app.bsky.feed.post/3juft56hdjg2o). It's this one right here:

![https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:ewvi7nxzyoun6zhxrhs64oiz/bafkreiebtvblnu4jwu66y57kakido7uhiigenznxdlh6r6wiswblv5m4py@jpeg](https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:ewvi7nxzyoun6zhxrhs64oiz/bafkreiebtvblnu4jwu66y57kakido7uhiigenznxdlh6r6wiswblv5m4py@jpeg)

And here's what the URL to that beautiful blob looks like when served from a CDN:

```
https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:ewvi7nxzyoun6zhxrhs64oiz/bafkreiebtvblnu4jwu66y57kakido7uhiigenznxdlh6r6wiswblv5m4py
```

Note the URL structure: the blob is served from the CDN, but the path includes the DID of the account that uploaded it, as well as the CID of the blob itself.

## Blob objects

Blobs are authoritatively stored by the account's PDS instance, but views are commonly served by CDNs associated with individual applications ("AppViews"), to reduce traffic on the PDS. CDNs may serve transformed (resized, transcoded, etc) versions of the original blob.

While blobs are universally content addressed (by CID), they are always referenced and managed in the context of an individual account (DID).

The empty blob (zero bytes) is valid in general, though it may be disallowed by individual Lexicons/applications.

Creating blobs requires the permission scope **`blob:*/*` .**

It is generally recommended to use an object storage implementation for working with blobs if you are hosting your own PDS. See [going to production](/guides/going-to-production#object-storage) for more.

## Further Reading and Resources

- [Blob lifecycle](/guides/blob-lifecycle)
- [Blob security](/guides/blob-security)
- [Video handling](/guides/video-handling)
- [Blob Specs](/specs/blob)
- [Streamplace](https://stream.place/docs/) provides an implementation and related Lexicon for video streaming on ATProto.

---
atproto/src/app/[locale]/guides/installing-lexicons/en.mdx
---
export const header = {
  title: 'Installing Lexicons',
  description: 'Using Lexicons in your dev environment',
}

## Using lex

Lexicon schemas enable code generation with types and validation. Our SDKs each install a `lex` tool that lets you generate a type-safe client that knows which parameters each AT endpoint expects.

## TypeScript

First, install the `@atproto/lex` package:

```bash
npm install -g @atproto/lex
```

This provides the `lex` command, which you can use to install Lexicons into a local project:

```bash
lex install app.bsky.feed.post app.bsky.feed.like
```

This creates:

- `lexicons.json` - manifest tracking installed Lexicons and their versions (CIDs)
- `lexicons/` - directory containing the Lexicon JSON files

Finally, generate TypeScript schemas from the installed Lexicons:

```bash
lex build
```

This generates TypeScript files in `./src/lexicons`. Now, you'll have these functions available to use in your code:

```tsx
import { Client } from '@atproto/lex'
import * as app from './lexicons/app.js'

// Create an unauthenticated client instance
const client = new Client('https://public.api.bsky.app')

// Start making requests using generated schemas
const response = await client.call(app.bsky.actor.getProfile, {
  actor: 'pfrazee.com',
})
```

For more guidance on working with `lex`, refer to the [readme](https://github.com/bluesky-social/atproto-internal/blob/msi/lex/packages/lex/lex/README.md#json-schemas).

## Further Reading and Resources

- [Publishing Lexicons](/guides/publishing-lexicons)
- [Lexicon Style Guide](/guides/lexicon-style-guide)
- [Lexicon Specs](/specs/lexicon)

---
atproto/src/app/[locale]/guides/labels/en.mdx
---
import {Container} from "@/components/Container"

export const header = {
  title: 'Labels',
  description: 'All about moderation labels',
}

## About labels

Moderation in Atproto consists of multiple, stackable systems, including:

1. Network takedowns which filter the content from the APIs
2. Labels placed on content by moderation services
3. User controls such as mutes and blocks

Developers building client applications should understand how to apply labels and user controls.

**Labels** are a form of metadata about any account or content in the AT ecosystem. Labels are published by *moderation services*, which are either hardcoded into the application or chosen by the user. They are attached to records in the responses under the `labels` key.

A label is published with the following information:

```markdown
{
  /** DID of the actor who created this label. */
  src: string
  /** AT URI of the record, repository (account), or other resource that this label applies to. */
  uri: string
  /** Optionally, CID specifying the specific version of 'uri' resource this label applies to. */
  cid?: string
  /** The short string name of the value or type of this label. */
  val: string
  /** If true, this is a negation label, overwriting a previous label. */
  neg?: boolean
  /** Timestamp when this label was created. */
  cts: string
}

```

## **Label values**

The *value* of a label will determine its behavior. Some example label values are `porn`, `gore`, and `spam`.

Label values are strings. They currently must only be lowercase a-z or a dash character `^[a-z-]+$`. Some of them start with `!`, but that can only be used by global label values.

Label values are interpreted by their definitions. Those definitions include these attributes:

- `blurs` which may be `content` or `media` or `none`
- `severity` which may be `alert` or `inform` or `none`
- `defaultSetting` which may be `hide` or `warn` or `ignore`
- `adultOnly` which is boolean

There are other definition attributes, but they are only used by the global label values.

## **Global label values**

There are a few label values which are defined by the protocol. They are:

- `!hide` which puts a generic warning on content that cannot be clicked through, and filters the content from listings. Not configurable by the user.
- `!warn` which puts a generic warning on content but can be clicked through. Not configurable by the user.
- `!no-unauthenticated` which makes the content inaccessible to logged-out users in applications which respect the label.
- `porn` which puts a warning on images and can only be clicked through if the user is 18+ and has enabled adult content.
- `sexual` which behaves like `porn` but is meant to handle less intense sexual content.
- `graphic-media` which behaves like `porn` but is for violence / gore.
- `nudity` which puts a warning on images but isn't 18+ and defaults to ignore.

There are two reasons global label values exist.

The first is because only label values which are defined globally can be used as self-labels (ie set by a user who is not a Labeler). The porn, sexual, gore, nudity, and !no-unauthenticated labels are global for this reason.

The second is because some special behaviors, like "non-configurable" and "applies only to logged out users," cannot be applied to custom labels. The !hide, !warn, and !no-unauthenticated labels are global for this reason.

## **Custom label values**

Labelers may define their own label values. Every Labeler has its own namespace of label values it defines. Custom definitions can override all global definitions for the defining Labeler except for the ones that start with a `!`, because those are reserved.

Since there are two behavior attributes (`blurs` and `severity`) with three values each, there are 9 possible behaviors for custom label values.

| **Blurs** | **Severity** | **Description** |
| --- | --- | --- |
| `content` | `alert` | Hide the content and put a "danger" warning label on the content if viewed |
| `content` | `inform` | Hide the content and put a "neutral" information label on the content if viewed |
| `content` | `none` | Hide the content |
| `media` | `alert` | Hide images in the content and put a "danger" warning label on the content if viewed |
| `media` | `inform` | Hide images in the content and put a "neutral" information label on the content if viewed |
| `media` | `none` | Hide images in the content |
| `none` | `alert` | Put a "danger" warning label on the content |
| `none` | `inform` | Put a "neutral" information label on the content |
| `none` | `none` | No visual effect |

Some examples of the definitions you might use for a label

- Harassment: `blurs=content` + `severity=alert`
- Spider warning: `blurs=media` + `severity=alert`
- Misinformation: `blurs=none` + `severity=alert`
- Verified user: `blurs=none` + `severity=inform`
- Curational down-regulate: `blurs=none` + `severity=none`

The `defaultSetting` establishes how the label will be configured when the user first subscribes to the labeler.

The `adultOnly` establishes whether the label should be configurable if adult content is disabled.

## **Label configuration**

A user may choose to hide, warn, or ignore each label from a labeler. Hiding and warning are basically similar, except that hide will also filter the labeled content from feeds and listings. Ignore just ignores the label. If adult content is not enabled in preferences, the behavior should force to hide with no override.

For more information, see the [Labels](/specs/label) spec.

## Further Reading and Resources

- [Moderation](/guides/moderation)
- [Subscriptions](/guides/subscriptions)
- [Creating a labeler](/guides/creating-a-labeler)
- [Using Ozone](/guides/using-ozone)
- [Label Specs](/specs/label)

---
atproto/src/app/[locale]/guides/lexicon-style-guide/en.mdx
---
export const header = {
  title: 'Lexicon Style Guide',
  description:
    'A style guide for creating new ATProto Lexicons',
}

## Overview

Here are some recommended conventions and best practices for designing Lexicon schemas.

Name casing conventions:

- Schemas & attributes: Use `lowerCamelCase` capitalization for schemas and names (as opposed to `UpperCamelCase`, `snake_case`, `ALL_CAPS`, etc).
- API error names: `UpperCamelCase`
- Fixed strings (eg `knownValues`): `kebab-case`

Acceptable characters:

- Field names should stick to the same character set as schema names (NSID name segments): ASCII alphanumeric, first character not a digit, no hyphens, case-sensitive
    - Exceptions may be justifiable in some situations, such as preservation of names in existing external schemas
    - Data objects should never contain schema-specified field names starting with `$` at any level of nesting; these are reserved for future protocol-level extensions

## Naming conventions

- Use singular nouns for `record` schemas
    - eg `post`, `like`, `profile`
- Use ‚Äúverb-noun‚Äù for `query` and `procedure` endpoints
    - eg `getPost`, `listLikes`, `putProfile`
    - Common verbs for `query` endpoints are: `get`, `list`, `search` (for full-text search), `query` (for flexible matching or filtering filtering)
    - Common verbs for `procedure` endpoints: `create`, `update`, `delete`, `upsert`, `put`
- Use ‚Äúsubscribe-plural-noun‚Äù for `subscription`
    - eg `subscribeLabels`
- Conventions for `permission-set` schema naming has not be established yet, but probably has ‚Äúauth‚Äù prefix (eg, `authBasic`)
- If an endpoint is experimental, unstable, or not intended for interoperability, indicate that in the NSID name
    - eg, include `.temp.` or `.unspecced.` in the NSID hierarchy
- Avoid generic names which conflict with popular programming language conventions
    - eg, avoid using `default` or `length` as schema names

## Documentation

- Add a description to every `main` schema definition (records, API endpoints, etc)
    - for API endpoints, mention in the description if authentication is required, and whether responses will be personalized if authentication is optional
- Add descriptions to potentially ambiguous fields and properties. This is particularly important for fields with generic names like `uri` or `cid`: CID of what?

NSID namespace grouping:

- Many applications and projects will have multiple distinct functions or features, and schemas of all types can have that grouping represented in the NSID hierarchy
    - eg `app.bsky.feed.*` , `app.bsky.graph.*`
- Very simple applications can include all endpoints under a single NSID ‚Äúgroup‚Äù
- use a `.defs` schema for definitions which might be reused by multiple schemas in the same namespace, or by third parties
    - eg `app.bsky.feed.defs`
    - putting these in a separate schema file means that deprecation or removal of other schema files doesn‚Äôt impact reuse
- Avoid conflicts and confusion between groups, names, and definitions
    - eg `app.bsky.feed.post#main` vs `app.bsky.feed.post.main`, or `com.example.record#foo` and `com.example.record.foo`
    - or defining both `app.bsky.feed` (as a record) and `app.bsky.feed.post` (with `app.bsky.feed` as a group)

## Other guidelines

- Specify the format of string fields when appropriate
- String fields in records should almost always have a maximum length if they don‚Äôt have a format type
    - Don‚Äôt redundantly specify both a format and length limits
    - If limiting the length of a string for semantic or visual reasons, grapheme limits should be used to ensure a degree of consistency across human languages. A data size (bytes) limit should also be added in these cases. A ratio of between 10 to 20 bytes to 1 grapheme is recommended.
- The string and bytes record data types are intended for constrained data size use-cases. For text or binary data of larger size, blob references should be used. This can include longer-form text and structured data.
- Enum sets are ‚Äúclosed‚Äù and can not be updated or extended without breaking schema evolution rules. For this reason they should almost always be avoided.
    - For strings, `knownValues` provides more flexible alternative
- String `knownValues` may include simple string constants, or may include schema references to a `token` (eg, the string `"com.example.defs#tokenOne"`)
    - Tokens provide an extension mechanism, and work well for values that have subjective definitions or may be expanded over time
    - See `com.atproto.moderation.defs#reasonType` and `com.atproto.sync.defs#hostStatus` for two contrasting instances, the former extensible and the later more constrained
- Take advantage of re-usable definitions, such as `com.atproto.repo.strongRef` (for versioned references to records) or `com.atproto.label.defs#label` (in an array, for hydrated labels)
- API endpoints which take an account identifier as an argument (eg, query parameter) should use `at-identifier` so that clients can avoid calling `resolveHandle` if they only have an account handle
- Record schemas should always use persistent identifiers (DIDs) for references to other accounts, instead of handles
- API endpoints should always specify an `output` with `encoding`, even if they have no meaningful response data
    - a good default is `application/json` with the schema being an object with no defined properties
- Optional `boolean` fields should be phrased such that `false` is the default and expected value
    - For example, if an endpoint can return a mix of ‚Äúfoo‚Äù and ‚Äúbar‚Äù, and the common behavior is to include ‚Äúfoo‚Äù but not ‚Äúbar‚Äù, then controlling parameters should be named `excludeFoo` (default `false`) and `includeBar` (default `false`), as opposed to `excludeBar` (default `true`)
- Content hashes (CIDs) may be represented as a string format or in binary encoding (`cid-link`)
    - In most situations, including versioned references between records, the string format is recommended.
    - Binary encoding is mostly used for protocol-level mechanisms, such as the firehose.

## Lexicon evolution

All Lexcions should be flexible to extension and evolution over time, without breaking the Lexicon schema evolution rules. This is particularly true for record schemas. Given the distributed storage model of atproto, developers do not have a reliable mechanism to update all data records in the network. Extensions could come from the original designer, or other developers and projects.

Experimental schemas and projects can use variant NSIDs (eg, including `.temp.` in the name hierarchy) to develop in the live network without committing to a stable record data schemas.

Major non-backwards-compatible schema changes are possible by declaring a new schema. The current naming convention is to append ‚ÄúV2‚Äù to the original name (or ‚ÄúV3‚Äù, etc).

Design recommendations to make schemas flexible to future evolution and extension:

- do not mark data fields or API parameters as `required` unless they are truly required for functionality
    - `required` fields can not be made optional or deprecated under the evolution rules
- you can add new `optional` fields to a schema without changing backwards compatibility or requiring a V2 schema, but you can‚Äôt add new `required` fields
- use object types containing a single element/field instead of atomic data types in arrays, to allow additional context to be included in the future
    - for example, in an API response listing accounts (DIDs), return an array of objects each with an `account` field listing the DID, instead of an array of strings
- make unions ‚Äúopen‚Äù in almost all situations, to allow future addition of types or values
    - open unions can be an extension mechanism for third parties to include self-defined data types

## Design Patterns

- There is a basic convention for pagination of `query` API endpoints:
    - query parameters include an optional `limit` (integer) and optional`cursor` (string)
    - the output body includes optional `cursor` (string) and a required array of response objects (with context-specific pluralized field name)
    - the initial client request does not define a `cursor`. If the response includes a `cursor`, then more results are available, and the client should query again with the new `cursor` to get more results
    - the `limit` value is an upper limit, and the response may include fewer (or even zero) results, while further results are still available. It is the lack of `cursor` in responses that indicates pagination is complete. The response set may have items removed if they are tombstoned or have been otherwise filtered from the response set.
- There is also a convention for subscription endpoints which support ‚Äúsequencing‚Äù and backfill cursors:
    - the endpoint has an optional `cursor` query parameter (integer)
    - all core message types include a `seq` field (integer). The `seq` of messages increases monotonically, though may have gaps.
    - if the `cursor` is not provided, the server will start returning new messages from the current point forward
    - if the `cursor` is provided, the server will attempt to return historical messages starting with the matching `seq`, continuing through to the current stream
    - if the `cursor` is in the future (higher than the current sequence), an error is returned and the connection closed
    - if the `cursor` is older than the earliest available message (or is 0), the server returns an info message of name `OutdatedCursor`, then returns messages starting from the oldest available
- A common pattern in API responses is to include ‚Äúhydrated views‚Äù of data records. For example, when viewing an account‚Äôs profile, the response might include CDN or thumbnail URLs for any media files, moderation labels, global aggregations, and viewer-specific social graph context.
    - For detailed views, a best practice to include the original record verbatim, instead of defining a new schema with a superset of fields. This is easier to maintain (can‚Äôt forget to update fields), and ensures any off-schema extension data is included.
    - Viewer-specific metadata should be optional and either indicated in descriptions or grouped under a sub-object. This makes schemas reusable between ‚Äúpublic‚Äù and ‚Äúlogged-in‚Äù views, and makes it clearer what information will be available when.
    - A helpful pattern for application developers is to ensure there is an API endpoint that accepts a reference to a record (eg, a AT URI or equivalent; or multiple references) returns the hydrated data object(s).
- the `app.bsky.richtext.facet` system can be used to annotate short text strings in a way that is simpler and safer to work with than full-featured markup languages
    - for more details see ["Why RichText facets in Bluesky"](https://www.pfrazee.com/blog/why-facets)
    - the feature type system is an open union which can be extended with additional types
    - more powerful systems like Markdown are more appropriate for long-form text
- One pattern for extending or supplementing a record is to define ‚Äúsidecar‚Äù records in the same account repository with the same record key and different types (collections).
    - Sidecar records can be defined and managed by the original Lexicon designer or by independent developers.
    - The sidecar records can be updated (mutated) without breaking strong references to the original record.
    - Sidecar context can be included in API responses.
- Because atproto accounts can be used flexibly with any application in the network, it can be ambiguous which accounts are participating in a particular app modality. This can be clarified if there is a known representative record type for the modality, and that clients create such a record for active accounts. Deletion of this record can be a way to indicate the user is no longer active. This works best if the record has a single known instance (fixed record key).
    - For example, an-app specific ‚Äúprofile‚Äù or ‚Äúdeclaration‚Äù record can indicate that the account has logged in to an associated app at least once, even if the record is ‚Äúempty‚Äù.
    - Backfill services can enumerate all accounts in the network with the given signaling record, and also process deletion of that record as deactivation of that modality.
    - This design pattern is strongly recommended for new app modalities.

---
atproto/src/app/[locale]/guides/lexicon/en.mdx
---
export const header = {
  title: 'Lexicons',
  description: 'A schema-driven interoperability framework',
}

## About lexicons

Lexicon is a schema system used to define RPC methods and record types. Every Lexicon schema is written in JSON, in a format similar to [JSON-Schema](https://json-schema.org/) for defining constraints.

**Lexicons provide Interoperability.** AT applications need a way to declare their own behaviors and semantics. Lexicon solves this while making it straightforward for developers to introduce new schemas.

The schemas are identified using [NSIDs](/specs/nsid), a reverse-DNS format. Here are some example API endpoints:

```
com.atproto.repo.getRecord
com.atproto.identity.resolveHandle
app.bsky.feed.getPostThread
app.bsky.notification.listNotifications
```

And here are some example record types:

```
app.bsky.feed.post
app.bsky.feed.like
app.bsky.actor.profile
app.bsky.graph.follow
```

The schema types, definition language, and validation constraints are described in the [Lexicon specification](/specs/lexicon), and representations in JSON and CBOR are described in the [Data Model specification](/specs/data-model).

## HTTP API methods

Atproto HTTP API methods each scope and implement a particular set of Lexicons. The AT Protocol's API system, [XRPC](/specs/xrpc), is essentially a thin wrapper around HTTPS. For example, a call to:

```tsx
client.call(app.bsky.actor.getProfile, {})
```

is actually just an HTTP request:

```
GET /xrpc/com.example.getProfile
```

The schemas establish valid query parameters, request bodies, and response bodies.

```json
{
  "lexicon": 1,
  "id": "com.example.getProfile",
  "defs": {
    "main": {
      "type": "query",
      "parameters": {
        "type": "params",
        "required": ["user"],
        "properties": { "user": { "type": "string" } }
      },
      "output": {
        "encoding": "application/json",
        "schema": {
          "type": "object",
          "required": ["did", "name"],
          "properties": {
            "did": { "type": "string" },
            "name": { "type": "string" },
            "displayName": { "type": "string", "maxLength": 64 },
            "description": { "type": "string", "maxLength": 256 }
          }
        }
      }
    }
  }
}
```

## Record types

Schemas define the possible values of a record. Every record has a ‚Äútype‚Äù which maps to a schema and also establishes the URL of a record.

For instance, this ‚Äúfollow‚Äù record:

```json
{
  "$type": "com.example.follow",
  "subject": "at://did:plc:12345",
  "createdAt": "2022-10-09T17:51:55.043Z"
}
```

‚Ä¶would have a URL like:

```
at://bob.com/com.example.follow/12345
```

‚Ä¶and a schema like:

```json
{
  "lexicon": 1,
  "id": "com.example.follow",
  "defs": {
    "main": {
      "type": "record",
      "description": "A social follow",
      "record": {
        "type": "object",
        "required": ["subject", "createdAt"],
        "properties": {
          "subject": { "type": "string" },
          "createdAt": { "type": "string", "format": "datetime" }
        }
      }
    }
  }
}
```

## Versioning

Once a Lexicon is published, it can never change its constraints. Loosening a constraint (adding possible values) will cause old software to fail validation for new data, and tightening a constraint (removing possible values) will cause new software to fail validation for old data. As a consequence, lexicons may only add optional constraints to previously unconstrained fields.

If a lexicon must change a previously-published constraint, it should be published as a new lexicon under a new NSID.

## Further Reading and Resources

Lexicons can be [installed locally](/guides/installing-lexicons) for each of our SDKs from our Lexicon registry. You can also [publish your own Lexicons](/guides/publishing-lexicons) following our [Style Guide](/guides/lexicon-style-guide).

- [Installing Lexicons](/guides/installing-lexicons)
- [Publishing Lexicons](/guides/publishing-lexicons)
- [Lexicon Style Guide](/guides/lexicon-style-guide)
- [Lexicon Specs](/specs/lexicon)

---
atproto/src/app/[locale]/guides/moderation/en.mdx
---
import {Container} from "@/components/Container"
import moderation from "./moderation.png"

export const header = {
  title: 'Moderation',
  description: 'Moderation in the Atmosphere',
}

## About moderation

The Atproto model is that *speech* and *reach* should be two separate layers, built to work with each other. The ‚Äúspeech‚Äù layer should remain permissive, distributing authority and designed to ensure everyone has a voice. The ‚Äúreach‚Äù layer lives on top, built for flexibility and designed to scale. Atproto moderation is implemented using [Labels](/guides/labels).

<Container>
  <Image src={moderation} alt="" className="w-full max-w-md mx-auto" />
</Container>

Our moderation architecture is provided by two services: [*Osprey*](#algorithmic-moderation), an event stream decisions engine and analysis UI designed to investigate and take automatic action; and [*Ozone*](#using-ozone), a labeling service and web frontend for making moderation decisions.

[Read more](https://docs.bsky.app/blog/blueskys-moderation-architecture) about moderation on our blog.

## Algorithmic moderation

[Osprey](https://github.com/bluesky-social/osprey-atproto) is an event stream decisions engine and analysis UI designed to investigate and take automatic action, to enable sustainable at-scale moderation. It makes use of [Kafka](https://kafka.apache.org/) with its own [rules engine](https://github.com/roostorg/osprey/blob/main/docs/rules.md).

From another perspective, Osprey is a Python library for processing actions through human written rules and outputting labels, webhooks back to an API and other sinks. It evaluates events using structured logic, user-defined functions, and external signals to assign labels, verdicts, and actions. It can make use of fine-tuned LLMs and other classifiers that expose their own web endpoints to Osprey. LLMs can be a useful tool for surfacing moderation reports; these reports can be acted on automatically and/or manually depending on your configuration. 

Osprey is maintained by [Roost](https://roost.tools/), who build open source moderation tools.

## Further Reading and Resources

Learn all about the types and format of moderation [labels](/guides/labels). Then, you can [subscribe](/guides/subscriptions) to moderation labelers to have their labels applied in your application. You can also [create your own labeler](/guides/creating-a-labeler) to publish labels. If you need to moderate content manually, you can use [Ozone](/guides/using-ozone) as a web interface for labeling content.

- [Labels](/guides/labels)
- [Creating a labeler](/guides/creating-a-labeler)
- [Subscriptions](/guides/subscriptions)
- [Label Specs](/specs/label)
- [Using Ozone](/guides/using-ozone)

---
atproto/src/app/[locale]/guides/oauth-patterns/en.mdx
---
import {Container} from "@/components/Container"
import {ResponsiveTable} from '@/components/ResponsiveTable'

export const header = {
  title: 'OAuth Patterns',
  description: 'OAuth implementation examples',
}

## TypeScript

- [@atproto/oauth-client-browser](https://github.com/bluesky-social/atproto/tree/main/packages/oauth/oauth-client-browser#readme) ([npmjs](https://www.npmjs.com/package/@atproto/oauth-client-browser)) - Suitable for frontend-only applications (e.g. SPAs).
- [@atproto/oauth-client-node](https://github.com/bluesky-social/atproto/tree/main/packages/oauth/oauth-client-node#readme) ([npmjs](https://www.npmjs.com/package/@atproto/oauth-client-node)) - Suitable for Electron desktop apps or server-side deployments.
- [@atproto/oauth-client-expo](https://github.com/bluesky-social/atproto/tree/main/packages/oauth/oauth-client-expo#readme) ([npmjs](https://www.npmjs.com/package/@atproto/oauth-client-expo)) - For use in React Native projects (e.g. mobile apps)
- [@atproto/oauth-client](https://github.com/bluesky-social/atproto/tree/main/packages/oauth/oauth-client#readme) ([npmjs](https://www.npmjs.com/package/@atproto/oauth-client)) (This is the core implementation on which the above three libraries depend)

## Go

- [Indigo](https://github.com/bluesky-social/indigo/tree/main/atproto/auth/oauth) ([Online API Docs](https://pkg.go.dev/github.com/bluesky-social/indigo/atproto/auth/oauth)) - Suitable for native apps or server-side deployments

## Atproto OAuth Example Apps

These simple example apps demonstrate usage of their respective SDKs.

<ResponsiveTable
  columns={[{label: 'Example App'}, {label: 'Language'}, {label: 'SDK'}, {label: 'Architecture'}, {label: 'Confidential Client?'}, {label: 'Localhost development client ID?', href: '/specs/oauth#localhost-client-development'}]}
  rows={[
    [{label: 'cookbook/react-native-oauth', href:' https://github.com/bluesky-social/cookbook/tree/main/react-native-oauth#readme' }, {label: 'TypeScript'}, {label: '@atproto/oauth-client-expo'}, {label: 'Mobile App (no backend)'}, {label: 'No'}, {label: 'No (requires hosted client metadata)'},],
    [{label: 'cookbook/vanillajs-oauth-web-app', href:' https://github.com/bluesky-social/cookbook/tree/main/vanillajs-oauth-web-app#readme' }, {label: 'JavaScript'}, {label: '@atproto/oauth-client-browser'}, {label: 'Web SPA (no backend)'}, {label: 'No'}, {label: 'Optionally'},],
    [{label: 'cookbook/go-oauth-web-app', href:' https://github.com/bluesky-social/cookbook/tree/main/go-oauth-web-app#readme' }, {label: 'Go'}, {label: 'indigo'}, {label: 'Web BFF'}, {label: 'Optionally'}, {label: 'Optionally'},],
    [{label: 'cookbook/go-oauth-cli-app', href:' https://github.com/bluesky-social/cookbook/tree/main/go-oauth-cli-app#readme' }, {label: 'Go'}, {label: 'indigo'}, {label: 'Native CLI'}, {label: 'No'}, {label: 'No (requires hosted client metadata)'},],
    [{label: 'cookbook/python-oauth-web-app', href:' https://github.com/bluesky-social/cookbook/tree/main/python-oauth-web-app#readme' }, {label: 'Python'}, {label: 'None'}, {label: 'Web BFF'}, {label: 'Optionally'}, {label: 'Optionally'},]
  ]}
/>

<Note>
The Python example does not use an SDK! It may be useful as reference when building an OAuth SDK from scratch, alongside the advanced [OAuth client implementation guide](https://docs.bsky.app/docs/advanced-guides/oauth-client)
</Note>

## Types of App

The *simplest* type of app is one where the OAuth session is established directly between the user's device and the PDS, as demonstrated in the [cookbook/vanillajs-oauth-web-app](https://github.com/bluesky-social/cookbook/tree/main/vanillajs-oauth-web-app#readme) example. It's simple because it doesn't require a dedicated backend service, only hosting static resources.

If your app doesn't need long-lived sessions, then this approach is completely fine. But for other use cases, session lifetime will be limited because the app is a "public client", as opposed to a "confidential client". Becoming a confidential client involves establishing a secret key, bound to the client ID. If the client is publicly available and runs on the user's own device, it cannot protect a client secret. Thus, implementing a confidential client necessitates hosting some backend infrastructure, which the [cookbook/go-oauth-web-app](https://github.com/bluesky-social/cookbook/tree/main/go-oauth-web-app#readme) and [cookbook/python-oauth-web-app](https://github.com/bluesky-social/cookbook/tree/main/python-oauth-web-app#readme) examples demonstrate (both using the "BFF" pattern, see below).

See [Types of Clients](/specs/oauth#types-of-clients) for more technical details on confidential vs. public clients.

There are several design strategies that can be used to "upgrade" a public client into a confidential client:

- [Backend For Frontend (BFF)](https://datatracker.ietf.org/doc/html/draft-ietf-oauth-browser-based-apps#name-backend-for-frontend-bff): The OAuth session is established between the app backend server and the PDS. This server-side session is associated with the user's frontend session via mechanisms such as session cookies. The frontend makes requests to the PDS by proxying them through the backend, which handles the OAuth authorization.
- [Token-Mediating Backend (TMB)](https://datatracker.ietf.org/doc/html/draft-ietf-oauth-browser-based-apps#name-token-mediating-backend): Similar to the BFF pattern, except the backend passes OAuth access tokens to the frontend once the session has been established, allowing the frontend to make direct requests to the PDS.
- [Client Assertion Backend](https://github.com/bluesky-social/proposals/tree/main/0010-client-assertion-backend): A proposed alternative to the TMB pattern with simplified server-side logic.

Of these three, the BFF pattern is the currently recommended approach for building confidential-client applications. This is demonstrated in the [cookbook/go-oauth-web-app](https://github.com/bluesky-social/cookbook/tree/main/go-oauth-web-app#readme) and [cookbook/python-oauth-web-app](https://github.com/bluesky-social/cookbook/tree/main/python-oauth-web-app#readme) examples.

## Further Reading and Resources

- [Auth](/guides/auth)
- [SDK authentication](/guides/sdk-auth)
- [Scopes](/guides/scopes)
- [Permission requests](/guides/permission-sets)
- [OAuth spec](/specs/oauth)
- [Permissions spec](/specs/permission)

---
atproto/src/app/[locale]/guides/oauth-tutorial/en.mdx
---
import {Container} from "@/components/Container"
import workingauth from "./authentication-working.png"
import starter from "./nextjs-starter.png"
import metadata from "./client-metadata.png"

export const header = {
  title: 'OAuth with NextJS Tutorial',
  description: 'Build a Next.js app supporting OAuth with ATProto identity.',
}

In this tutorial, you'll build a Next.js app where users can log in with their AT Protocol identity using OAuth. If you [host your own PDS](/guides/self-hosting), that means you can provide your own auth source for any apps in the Atmosphere.

You can find the source code for this tutorial, along with other example projects, in the [Cookbook](https://github.com/bluesky-social/cookbook/) repository, under `nextjs-oauth`.

See a demo version running at: [https://nextjs-oauth-tutorial.up.railway.app/](https://nextjs-oauth-tutorial.up.railway.app/).

## Prerequisites

This OAuth tutorial does not use many Atproto concepts or dependencies. You should have a working understanding of Next.js and TypeScript.

You should have installed:
- Node.js 20+
- The pnpm package manager

On platforms supported by [homebrew](https://brew.sh/), you can install these with:

```bash
brew install node pnpm
```

## Part 1: Project Setup

You'll start by creating a new Next.js project using the `create-next-app` command.

```bash
npx create-next-app@latest my-app --yes
cd my-app
```

Next, add the `oauth-client-node` package from the Atproto ecosystem, which provides OAuth client functionality.

```bash
pnpm add @atproto/oauth-client-node
```

That's all you need to get started! You can now run the development server with:

```bash
pnpm dev
```

Navigate to your app in a browser at [http://127.0.0.1:3000](http://127.0.0.1:3000).

<Note>
In general, when working with OAuth, you should use `127.0.0.1` URLs instead of `localhost`. This will be important for the OAuth redirect flow to work correctly.
</Note>

You should receive the Next.js starter page:

<Container>
  <Image src={starter} alt="" className="w-full max-w-md mx-auto" />
</Container>

Next, you'll implement baseline OAuth functionality.

## Part 2: Implementing OAuth

You'll start out implementing OAuth using a local *loopback* client with in-memory storage.

OAuth loopback works by using a special redirect URI (like http://127.0.0.1:PORT/callback) to send authorization codes directly back to an app, which listens on a local port, avoiding browser-based callbacks and enabling secure, direct communication for token exchange. Instead of a web redirect, the app opens a local listener, you can auth in a browser, and the server redirects back to the app's local URI, letting the app grab the code and exchange it for a token. It's particularly useful for development scenarios.

To do this, you'll implement a [confidential client](/specs/oauth#types-of-clients). Your Next.js server will hold credentials for talking to a user's PDS. The server will verify incoming requests from the browser using cookies for session auth.

First, create `lib/auth/client.ts`:

```typescript
import {
  NodeOAuthClient,
  buildAtprotoLoopbackClientMetadata,
} from "@atproto/oauth-client-node";
import type {
  NodeSavedSession,
  NodeSavedState,
} from "@atproto/oauth-client-node";

export const SCOPE = "atproto";

// Use globalThis to persist across Next.js hot reloads
const globalAuth = globalThis as unknown as {
  stateStore: Map<string, NodeSavedState>;
  sessionStore: Map<string, NodeSavedSession>;
};
globalAuth.stateStore ??= new Map();
globalAuth.sessionStore ??= new Map();

let client: NodeOAuthClient | null = null;

export async function getOAuthClient(): Promise<NodeOAuthClient> {
  if (client) return client;

  client = new NodeOAuthClient({
    clientMetadata: buildAtprotoLoopbackClientMetadata({
      scope: SCOPE,
      redirect_uris: ["http://127.0.0.1:3000/oauth/callback"],
    }),
  
    stateStore: {
      async get(key: string) {
        return globalAuth.stateStore.get(key);
      },
      async set(key: string, value: NodeSavedState) {
        globalAuth.stateStore.set(key, value);
      },
      async del(key: string) {
        globalAuth.stateStore.delete(key);
      },
    },

    sessionStore: {
      async get(key: string) {
        return globalAuth.sessionStore.get(key);
      },
      async set(key: string, value: NodeSavedSession) {
        globalAuth.sessionStore.set(key, value);
      },
      async del(key: string) {
        globalAuth.sessionStore.delete(key);
      },
    },
  });

  return client;
}
```

`stateStore` is temporary storage using during the OAuth flow; `sessionStore` is persistent storage keyed by a user's DID. The `globalThis` pattern is a standard Next.js technique for persisting data across hot module reloads in development. Without this, the in-memory stores would be wiped every time you edit a file.

AT Protocol OAuth has a special carveout for local development. The `client_id` must be `localhost` and the `redirect_uri` must be on host 127.0.0.1. Read more in the [Localhost Client Development specs](/specs/oauth#localhost-client-development).

<Note>
There's no way around it ‚Äî this section of the tutorial is pretty dry! You'll be implementing standard OAuth flows, which are inherently boilerplate-heavy. But once you have this baseline working, you'll be able to build on top of it!
</Note>

Next, create `lib/auth/session.ts` to manage user sessions:

```typescript
import { cookies } from "next/headers";
import { getOAuthClient } from "./client";
import type { OAuthSession } from "@atproto/oauth-client-node";

export async function getSession(): Promise<OAuthSession | null> {
  const did = await getDid();
  if (!did) return null;

  try {
    const client = await getOAuthClient();
    return await client.restore(did);
  } catch {
    return null;
  }
}

export async function getDid(): Promise<string | null> {
  const cookieStore = await cookies();
  return cookieStore.get("did")?.value ?? null;
}
```

Then, create `app/oauth/login/route.ts`. This route initiates the login flow. You only need the user's handle ‚Äî with it, you can resolve the user's Authorization Server (their PDS) and redirect them there:

```typescript
import { NextRequest, NextResponse } from "next/server";
import { getOAuthClient, SCOPE } from "@/lib/auth/client";

export async function POST(request: NextRequest) {
  try {
    const { handle } = await request.json();

    if (!handle || typeof handle !== "string") {
      return NextResponse.json(
        { error: "Handle is required" },
        { status: 400 }
      );
    }

    const client = await getOAuthClient();

    // Resolves handle, finds their auth server, returns authorization URL
    const authUrl = await client.authorize(handle, {
      scope: SCOPE,
    });

    return NextResponse.json({ redirectUrl: authUrl.toString() });
  } catch (error) {
    console.error("OAuth login error:", error);
    return NextResponse.json(
      { error: error instanceof Error ? error.message : "Login failed" },
      { status: 500 }
    );
  }
}
```

After a user approves the authorization consent screen, they'll be redirected to a callback route. Here, you'll exchange the code from the redirect for actual credentials, then set a cookie for the user's DID. Create `app/oauth/callback/route.ts`:

```typescript
import { NextRequest, NextResponse } from "next/server";
import { getOAuthClient } from "@/lib/auth/client";

const PUBLIC_URL = process.env.PUBLIC_URL || "http://127.0.0.1:3000";

export async function GET(request: NextRequest) {
  try {
    const params = request.nextUrl.searchParams;
    const client = await getOAuthClient();

    // Exchange code for session
    const { session } = await client.callback(params);

    const response = NextResponse.redirect(new URL("/", PUBLIC_URL));

    // Set DID cookie
    response.cookies.set("did", session.did, {
      httpOnly: true,
      secure: process.env.NODE_ENV === "production",
      sameSite: "lax",
      maxAge: 60 * 60 * 24 * 7, // 1 week
      path: "/",
    });

    return response;
  } catch (error) {
    console.error("OAuth callback error:", error);
    return NextResponse.redirect(new URL("/?error=login_failed", PUBLIC_URL));
  }
}
```

`PUBLIC_URL` falls back to `127.0.0.1:3000` to ensure you always redirect to the correct host. This avoids cookie issues that arise from `localhost` vs `127.0.0.1` mismatches.

Next, you should also create a logout route at `app/oauth/logout/route.ts` to clear the user's session:

```typescript
import { NextResponse } from "next/server";
import { cookies } from "next/headers";
import { getOAuthClient } from "@/lib/auth/client";

export async function POST() {
  try {
    const cookieStore = await cookies();
    const did = cookieStore.get("did")?.value;

    if (did) {
      const client = await getOAuthClient();
      await client.revoke(did);
    }

    cookieStore.delete("did");
    return NextResponse.json({ success: true });
  } catch (error) {
    console.error("Logout error:", error);
    const cookieStore = await cookies();
    cookieStore.delete("did");
    return NextResponse.json({ success: true });
  }
}
```

Finally, you'll make one last route, to expose the `oauth-client-metadata.json` at a well-known URL. This is [used in ATProto OAuth](/specs/oauth#clients) to discover client configuration. Create `app/oauth-client-metadata.json/route.ts`:

```typescript
import { getOAuthClient } from "@/lib/auth/client";
import { NextResponse } from "next/server";

// The URL of this endpoint IS your client_id
// Authorization servers fetch this to learn about your app

export async function GET() {
  const client = await getOAuthClient();
  return NextResponse.json(client.clientMetadata);
}
```

You can visit http://127.0.0.1:3000/oauth-client-metadata.json to see your client configuration.

<Container>
  <Image src={metadata} alt="" className="w-full max-w-md mx-auto" />
</Container>

Now you just need to add a few forms. Create `components/LoginForm.tsx`:

```tsx
"use client";

import { useState } from "react";

export function LoginForm() {
  const [handle, setHandle] = useState("");
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);

  async function handleSubmit(e: React.FormEvent) {
    e.preventDefault();
    setLoading(true);
    setError(null);

    try {
      const res = await fetch("/oauth/login", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ handle }),
      });

      const data = await res.json();

      if (!res.ok) {
        throw new Error(data.error || "Login failed");
      }

      // Redirect to authorization server
      window.location.href = data.redirectUrl;
    } catch (err) {
      setError(err instanceof Error ? err.message : "Login failed");
      setLoading(false);
    }
  }

  return (
    <form onSubmit={handleSubmit} className="space-y-4">
      <div>
        <label className="block text-sm font-medium text-zinc-700 dark:text-zinc-300 mb-1">
          Handle
        </label>
        <input
          type="text"
          value={handle}
          onChange={(e) => setHandle(e.target.value)}
          placeholder="user.example.com"
          className="w-full px-3 py-2 border border-zinc-300 dark:border-zinc-700 rounded-lg bg-white dark:bg-zinc-800 text-zinc-900 dark:text-zinc-100"
          disabled={loading}
        />
      </div>

      {error && <p className="text-red-500 text-sm">{error}</p>}

      <button
        type="submit"
        disabled={loading || !handle}
        className="w-full py-2 px-4 bg-blue-600 text-white rounded-lg hover:bg-blue-700 disabled:opacity-50"
      >
        {loading ? "Signing in..." : "Sign in"}
      </button>
    </form>
  );
}
```

Do the same for `components/LogoutButton.tsx`:

```tsx
"use client";

import { useRouter } from "next/navigation";

export function LogoutButton() {
  const router = useRouter();

  async function handleLogout() {
    await fetch("/oauth/logout", { method: "POST" });
    router.refresh();
  }

  return (
    <button
      onClick={handleLogout}
      className="text-sm text-zinc-500 hover:text-zinc-700 dark:text-zinc-400 dark:hover:text-zinc-200"
    >
      Sign out
    </button>
  );
}
```

And, finally, replace the starter `app/page.tsx` to use these components and show the user's DID when logged in:

```tsx
import { getSession } from "@/lib/auth/session";
import { LoginForm } from "@/components/LoginForm";
import { LogoutButton } from "@/components/LogoutButton";

export default async function Home() {
  const session = await getSession();

  return (
    <div className="flex min-h-screen items-center justify-center bg-zinc-50 dark:bg-zinc-950">
      <main className="w-full max-w-md mx-auto p-8">
        <div className="text-center mb-8">
          <h1 className="text-3xl font-bold text-zinc-900 dark:text-zinc-100 mb-2">
            AT Protocol OAuth
          </h1>
          <p className="text-zinc-600 dark:text-zinc-400">
            Sign in with your AT Protocol account
          </p>
        </div>

        <div className="bg-white dark:bg-zinc-900 rounded-lg border border-zinc-200 dark:border-zinc-800 p-6">
          {session ? (
            <div className="space-y-4">
              <div className="flex items-center justify-between">
                <p className="text-sm text-zinc-600 dark:text-zinc-400">
                  Signed in as{" "}
                  <span className="font-mono">{session.did}</span>
                </p>
                <LogoutButton />
              </div>
              <p className="text-green-600">Authentication working!</p>
            </div>
          ) : (
            <LoginForm />
          )}
        </div>
      </main>
    </div>
  );
}
```

At this point, you should be able to test the OAuth flow!

If your app isn't already running from Part 1, start it with:

```bash
pnpm dev
```

Then:
1. Navigate to your app in a browser at [http://127.0.0.1:3000](http://127.0.0.1:3000).
2. Enter your handle
3. Authorize the app
4. You should see "Authentication working!" with your DID:

<Container>
  <Image src={workingauth} alt="" className="w-full max-w-md mx-auto" />
</Container>

Congratulations ‚Äî you've now implemented working OAuth as a baseline, and can move on to more exciting features.

## Part 3: Adding Database Persistence

The in-memory approach used so far works for local development, but for production you'll want a proper database. Let's add SQLite and some other dependencies to manage this:

<Note>
If you're using `pnpm` as a package manager as suggeseted here, you should create a file called `pnpm-workspace.yaml` in the root of your project with the following content to ensure `better-sqlite3` is built correctly in the next step:

```
onlyBuiltDependencies:
  - better-sqlite3
```
</Note>

```bash
pnpm add better-sqlite3 kysely
pnpm add -D @types/better-sqlite3 tsx
```

Next, replace the contents of `next.config.ts` to use `better-sqlite3` server-side:

```typescript
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  serverExternalPackages: ["better-sqlite3"],
};

export default nextConfig;
```

Then, add the actual database connection in `lib/db/index.ts`:

```typescript
import Database from "better-sqlite3";
import { Kysely, SqliteDialect } from "kysely";

const DATABASE_PATH = process.env.DATABASE_PATH || "app.db";

let _db: Kysely<DatabaseSchema> | null = null;

export const getDb = (): Kysely<DatabaseSchema> => {
  if (!_db) {
    const sqlite = new Database(DATABASE_PATH);
    sqlite.pragma("journal_mode = WAL");

    _db = new Kysely<DatabaseSchema>({
      dialect: new SqliteDialect({ database: sqlite }),
    });
  }
  return _db;
};

export interface DatabaseSchema {
  auth_state: AuthStateTable;
  auth_session: AuthSessionTable;
}

interface AuthStateTable {
  key: string;
  value: string;
}

interface AuthSessionTable {
  key: string;
  value: string;
}
```

Next, create the "migrations" that you'll run to populate the database schema. Create `lib/db/migrations.ts`:

```typescript
import { Kysely, Migration, Migrator } from "kysely";
import { getDb } from ".";

const migrations: Record<string, Migration> = {
  "001": {
    async up(db: Kysely<unknown>) {
      await db.schema
        .createTable("auth_state")
        .addColumn("key", "text", (col) => col.primaryKey())
        .addColumn("value", "text", (col) => col.notNull())
        .execute();

      await db.schema
        .createTable("auth_session")
        .addColumn("key", "text", (col) => col.primaryKey())
        .addColumn("value", "text", (col) => col.notNull())
        .execute();
    },
    async down(db: Kysely<unknown>) {
      await db.schema.dropTable("auth_session").execute();
      await db.schema.dropTable("auth_state").execute();
    },
  },
};

export function getMigrator() {
  const db = getDb();
  return new Migrator({
    db,
    provider: {
      getMigrations: async () => migrations,
    },
  });
}
```

Next, create `scripts/migrate.ts` so the migrations can be run from the command line:

```typescript
import { getMigrator } from "@/lib/db/migrations";

async function main() {
  const migrator = getMigrator();
  const { error } = await migrator.migrateToLatest();
  if (error) throw error;
  console.log("Migrations complete.");
}

main();
```

Add those command line scrips to `package.json`:

```json
{
  "scripts": {
    "dev": "pnpm migrate && next dev",
    "build": "next build",
    "start": "pnpm migrate && next start",
    "lint": "eslint",
    // Add this line:
    "migrate": "tsx scripts/migrate.ts",
  }
}
```

And finally, update the `stateStore:` and `sessionStore` code blocks in `lib/auth/client.ts` to use the database. You'll need to import `getDb` at the top too:

```typescript
import { getDb } from "../db";

...

stateStore: {
  async get(key: string) {
    const db = getDb();
    const row = await db
      .selectFrom("auth_state")
      .select("value")
      .where("key", "=", key)
      .executeTakeFirst();
    return row ? JSON.parse(row.value) : undefined;
  },
  async set(key: string, value: NodeSavedState) {
    const db = getDb();
    const valueJson = JSON.stringify(value);
    await db
      .insertInto("auth_state")
      .values({ key, value: valueJson })
      .onConflict((oc) => oc.column("key").doUpdateSet({ value: valueJson }))
      .execute();
  },
  async del(key: string) {
    const db = getDb();
    await db.deleteFrom("auth_state").where("key", "=", key).execute();
  },
},

sessionStore: {
  async get(key: string) {
    const db = getDb();
    const row = await db
      .selectFrom("auth_session")
      .select("value")
      .where("key", "=", key)
      .executeTakeFirst();
    return row ? JSON.parse(row.value) : undefined;
  },
  async set(key: string, value: NodeSavedSession) {
    const db = getDb();
    const valueJson = JSON.stringify(value);
    await db
      .insertInto("auth_session")
      .values({ key, value: valueJson })
      .onConflict((oc) => oc.column("key").doUpdateSet({ value: valueJson }))
      .execute();
  },
  async del(key: string) {
    const db = getDb();
    await db.deleteFrom("auth_session").where("key", "=", key).execute();
  },
}
```

All set! You can Ctrl+C the dev server if it's running, then start it again with:

```bash
pnpm dev
```

You should see "Migrations complete." and an `app.db` file created. Now this app will persist state and sessions across restarts.

## Part 4: Deploying to Production

For production, you need a "confidential client" instead of the loopback client you used. This requires:
- a public URL
- a private key for signing
- public endpoints for client metadata and JWKS

You don't need your own domain name to test the app at a public URL. You can host it on [Railway](https://railway.app/) by following our [deployment guide](https://github.com/bluesky-social/nextjs-oauth-tutorial/blob/main/RAILWAY_DEPLOY.md). First, follow these steps.

Create a `well-known` JWKS endpoint that advertises your client's public key at `app/.well-known/jwks.json/route.ts`:

```typescript
import { NextResponse } from "next/server";
import { JoseKey } from "@atproto/oauth-client-node";

// Serves the public keys for the OAuth client
// Required for confidential clients using private_key_jwt authentication

const PRIVATE_KEY = process.env.PRIVATE_KEY;

export async function GET() {
  if (!PRIVATE_KEY) {
    return NextResponse.json({ keys: [] });
  }

  const key = await JoseKey.fromJWK(JSON.parse(PRIVATE_KEY));
  return NextResponse.json({
    keys: [key.publicJwk],
  });
}
```

Next, update `lib/auth/client.ts` once more to use the confidential client configuration when in production:

```typescript
import {
    JoseKey,
    Keyset,
    NodeOAuthClient,
    buildAtprotoLoopbackClientMetadata,
} from "@atproto/oauth-client-node";
import type {
    NodeSavedSession,
    NodeSavedState,
    OAuthClientMetadataInput,
} from "@atproto/oauth-client-node";
import { getDb } from "../db";

export const SCOPE = "atproto";

let client: NodeOAuthClient | null = null;

const PUBLIC_URL = process.env.PUBLIC_URL;
const PRIVATE_KEY = process.env.PRIVATE_KEY;

function getClientMetadata(): OAuthClientMetadataInput {
    if (PUBLIC_URL) {
        return {
            client_id: `${PUBLIC_URL}/oauth-client-metadata.json`,
            client_name: "OAuth Tutorial",
            client_uri: PUBLIC_URL,
            redirect_uris: [`${PUBLIC_URL}/oauth/callback`],
            grant_types: ["authorization_code", "refresh_token"],
            response_types: ["code"],
            scope: SCOPE,
            token_endpoint_auth_method: "private_key_jwt" as const,
            token_endpoint_auth_signing_alg: "ES256" as const, // must match the alg in scripts/gen-key.ts
            jwks_uri: `${PUBLIC_URL}/.well-known/jwks.json`,
            dpop_bound_access_tokens: true,
        };
    } else {
        return buildAtprotoLoopbackClientMetadata({
            scope: SCOPE,
            redirect_uris: ["http://127.0.0.1:3000/oauth/callback"],
        });
    }
}

async function getKeyset(): Promise<Keyset | undefined> {
    if (PUBLIC_URL && PRIVATE_KEY) {
        return new Keyset([await JoseKey.fromJWK(JSON.parse(PRIVATE_KEY))]);
    } else {
        return undefined;
    }
}

export async function getOAuthClient(): Promise<NodeOAuthClient> {
    if (client) return client;

    client = new NodeOAuthClient({
        clientMetadata: getClientMetadata(),
        keyset: await getKeyset(),
...
```

This will serve a [client metadata document](/specs/oauth#client-id-metadata-document).

Finally, you'll need to generate a private key for signing. Create a script for that in `scripts/gen-key.ts`:

```typescript
import { JoseKey } from "@atproto/oauth-client-node";

async function main() {
  const kid = Date.now().toString();
  const key = await JoseKey.generate(["ES256"], kid);
  console.log(JSON.stringify(key.privateJwk));
};

main();
```

Add this script to `package.json`:

```json
{
  "scripts": {
    // Add this line:
    "gen-key": "tsx scripts/gen-key.ts",
  }
}
```

Run the script as `pnpm gen-key` and save the output. This will be `PRIVATE_KEY`, one of two environment variables needed for your deployment:

```bash
PRIVATE_KEY={"kty":"EC","kid":"...","alg":"ES256",...}
PUBLIC_URL=https://your-app.example.com
```

You can provide these as part of our Railway [deployment guide](https://github.com/bluesky-social/nextjs-oauth-tutorial/blob/main/RAILWAY_DEPLOY.md). Follow those steps, and this app should then work from a public URL just like our [demo instance](https://nextjs-oauth-tutorial.up.railway.app/).

## Conclusion

You now have a complete, production-deployed app with persistent storage. However, it doesn't actually do anything with the AT Protocol yet! As you add features from here, you'll need to request additional permission scopes during the OAuth flow.

By default, the app requests only the `atproto` scope. The `atproto` scope is required and offers basic authentication for an `atproto` identity, but it does not authorize the client to access any privileged information or perform any actions on behalf of the user. Refer to the [Scopes guide](/guides/scopes) for more information.

To change the requested scope, update the SCOPE constant in `lib/auth/client.ts`. It should be a space-delimited string:

```typescript
export const SCOPE = "atproto account:email repo:com.example.record";
```

This constant is used in three places:

- The loopback client metadata (for local development)
- The production client metadata
- The login route's authorize call

By centralizing it in one constant, you only need to change it in one place. Now, when you log in to your app again, you'll be prompted to approve the additional scopes.

Congratulations on completing this tutorial! You now have a solid Next.js foundation for building Atproto apps with OAuth. From here, you can move on to our [**Statusphere Example App Tutorial**](/guides/statusphere-tutorial).

---
atproto/src/app/[locale]/guides/overview/en.mdx
---
export const header = {
  title: 'Protocol Overview',
  description:
    'An introduction to the AT Protocol.',
}

# Protocol Overview

The **Authenticated Transfer Protocol**, aka **Atproto**, is a decentralized protocol for large-scale social web applications. This document will introduce you to the ideas behind the AT Protocol.

## Identity

Users in AT Protocol have permanent decentralized identifiers (DIDs) for their accounts. They also have a configurable domain name, which acts as a human-readable handle. Identities include a reference to the user's current hosting provider and cryptographic keys.

## Data Repositories

User data is exchanged in [signed data repositories](/guides/data-repos). These repositories are collections of records which include posts, comments, likes, follows, etc.

## Network Architecture

The AT Protocol has a federated network architecture, meaning that account data is stored on host servers, as opposed to a peer-to-peer model between end devices. Federation was chosen to ensure the network is convenient to use and reliably available. Repository data is synchronized between servers over standard web technologies ([HTTP](/specs/xrpc) and [WebSockets](/specs/event-stream)).

The three core services in our network are Personal Data Servers (PDS), Relays, and App Views. There are also supporting services such as feed generators and labelers.

The lower-level primitives that can get stacked together differently are the repositories, lexicons, and DIDs. We published an overview of our technical decisions around federation architecture [on our blog](https://bsky.social/about/blog/5-5-2023-federation-architecture).

## Interoperation

A global schemas network called [Lexicon](/specs/lexicon) is used to unify the names and behaviors of the calls across the servers. Servers implement "lexicons" to support featuresets, including the core `com.atproto.*` lexicons for syncing user repositories and the `app.bsky.*` lexicons to provide basic social behaviors.

While the Web exchanges documents, the AT Protocol exchanges schematic and semantic information, enabling the software from different organizations to understand each others' data. This gives atproto clients freedom to produce user interfaces independently of the servers, and removes the need to exchange rendering code (HTML/JS/CSS) while browsing content.

## Achieving Scale

Personal Data Servers are your home in the cloud. They host your data, distribute it, manage your identity, and orchestrate requests to other services to give you your views.

Relays collect data updates from many servers in to a single firehose.

App Views provide aggregated application data for the entire network. They support large-scale metrics (likes, reposts, followers), content discovery (algorithms), and user search.

This separation of roles is intended to provide users with choice between multiple interoperable providers, while also scaling to large network sizes.

## Algorithmic choice

As with Web search engines, users are free to select their aggregators. Feeds, labelers, and search indices can be provided by independent third parties, with requests routed by the PDS, based on client app configuration. Client apps may be tied to specific services, such as App Views or mandatory labelers.

## Account portability

We assume that a Personal Data Server may fail at any time, either by going offline in its entirety, or by ceasing service for specific users. The goal of the AT Protocol is to ensure that a user can migrate their account to a new PDS without the server's involvement.

User data is stored in [signed data repositories](/guides/data-repos) and authenticated by [DIDs](/guides/identity). Signed data repositories are like Git repos but for database records, and DIDs provide a directory of cryptographic keys, similar in some ways to the TLS certificate system. Identities are expected to be secure, reliable, and independent of the user's PDS.

Most DID documents publish two types of public keys: a signing key and rotation keys.

* **Signing key**: Validates the user's data repository. All DIDs include such a key.
* **Rotation keys**: Asserts changes to the DID Document itself. The PLC DID method includes this, while the DID Web method does not.

The signing key is entrusted to the PDS so that it can manage the user's data, but rotation keys can be controlled by the user, e.g. as a paper key. This makes it possible for the user to update their account to a new PDS without the original host's help.

A backup of the user's data could be persistently synced to a user's own device as a backup (contingent on the disk space available), or mirrored by a third-party service. In the event their PDS disappears without notice, the user should be able to migrate to a new provider by updating their DID Document and uploading their data backup.

## Speech, reach, and moderation

AT Protocol's model is that _speech_ and _reach_ should be two separate layers, built to work with each other. The ‚Äúspeech‚Äù layer should remain permissive, distributing authority and designed to ensure everyone has a voice. The ‚Äúreach‚Äù layer lives on top, built for flexibility and designed to scale.

The base layer of atproto (personal data repositories and federated networking) creates a common space for speech where everyone is free to participate, analogous to the Web where anyone can put up a website. The indexing services then enable reach by aggregating content from the network, analogous to a search engine.

## Specifications

Some of the primary specifications comprising the initial version of the AT Protocol are:

- [Authenticated Transfer Protocol](/specs/atp)
- [DIDs](/specs/did) and [Handles](/specs/handle)
- [Repository](/specs/repository) and [Data Model](/specs/data-model)
- [Lexicon](/specs/lexicon)
- [HTTP API (XRPC)](/specs/xrpc) and [Event Streams](/specs/event-stream)


---
atproto/src/app/[locale]/guides/permission-sets/en.mdx
---
import {Container} from "@/components/Container"
import oauth from "./oauth-only.png"
import skyblur from "./skyblur-oauth.png"
import granular from "./skyblur-granular.png"
import lexicon from "./skyblur-lexicon.png"

export const header = {
  title: 'Permission Requests',
  description:
    'A permissions guide for app developers and Lexicon designers',
}

## For app developers

This guide describes how to request permissions for your Atproto application. [OAuth](/guides/auth) is used to authorize third-party applications to access users' social graphs, and is a prerequisite for the concepts described here.

Control over Atproto resources is described and granted using [permissions](/specs/permission). A group of a permissions related to a specific Lexicon namespace (record types and API endpoints) can be bundled together as a "permission set". Both are used in the context of [OAuth](/specs/auth) to grant client software access to account resources on a PDS: For example, the ability to write records of specific types to the user's public repository, or make authenticated API requests to remote services. Developers declare the permissions their app requires to function, and end users are shown the permissions when granting access to the app.

In general, it is best to use existing permission sets for cleaner UI, but you can also request individual permissions and define your own permission sets as needed. Refer to the [OAuth flow examples](#oauth-flow-examples) section for how these requests appear in the OAuth flow.

## Requesting permissions

When your app initiates an OAuth flow with an Atproto identity provider, it can request specific permission sets by including them in the `scope` parameter of the authorization request. This should be supplied as a *space-separated list* in the [`oauth-client-metadata.json` file](/specs/oauth#clients) when registering your OAuth client:

```
{
scope: "atproto rpc:app.bsky.feed.searchPosts?aud=* blob:*/*",
...
}
```

OAuth in AT uses [Pushed Authorization Requests](https://auth0.com/blog/what-are-oauth-push-authorization-requests-par/) (PAR). The client metadata file will be fetched dynamically during the session lifecycle, and the `scope` parameter in `oauth-client-metadata.json` *needs to match* the actual authorization HTTP request made by your app:

```
https://your-authorization-server.com/authorize?
    client_id=1234567890&
    redirect_uri=https://your-app/callback&
    scope=atproto rpc:app.bsky.feed.searchPosts?aud=* blob:*/*
    response_type=code&
    audience=https://myapi&
    state=1234567890
```

You should try to only request enough permissions to cover the features your app needs. [Transition scopes](/specs/oauth#authorization-scopes) appear to grant permissions to create, update, or delete any of a user's AT records, including posts from other applications. In general, these broad permission requests should be avoided. This helps build trust with your users, and minimizes friction during the authorization process.

From a user experience perspective, it's best to keep the number of requested permission sets low. Each additional permission set adds more information to the authorization dialog, which can lead to decision fatigue. It is possible to allow users to grant only a subset of requested permissions, but this can lead to edge cases.

## Permission types

Permissions relate to user owned resources on PDS instances are are represented by five types:

- `repo`: Public Repository (records and collections)
- `rpc`: Service Authentication (API calls to external services)
- `blob`: uploaded media files
- `identity`: DID and handle
- `account`: hosting status, email address

Wildcards (*) are allowed in scope string syntax, and grants access to all records ‚Äî the most common example being `blob:*/*`, for working with images and video. Partial wildcards are *not* supported ‚Äî for example, `app.bsky.*` is not a valid scope string. If you wanted to request multiple permissions from a particular namespace or Lexicon, you would need to list them out individually, or use a [permission set](#for-lexicon-designers). This is intentional, to avoid overbroad permission requests.

Permission sets are themselves Lexicon schemas and are namespaced like other Lexicons. Unlike other Lexicons, permission sets are prefixed with `include:` in the scope string, and are dynamic; they can be updated.

## Rolling out changes

When you update your app to use new permission sets, it's important to coordinate the software release with the permission-set updates. If you add new permission sets that require user approval, users will need to re-authorize your app to grant these new permissions.

Your release and deployment process for the [client metadata document](/specs/oauth#clients) may be separate from your app's code deployment process. Make sure to plan accordingly ‚Äî an updated `oauth-client-metadata.json` that requests new permissions must be live before a version of your app that requires those permissions.

## For Lexicon designers

Lexicons are authored in JSON ‚Äî you can find examples in the [Lexicon Guide](/guides/lexicon). Atmosphere HTTP API methods each scope and implement a particular set of Lexicons, so Lexicons must include valid query parameters, request bodies, and response bodies as appropriate. Each Lexicon, in turn, becomes a permission scope ‚Äî in other words, `app.bsky.feed.post` is an AT record format, a web endpoint, and a granular permission all at once.

Lexicon schemas are published publicly as records in ATProto Lexicon repositories. Refer to [Lexicon Publication and Resolution](/specs/lexicon#lexicon-publication-and-resolution) for technical details.

## Permission Set design

Full-featured client apps require a large number of granular permissions to function: dozens or even hundreds of individual permissions. To simplify permission management, Lexicon designers can define "sets" of permissions as part of the schemas they publish. These permission sets are themselves Lexicon schemas and are referred to by namespace ID, such as `com.example.authBasicFeatures`.

For example, your application could request a permission set like `include:com.example.authBasicFeatures?aud=did:web:api.example.com%23svc_appview`. This would resolve to a Lexicon that defines a set of permissions required for the app to function:

```
{
  "lexicon": 1,
  "id": "com.example.authBasicFeatures",
  "defs": {
    "main": {
      "type": "permission-set",
      "title": "Basic App Functionality",
      "detail": "Creation of posts and interactions",
      "permissions": [
        {
          "type": "permission",
          "resource": "repo",
          "collection": ["app.example.post"]
        },
        {
          "type": "permission",
          "resource": "rpc",
          "inheritAud": true,
          "lxm": [
            "app.example.getFeed"
            "app.example.getProfile",
            ...
          ]
        },
      ]
    }
  }
}
```

Note the `auth` prefix in the `app.example.feed.authOnlyPost` and `com.example.authBasicFeatures` examples. This is a naming convention to indicate that the Lexicon is designed for use as a permission set.

Permission sets are limited to expressing permissions that reference resources under the same NSID namespace as the set itself. For example, the set `app.example.feed.authOnlyPost` could include permissions to `app.example.feed.post` records and making `app.example.feed.getPostThread` API endpoint requests to remote services. But it could not grant permissions to `app.example.actor.profile`. A permission set `app.example.authFull`, which is a level up in the hierarchy, could include permissions to all these resources.

The `title` and `details` fields are descriptive, and will be displayed as part of the OAuth flow; see the [OAuth flow examples](#oauth-flow-examples) section for an example. Internationalization of these descriptive fields is supported ‚Äî refer to [the permission sets spec](/specs/permission#permission-sets).

The use of `inheritAud` in the `rpc` permission allows you to [control inheritance behavior](/specs/permission#rpc).

The `blob:*/*` permission for image/video handling cannot be included in permission sets and must always be requested separately.

## Linting and validating

You can use [`goat`](https://github.com/bluesky-social/goat) to create and publish Lexicons. Before publishing, `goat` can also be used to lint and validate your Lexicon. For example, pulling and linting `app.bsky.feed.post`:

```bash
$ goat lex pull app.bsky.feed.post
 üü¢ app.bsky.feed.post

$ goat lex lint lexicons/app/bsky/feed/post.json
 üü° lexicons/app/bsky/feed/post.json
    [unlimited-string]: no max length
    [unlimited-string]: no max length
error: linting issues detected
```

Nobody's perfect üòä

You can create a new Lexicon record with `goat lex new record`:

```bash
$ goat lex new record dev.project.thing
```

Develop your Lexicon following the [style guide](/guides/lexicon-style-guide), and then eventually publish it with `goat lex publish`:

```bash
$ goat lex publish
 üü¢ dev.project.thing
```

After publishing a new Lexicon, you can verify that it is live with `goat lex resolve`:

```bash
$ goat lex resolve app.bsky.feed.post

{
  "$type": "com.atproto.lexicon.schema",
  "defs": {
    "entity": {
      "description": "Deprecated: use facets instead.",
      "properties": {
        "index": {
          "ref": "#textSlice",
          "type": "ref"
        },
        "type": {
          "description": "Expected values are 'mention' and 'link'.",
          "type": "string"
        },
        "value": {
          "type": "string"
...
```

## Revising Permission Sets

Permission sets can be revised, but removing them or making destructive changes can break existing applications. In general, permission sets should be closely tied to the applications they are designed for, but not necessarily 1:1. Remember that Atproto applications can utilize multiple different Lexicons.

## OAuth flow examples

Apps that are using Atproto identity providers *solely* for authentication [only need to request the `atproto` permission](/guides/scopes). This provides the same OAuth flow that you might expect from other identity providers; no features of the Atmosphere social graph are used, and no additional permissions are required:

<Container>
  <Image src={oauth} alt="" className="w-full max-w-md mx-auto"/>
</Container>

This is a fully usable Atmosphere integration, and many applications will use this as a starting point. Note that it produces a straightforward, readable "Authorize" flow. The URL of the requesting app is clearly visible, and the permissions dialog states that the app "wants to uniquely identify you," but nothing else.

Next, consider a more complex integration, that requests additional permissions. By default, these will be presented in a summarized view:

<Container>
  <Image src={skyblur} alt="" className="w-full max-w-md mx-auto" />
</Container>

In this image, note that there are two different Lexicon groupings: *Bluesky* permissions, and *Skyblur* permissions. Each Lexicon defines its own permission sets, as well as its own grouping and presentation of those permissions.

You can click through on the **?** icons to see the details of each permission set:

<Container>
  <Image src={granular} alt="" className="w-full max-w-md mx-auto" />
</Container>

From the expanded Atproto and Bluesky permissions, you might recognize `atproto` (for OAuth login), `blob:*/*` (for image and video handling), and several other `app.bsky.*` permissions for profile and social graph interactions.

The *Skyblur* Lexicon permissions are defined and summarized differently:

<Container>
  <Image src={lexicon} alt="" className="w-full max-w-md mx-auto" />
</Container>

This is all at the discretion of app developers and Lexicon designers. We provide a model for using different Lexicons side by side, and for each Lexicon to define its own permission sets and presentation. 

## Further Reading and Resources

- [Auth](/guides/auth)
- [SDK authentication](/guides/sdk-auth)
- [OAuth patterns](/guides/oauth-patterns)
- [Scopes](/guides/scopes)
- [OAuth spec](/specs/oauth)
- [Permissions spec](/specs/permission)


---
atproto/src/app/[locale]/guides/publishing-lexicons/en.mdx
---
export const header = {
  title: 'Publishing Lexicons',
  description: 'Sharing your lexicons with the world',
}

## Using goat

Lexicons are designed to be machine-readable and network-accessible. While it is not currently *required* that a lexicon is available on the network, it is strongly advised to publish lexicon so that a single canonical & authoritative representation is available to consumers of the method.

You can use [`goat`](https://github.com/bluesky-social/goat), our command line tool, for creating and publishing Lexicons. The Lexicon-related features are available under [`goat lex`](https://github.com/bluesky-social/goat?tab=readme-ov-file#lexicon-development) :

- publishing schemas to and synchronizing from the AT network
- diffing, linting, and verifying schema evolution rules

`goat` can be installed from [Homebrew](https://formulae.brew.sh/formula/goat) on macOS and Linux:

```shell
brew install goat
```

In a project directory, you can download some existing Lexicons, which will get saved as JSON files in `./lexicons/`:

```markdown
$ goat lex pull com.atproto.repo.strongRef com.atproto.moderation. app.bsky.actor.profile
 üü¢ com.atproto.repo.strongRef
 üü¢ com.atproto.moderation.defs
 üü¢ com.atproto.moderation.createReport
 üü¢ app.bsky.actor.profile
```

You can also create a new Lexicon record with `goat lex new record`:

```markdown
$ goat lex new record dev.project.thing

$ open ./lexicons/dev/project/thing.json
```

And eventually publish it with `goat lex publish`:

```markdown
$ goat lex publish
 üü¢ dev.project.thing
```

Refer to the [Lexicon Style Guide](/guides/lexicon-style-guide) for guidance on creating new Lexicons.

## Further Reading and Resources

- [Lexicons](/guides/lexicon)
- [Lexicon Style Guide](/guides/lexicon-style-guide)
- [Lexicon Specs](/specs/lexicon)

---
atproto/src/app/[locale]/guides/reading-data/en.mdx
---
export const header = {
  title: 'Reading Data',
  description: 'Read AT Protocol Records',
}

## Listing Records

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
    You can read records from a user's data repository with `client.list()`:

    ```tsx
    const posts = await client.list(app.bsky.feed.post, {
      limit: 10,
      repo: 'alex.bsky.team',
    })
    ```

    Each Lexicon will have its own parameters; for `app.bsky.feed.post`, you can specify the `repo` (a user's handle or [DID](#did-resolution)) and a `limit` on the number of posts to return. 
    </TabPanel>
    <TabPanel value="go">
    You can read posts from a user's data repository with `RepoListRecords()`:

    ```go
    resp, _ := comatproto.RepoListRecords(ctx, client,
        "app.bsky.feed.post",  // lexicon
        "",                    // cursor
        10,                    // limit
        "alex.bsky.team",      // handle or DID
        false,                 // reverse
    )

    for _, record := range resp.Records {
        fmt.Printf("Record: %s\n", record.Uri)
    }
    ```
    </TabPanel>
  </TabPanels>
</TabGroup>

Refer to [`app.bsky.feed.post`](https://lexicon.garden/lexicon/did:plc:4v4y5r3lwsbtmsxhile2ljac/app.bsky.feed.post) for this complete Lexicon.

## Getting Records

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
    You can use `client.get()` to read an individual record by its [`rkey`](/specs/record-key):

    ```tsx
    const post = await client.get(app.bsky.feed.post, {
      rkey: '3jxf7z2k3q2',
    })
    ```
    </TabPanel>
    <TabPanel value="go">
    You can use `RepoGetRecord()` to read an individual record by its [`rkey`](/specs/record-key):
    
    ```go
    resp, _ := comatproto.RepoGetRecord(ctx, client, 
        "",                    // cid (empty for latest)
        "app.bsky.feed.post",  // lexicon
        "alex.bsky.team",      // handle or DID
        "3mbl5xdw4yk2r",       // rkey
    )

    fmt.Printf("URI: %s\n", resp.Uri)
    // resp.Value contains the record data
    ```
    </TabPanel>
  </TabPanels>
</TabGroup>


## DID Resolution

In AT, user data is resolved through [DIDs](/guides/sync#data-repositories), also known as [Decentralized Identifiers](https://en.wikipedia.org/wiki/Decentralized_identifier). Most AT Protocol applications use [`did:plc` DIDs](/guides/the-at-stack#did-plc) when registering user accounts.

You can resolve a known username to a DID using the `com.atproto.identity.resolveHandle` endpoint. This means that you can just visit this address in a browser with a `handle=` arugment to look up your DID:

```
https://bsky.social/xrpc/com.atproto.identity.resolveHandle?handle=youraccount.bsky.social
```

```
{
did: "did:plc:3vekukmab2djjeengw2o3wln"
}
```

The Typescript SDK also provides the DID of the currently-authenticated as `client.did`:

```tsx
const your_did = client.did
```

## Unauthenticated Reads

Some API endpoints can be accessed without authentication. You can create an unauthenticated client by passing a public API URL to a new `Client`:

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
    ```tsx
    import { Client } from '@atproto/lex'
    import * as app from './lexicons/app.js'

    const client = new Client('https://public.api.bsky.app')
    ```

    As with an authenticated client, you can `.get()` any public records, like a user's profile:

    ```tsx
    const profile = await client.get(app.bsky.actor.profile)
    ```

    You can make API calls to any Lexicon using the same SDK client pattern. For more examples, refer to [the SDK documentation](https://www.npmjs.com/package/@atproto/lex).
    </TabPanel>
    <TabPanel value="go">
    ```go
    import (
        "context"
        appbsky "github.com/bluesky-social/indigo/api/bsky"
        "github.com/bluesky-social/indigo/xrpc"
    )

    ctx := context.Background()

    client := &xrpc.Client{
        Host: "https://public.api.bsky.app", // public API endpoint
    }
    ```

    As with an authenticated client, you can `.get()` any public records, like a user's profile:

    ```go
    profile, _ := appbsky.ActorGetProfile(ctx, client, "alex.bsky.team")

    fmt.Printf("Handle: %s\n", profile.Handle)
    fmt.Printf("Display Name: %s\n", *profile.DisplayName)
    fmt.Printf("Followers: %d\n", *profile.FollowersCount)
    ```

    You can make API calls to any Lexicon using the same SDK client pattern. For more examples, refer to [the Go SDK libraries](https://pkg.go.dev/github.com/bluesky-social/indigo).
    </TabPanel>
  </TabPanels>
</TabGroup>

Next, you might try [writing](/guides/writing-data) to a user's data repository.

## Further Reading and Resources

- [Reads and Writes](/guides/reads-and-writes)
- [Writing data](/guides/writing-data)
- [Accounts and deletions](/guides/account-lifecycle)
- [Social graph](/guides/social-graph)
- [Record Key spec](/specs/record-key)
- [URI scheme](/specs/at-uri-scheme)

---
atproto/src/app/[locale]/guides/reads-and-writes/en.mdx
---
export const header = {
  title: 'Reads and Writes',
  description: 'Read and Write from the Atmosphere',
}

## AT Records

The AT Protocol distributes user data across different nodes of your network, using the core building blocks of the web. The AT Protocol interconnects applications so that their backends share state, including user accounts and content, in individual data repositories. Many operations you'll perform when working with Atproto apps involve reading and writing these data repositories.

For more context, see [Atproto for distributed systems engineers](/articles/atproto-for-distsys-engineers) and [The ATProto Ethos](/articles/atproto-ethos).

## Prerequisites

Reading and writing AT data repositories requires an authenticated client. You can authenticate with [password authentication](/guides/sdk-auth) or [OAuth](/guides/oauth-patterns). The guides in this section assume you have already set up an authenticated client.

## API Methods

After [authenticating](/guides/sdk-auth#creating-a-client), you'll be able to make other API requests.

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
    To make API requests, you'll need to specify a particular [Lexicon](/guides/lexicon). For example, Bluesky posts are available under the `app.bsky.feed.post` Lexicon.

    Use `lex` to install the `app.bsky.feed.post` and `app.bsky.actor.profile` Lexicons as described in [Installing Lexicons](/guides/installing-lexicons):

    ```bash
    npm install -g @atproto/lex
    lex install app.bsky.feed.post app.bsky.actor.profile
    lex build
    ```

    This will generate TypeScript types for the Lexicons in your project. You can use this `import` statement to access all installed Lexicons:

    ```tsx
    import * as app from './lexicons/app.js'
    ```

    This will give you access to the API methods for your Lexicons. Now you are ready to start [making requests](/guides/reading-data)!
    </TabPanel>
    <TabPanel value="go">
    ```go
    import (
      "context"
      appbsky "github.com/bluesky-social/indigo/api/bsky"
      comatproto "github.com/bluesky-social/indigo/api/atproto"
      lexutil "github.com/bluesky-social/indigo/lex/util"
      "github.com/bluesky-social/indigo/xrpc"
    )

    ctx := context.Background()
    ```
    Import the SDK libraries and go directly to [making requests](/guides/reading-data)!

    <Note>
    Note that you'll need to [blank import](https://www.oreilly.com/library/view/the-go-programming/9780134190570/ebook_split_100.html) the `bsky` and `util` libraries if you aren't using them directly, due to the way Go imports work:

    ```go
    _ "github.com/bluesky-social/indigo/api/bsky"
    _ "github.com/bluesky-social/indigo/lex/util"
    ```
    </Note>

    </TabPanel>
  </TabPanels>
</TabGroup>

## Further Reading and Resources

- [Reading data](/guides/reading-data)
- [Writing data](/guides/writing-data)
- [Accounts and deletions](/guides/account-lifecycle)
- [Social graph](/guides/social-graph)
- [Record Key spec](/specs/record-key)
- [URI scheme](/specs/at-uri-scheme)

---
atproto/src/app/[locale]/guides/scopes/en.mdx
---
import {Container} from "@/components/Container"
import oauthdomain from "./oauth-domain.jpg"

export const header = {
  title: 'Scopes',
  description: 'OAuth permission scopes and sets',
}

## Implementing OAuth Scopes

The OAuth flow requires permission "scopes" which describe the level of access granted with the session. For now, granular permissions have been implemented on the `bsky.social` hosting service, and the [self-hosted PDS distribution](/guides/self-hosting). Work on permission sets (including lexicon resolution) is in progress. Developers may start to implement more granular permissions -- for example, apps only asking for permissions to specific record types or API endpoints. We expect most apps to use the higher-level "permission sets" when they become available, which will provide more accessible language about the permissions being granted.

The minimal set of OAuth permissions that a third-party AT application should request to sign in with Bluesky are:
- `atproto`

If you are making use of Bluesky profile information, you may also need:
- `rpc:app.bsky.actor.getProfile?aud=did:web:api.bsky.app#bsky_appview`

If you need to link any uploaded images or videos to the records (posts) you create, you will also need:
- `blob:*/*`

The ergonomics of OAuth scopes in Atproto are still evolving. This minimal set is designed to provide a known-good starting point for app developers that does not grant unnecessary permissions. While we finalize granular permission scopes, we are also supporting a set of temporary coarse-grained scopes called the "transition scopes". There are currently no plans to deprecate either the transition scopes or the use of App Passwords. For an overview of these transition scopes, and more in-depth information on OAuth, see [Authorization Scopes](/specs/oauth#authorization-scopes). For the granular permission scopes, see [Permissions](/specs/permission).

## Clients

OAuth client software is identified by a globally unique `client_id`. Distinct variants of client software may have distinct `client_id` values; for example the browser app and Android (mobile OS) variants of the same software might have different `client_id` values. The `client_id` must be a fully-qualified web URL from which the client-metadata JSON document can be fetched. For example, `https://app.example.com/oauth-client-metadata.json`. Some more about the `client_id`:

- it must be a well-formed URL, following the W3C URL specification
- the schema must be `https://`, and there must not be a port number included. Note that there is a [special exception](/specs/oauth#localhost-client-development) for `http://localhost` `client_id` values for development.
- the path does not need to include `oauth-client-metadata.json`, but it is helpful convention.

<Note>
If you have been using `client-metadata.json` rather than `oauth-client-metadata.json`, you can make this change to have your domain display on the auth flow page, rather than a URL string.
</Note>

<Container>
  <Image src={oauthdomain} alt="" className="w-full max-w-md mx-auto" />
</Container>

The auth flow page will display all the relevent permissions being granted. This is true both for apps using [granular permissions](/specs/permission), and for those using the [transitional](/specs/oauth#authorization-scopes) OAuth scopes.

Client Apps should be sure to check which auth scopes were actually approved during the auth flow. In particular, when requesting permission to read an account email address (`account:email`), the user might decline that permission while granting others. In theory, any individual permission might be denied while the overall request is granted.

## Further Reading and Resources

- [Auth](/guides/auth)
- [SDK authentication](/guides/sdk-auth)
- [OAuth patterns](/guides/oauth-patterns)
- [Permission requests](/guides/permission-sets)
- [OAuth spec](/specs/oauth)
- [Permissions spec](/specs/permission)

---
atproto/src/app/[locale]/guides/sdk-auth/en.mdx
---
import {Container} from "@/components/Container"

export const header = {
  title: 'SDK authentication',
  description: 'Authenticating with the SDKs',
}

## Password Authentication

If you are using password auth, you should generate an [app password](/specs/xrpc#app-passwords) for your account rather than using your main account password. If you are using Bluesky, you can generate an app password in your [account settings](https://bsky.app/settings/app-passwords).

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
    Start by importing `PasswordSession`, providing credentials to `PasswordSession.login()`, and creating a `session`:

    ```tsx
    import { PasswordSession } from '@atproto/lex-password-session'

    const result = await PasswordSession.login({
      service: 'https://bsky.social', // or your PDS host
      identifier: 'your-handle.bsky.social',
      password: 'your-app-password',
    })

    if (result.success) {
        const session = result.value
    }
    ```

    Applications with an end user login flow should use [OAuth](/guides/auth) authentication rather than app password sessions. Password auth is acceptable for bots and command line tools. Both methods will produce the same authenticated `session`.
    </TabPanel>
    <TabPanel value="go">
    Start by importing the `atproto` and `xrpc` libraries, creating an `&xrpc.Client{}`, and providing credentials to call `ServerCreateSession()`:

    ```go
    import (
      "context"
      comatproto "github.com/bluesky-social/indigo/api/atproto"
      "github.com/bluesky-social/indigo/xrpc"
    )

    func main() {
      ctx := context.Background()

      client := &xrpc.Client{
        Host: "https://bsky.social", // or your PDS host
      }

      session, _ := comatproto.ServerCreateSession(ctx, client, &comatproto.ServerCreateSession_Input{
        Identifier: "your-handle.bsky.social",
        Password:   "your-app-password",
      })
    }
    ```
    </TabPanel>
  </TabPanels>
</TabGroup>

## Client Sessions

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
    Once you have a logged-in `session`, you can create a `Client` instance to read and write data:

    ```tsx
    import { Client } from '@atproto/lex'
    const client = new Client(session)

    // Make authenticated API calls
    console.log('Logged in as:', session.did)
    ```
    </TabPanel>
    <TabPanel value="go">
    Once you have an authed `client`, you can store auth info and make API calls:

    ```go
    // Store auth info on the client for subsequent requests
    client.Auth = &xrpc.AuthInfo{
      AccessJwt:  session.AccessJwt,
      RefreshJwt: session.RefreshJwt,
      Handle:     session.Handle,
      Did:        session.Did,
    }

    fmt.Printf("Logged in as: %s (%s)\n", session.Handle, session.Did)
    ```
    </TabPanel>
  </TabPanels>
</TabGroup>

From here, you can go directly to [making API requests](/guides/reads-and-writes).

## API Tokens

Rate limits for logging in are generally lower than for other API operations. Calling the login method multiple times in a short period may trigger rate limits. You only need to call the login method once per session to authenticate.

The `com.atproto.server.createSession` API endpoint called by login methods returns a session object containing two API tokens:
* `accessJwt`: an access token which is used to authenticate requests but expires after a few minutes
* `refreshJwt`: a refresh token which lasts longer and is used only to update the session with a new access token

## Further Reading and Resources

- [Auth](/guides/auth)
- [OAuth patterns](/guides/oauth-patterns)
- [Scopes](/guides/scopes)
- [Permission requests](/guides/permission-sets)
- [OAuth spec](/specs/oauth)
- [Permissions spec](/specs/permission)

---
atproto/src/app/[locale]/guides/self-hosting/en.mdx
---
export const header = {
  title: 'Self-hosting',
  description:
    'Resources for self-hosting various components of the Atmosphere stack',
}

Most parts of the Atmosphere stack have supported deployments. Some components are easier to self-host than others, depending on whether you are providing application-level infrastructure (like an AppView or Relay) or data-level infrastructure (like a PDS).

## PDS

Resources for self-hosting a PDS, including install steps, Docker images and Frequently Asked Questions, are available from the [PDS](https://github.com/bluesky-social/pds) Github repo. Start here if you want to host your own PDS!

The PDS Docker image includes [goat](https://github.com/bluesky-social/goat), our command line tool for performing admin functions.

If you are moving your account from one PDS (such as a Bluesky-hosted PDS) to another, refer to [Account Migration](/guides/account-migration).

If you need to change the *domain name* of a self-hosted PDS, you need to:

- Change the PLC entries for *each* of your hosted accounts. You can use community-maintained interfaces like [boat](https://boat.kelinci.net/plc-applicator) for this.
- Update your DNS configuration as you would for any other server, then change the domain name in `pds.env`.

For guidance on scaling and hardening a PDS, refer to [Going to Production](/guides/going-to-production).

Bluesky runs many PDSs with an ‚Äúentryway‚Äù in front of them. For information on this implementation, refer to [Entryway](https://docs.bsky.app/docs/advanced-guides/entryway).

There are also several community PDS implementations! For example:
- [Tranquil PDS](https://tangled.org/tranquil.farm/tranquil-pds), written in Rust.
- [Cocoon](https://tangled.org/hailey.at/cocoon), written in Go.

## Tap

Tap is used for syncing existing records from the AT network and then continuing to receive new records from the firehose. For a guide to deploying Tap, refer to the [Tap readme](https://github.com/bluesky-social/indigo/blob/main/cmd/tap/README.md#distribution--deployment).

We also provide a quick start guide for [deploying Tap](https://github.com/bluesky-social/indigo/blob/main/cmd/tap/RAILWAY_DEPLOY.md) to [Railway](https://railway.com/).

## AppViews or Relays

Hosting your own AppView or Relay is possible, but resource intensive. Compared with hosting your own PDS, which is a way of personally controlling access to your own data repository, hosting an AppView or Relay is something that you would only do to provide application-level infrastructure. Generally, the first thing you would need to do here is replicate all of the data in the network for the Lexicons that your application handles. Refer to the [Sync](/guides/sync) docs for a starting point.

There are lots of other decisions that will go into designing and running your own Atmosphere apps! This will not be a comprehensive list of recommendations.

- For your app's signup flow, you will need users to log in or create an account. We generally recommend hosting your own PDS alongside your app to allow first-time users of the Atmosphere ‚Äî who may not yet be aware of the concept of a PDS ‚Äî to sign up directly within your application. Your PDS and AppView should have [different domain names](/going-to-production#domain-names).

## Ozone

Refer to [Using Ozone](/guides/using-ozone) for self-hosting guidance.

## Deploy Recipes

For community-contributed deployment solutions that do not use our official tooling, and may support different environments, refer to the [deploy-recipes](https://github.com/bluesky-social/deploy-recipes) Github repo. Contributions are welcome!


---
atproto/src/app/[locale]/guides/social-graph/en.mdx
---
export const header = {
  title: 'Social graph',
  description: 'Working with AT Protocol Social Graphs',
}

## Querying social graphs

New applications that build on the AT Protocol can leverage existing social graph data ‚Äî for example, from the `app.bsky.graph.getFollows` Lexicon RPC endpoint ‚Äî to build new use cases, and bootstrap with an existing user base. This, in turn, gets you to native ATProtocol concepts like [A Social Filesystem](https://overreacted.io/a-social-filesystem/).

See [Exploring AT Protocol with Python](https://davidgasquez.com/exploring-atproto-python/) for another example of this approach.

## Backlinks

One of the most powerful ways to get social graph data from AT Protocol is through the use of **backlinks**. Querying backlinks lets you resolve all of the records linking *to* any given record, *or* identity.

[Microcosm](https://constellation.microcosm.blue) provides a set of resources for working with backlinks. Because all AT Protocol records are openly available and addressible via an `at://` URI, this makes it possible to encode nested API queries into individual URLs. Microcosm [does this](https://constellation.microcosm.blue/xrpc/blue.microcosm.links.getBacklinks?subject=at%3A%2F%2Fdid%3Aplc%3Aa4pqq234yw7fqbddawjo7y35%2Fapp.bsky.feed.post%2F3m237ilwc372e&source=app.bsky.feed.like%3Asubject.uri&limit=16), for example.

## Further Reading and Resources

- [Reads and Writes](/guides/reads-and-writes)
- [Reading data](/guides/reading-data)
- [Writing data](/guides/writing-data)
- [Accounts and deletions](/guides/account-lifecycle)
- [Record Key spec](/specs/record-key)
- [URI scheme](/specs/at-uri-scheme)

---
atproto/src/app/[locale]/guides/statusphere-tutorial/en.mdx
---
import {Container} from "@/components/Container"
import setstatus from "./set-status.png"
import tap from "./tap-status.png"
import feed from "./feed-status.png"
import taproot from "./taproot-status.png"

export const header = {
  title: 'Create a Social App',
  description: 'Build an app that lets you broadcast and receive status updates using AT Record Lexicons.',
}

In this tutorial, you'll build a status-setting app using custom Lexicons and real-time sync.

You can find the source code for this tutorial in the [statusphere-example-app](https://github.com/bluesky-social/statusphere-example-app) repository.

## Prerequisites

This tutorial builds on top of the app created in our [OAuth with NextJS Tutorial](/guides/oauth-tutorial). You should have a completed NextJS OAuth app before starting this tutorial. If you want to skip that step, you can clone the completed OAuth app from the [Cookbook](https://github.com/bluesky-social/nextjs-oauth-tutorial/tree/129cf28b4146ae709e0dac43ce20154ab5237559).

You should have installed:
- Node.js 18+
- Go 1.25+
- The pnpm package manager

On platforms supported by [homebrew](https://brew.sh/), you can install them with:

```bash
brew install node pnpm go
```

Begin by adding these Atproto libraries to your existing project:

```bash
pnpm add @atproto/common-web @atproto/lex @atproto/syntax @atproto/tap
```

You should also have our `lex` CLI tool installed globally:

```bash
npm install -g @atproto/lex
```

You'll begin by adding Lexicons to your project.

## Part 1: Lexicons

[Lexicons](/guides/lexicon) define the schema for records in Atproto. To create this tutorial, we've already published a new "Statusphere" Lexicon to our repository, available at `xyz.statusphere.status`. You can use `lex` to download and build this Lexicon into your project:

```bash
lex install xyz.statusphere.status
```

If you want, you can view the downloaded Lexicon file at `lib/lexicons/xyz.statusphere.status.json`. From here, you can run `lex build` to generate TypeScript types for all of your installed Lexicons:

```bash
lex build --importExt=\"\"
```

<Note>
You could add this command to `package.json` if you needed, but we won't be using it again in this tutorial now that our Lexicons have been built. If you ever add another Lexicon to a project with `lex install`, you'll need to re-run `lex build` with the `--override` flag to regenerate the types.
</Note>

One more thing you should do now is update the `SCOPE` constant in `lib/auth/client.ts` to request access to this collection. Atproto permission scopes are Lexicons, so you can add `xyz.statusphere.status` to the list of scopes your app requests:

```typescript
export const SCOPE = "atproto repo:xyz.statusphere.status";
```

Now you can move on to creating the database schema.

## Part 2: Database Schema

Update `lib/db/index.ts` to add new tables:

```typescript
export interface DatabaseSchema {
  auth_state: AuthStateTable;
  auth_session: AuthSessionTable;
  account: AccountTable;   // New
  status: StatusTable;     // New
}

// ... existing auth tables ...

export interface AccountTable {
  did: string;
  handle: string;
  active: 0 | 1;
}

export interface StatusTable {
  uri: string;
  authorDid: string;
  status: string;
  createdAt: string;
  indexedAt: string;
  current: 0 | 1;
}
```

Next, add the migrations script that creates these tables. If you're building on top of the OAuth tutorial and have already deployed/migrated your database, you'll want to put these in a new migration, e.g. called "002", in `lib/db/migrations.ts`:

```typescript
const migrations: Record<string, Migration> = {
  "001": {} // existing migrations for auth tables
  "002": {
    async up(db: Kysely<unknown>) {
      await db.schema
        .createTable("account")
        .addColumn("did", "text", (col) => col.primaryKey())
        .addColumn("handle", "text", (col) => col.notNull())
        .addColumn("active", "integer", (col) => col.notNull().defaultTo(1))
        .execute();

      await db.schema
        .createTable("status")
        .addColumn("uri", "text", (col) => col.primaryKey())
        .addColumn("authorDid", "text", (col) => col.notNull())
        .addColumn("status", "text", (col) => col.notNull())
        .addColumn("createdAt", "text", (col) => col.notNull())
        .addColumn("indexedAt", "text", (col) => col.notNull())
        .addColumn("current", "integer", (col) => col.notNull().defaultTo(0))
        .execute();

      await db.schema
        .createIndex("status_current_idx")
        .on("status")
        .columns(["current", "indexedAt"])
        .execute();
    },
    async down(db: Kysely<unknown>) {
      await db.schema.dropTable("status").execute();
      await db.schema.dropTable("account").execute();
      await db.schema.dropTable("auth_session").execute();
      await db.schema.dropTable("auth_state").execute();
    },
  },
};
```

You can run `pnpm migrate` to apply these new migrations to your database. Now, to actually build the app logic!

## Part 3: Status Submission

First you'll build the feature that allows users to write their status to their [PDS](/guides/the-at-stack#pds). Create a new API route at `app/api/status/route.ts`:

```typescript
import { NextRequest, NextResponse } from "next/server";
import { Client } from "@atproto/lex";
import { getSession } from "@/lib/auth/session";
import { getOAuthClient } from "@/lib/auth/client";
import * as xyz from "@/src/lexicons/xyz";

export async function POST(request: NextRequest) {
  const session = await getSession();
  if (!session) {
    return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
  }

  const { status } = await request.json();

  if (!status || typeof status !== "string") {
    return NextResponse.json({ error: "Status is required" }, { status: 400 });
  }

  const client = await getOAuthClient();
  const oauthSession = await client.restore(session.did);
  const lexClient = new Client(oauthSession);

  const createdAt = new Date().toISOString();
  const res = await lexClient.create(xyz.statusphere.status, {
    status,
    createdAt,
  });

  return NextResponse.json({
    success: true,
    uri: res.uri,
  });
}
```

This verifies that a user is logged in, creates a lex `Client` with their OAuth session, and then creates a new `xyz.statusphere.status` record with the submitted status text.

Next, create a Status picker component at `components/StatusPicker.tsx`:

```tsx
"use client";

import { useState } from "react";
import { useRouter } from "next/navigation";

const EMOJIS = ["üëç", "üëé", "üíô", "üî•", "üòÜ", "üò¢", "ü§î", "üò¥", "üéâ", "ü§©", "üò≠", "ü•≥", "üò§", "üíÄ", "‚ú®", "üëÄ", "üôè", "üìö", "üíª", "üçï", "üå¥"];

interface StatusPickerProps {
  currentStatus?: string | null;
}

export function StatusPicker({ currentStatus }: StatusPickerProps) {
  const router = useRouter();
  const [selected, setSelected] = useState<string | null>(currentStatus ?? null);
  const [loading, setLoading] = useState(false);

  async function handleSelect(emoji: string) {
    setLoading(true);
    setSelected(emoji);

    try {
      const res = await fetch("/api/status", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ status: emoji }),
      });

      if (!res.ok) {
        throw new Error("Failed to update status");
      }

      router.refresh();
    } catch (err) {
      console.error("Failed to update status:", err);
      setSelected(currentStatus ?? null);
    } finally {
      setLoading(false);
    }
  }

  return (
    <div>
      <p className="text-sm text-zinc-500 dark:text-zinc-400 mb-3">
        Set your status
      </p>
      <div className="flex flex-wrap gap-2">
        {EMOJIS.map((emoji) => (
          <button
            key={emoji}
            onClick={() => handleSelect(emoji)}
            disabled={loading}
            className={`text-2xl p-2 rounded-lg transition-all
              ${selected === emoji
                ? "bg-blue-100 dark:bg-blue-900 ring-2 ring-blue-500"
                : "hover:bg-zinc-100 dark:hover:bg-zinc-800"
              }
              disabled:opacity-50 disabled:cursor-not-allowed`}
          >
            {emoji}
          </button>
        ))}
      </div>
    </div>
  );
}
```

Finally, update `app/page.tsx` to include the status picker:

```tsx
import { getSession } from "@/lib/auth/session";
import { LoginForm } from "@/components/LoginForm";
import { LogoutButton } from "@/components/LogoutButton";
import { StatusPicker } from "@/components/StatusPicker";

export default async function Home() {
  const session = await getSession();

  return (
    <div className="flex min-h-screen items-center justify-center bg-zinc-50 dark:bg-zinc-950">
      <main className="w-full max-w-md mx-auto p-8">
        <div className="text-center mb-8">
          <h1 className="text-3xl font-bold text-zinc-900 dark:text-zinc-100 mb-2">
            Statusphere
          </h1>
          <p className="text-zinc-600 dark:text-zinc-400">
            Set your status on the Atmosphere
          </p>
        </div>

        <div className="bg-white dark:bg-zinc-900 rounded-lg border border-zinc-200 dark:border-zinc-800 p-6">
          {session ? (
            <div className="space-y-4">
              <div className="flex items-center justify-between mb-4">
                <p className="text-sm text-zinc-500 dark:text-zinc-400">
                  Signed in
                </p>
                <LogoutButton />
              </div>
              <StatusPicker />
            </div>
          ) : (
            <LoginForm />
          )}
        </div>
      </main>
    </div>
  );
}
```

Run the app with `pnpm dev` ‚Äî you'll be able to log in, and select an emoji to set your status!

<Container>
  <Image src={setstatus} alt="" className="w-full max-w-md mx-auto" />
</Container>

You won't yet have a way to see your status in this app, but you can look up your account on [atproto.at](https://atproto.at/) and check the `xyz.statusphere.status` Lexicon to see this record.

<Container>
  <Image src={taproot} alt="" className="w-full max-w-md mx-auto" />
</Container>

Next, you'll add sync features to actually receive status updates.

## Part 4: Realtime Sync with Tap

[Tap](/guides/backfilling#using-tap) is the best way to synchronize and stream AT Protocol records. You'll use the Tap [TypeScript library](https://www.npmjs.com/package/@atproto/tap) to sync new records in your database. Create `lib/tap/index.ts`:

```typescript
import { Tap } from "@atproto/tap";

const TAP_URL = process.env.TAP_URL || "http://localhost:2480";

let _tap: Tap | null = null;

export const getTap = (): Tap => {
  if (!_tap) {
    _tap = new Tap(TAP_URL);
  }
  return _tap;
};
```

Next, create `lib/db/queries.ts` with some database queries for handling Tap events:

```typescript
import { getDb, AccountTable, StatusTable, DatabaseSchema } from ".";
import { AtUri } from "@atproto/syntax";
import { Transaction } from "kysely";

export async function getAccountStatus(did: string) {
  const db = getDb();
  const status = await db
    .selectFrom("status")
    .selectAll()
    .where("authorDid", "=", did)
    .orderBy("createdAt", "desc")
    .limit(1)
    .executeTakeFirst();
  return status ?? null;
}

export async function insertStatus(data: StatusTable) {
  getDb()
    .transaction()
    .execute(async (tx) => {
      await tx
        .insertInto("status")
        .values(data)
        .onConflict((oc) =>
          oc.column("uri").doUpdateSet({
            status: data.status,
            createdAt: data.createdAt,
            indexedAt: data.indexedAt,
          }),
        )
        .execute();
      setCurrStatus(tx, data.authorDid);
    });
}

export async function deleteStatus(uri: AtUri) {
  await getDb()
    .transaction()
    .execute(async (tx) => {
      await tx.deleteFrom("status").where("uri", "=", uri.toString()).execute();
      await setCurrStatus(tx, uri.hostname);
    });
}

export async function upsertAccount(data: AccountTable) {
  await getDb()
    .insertInto("account")
    .values(data)
    .onConflict((oc) =>
      oc.column("did").doUpdateSet({
        handle: data.handle,
        active: data.active,
      }),
    )
    .execute();
}

export async function deleteAccount(did: string) {
  await getDb().deleteFrom("account").where("did", "=", did).execute();
  await getDb().deleteFrom("status").where("authorDid", "=", did).execute();
}

// Helper to update which status is "current" for a user (inside a transaction)
async function setCurrStatus(tx: Transaction<DatabaseSchema>, did: string) {
  // Clear current flag for all user's statuses
  await tx
    .updateTable("status")
    .set({ current: 0 })
    .where("authorDid", "=", did)
    .where("current", "=", 1)
    .execute();
  // Set the most recent status as current
  await tx
    .updateTable("status")
    .set({ current: 1 })
    .where("uri", "=", (qb) =>
      qb
        .selectFrom("status")
        .select("uri")
        .where("authorDid", "=", did)
        .orderBy("createdAt", "desc")
        .limit(1),
    )
    .execute();
}
```

The `setCurrStatus` helper ensures only the most recent status per user has `current = 1`. This will be used later on.

Tap will deliver events through a webhook endpoint at `app/api/webhook/route.ts`:

```typescript
import { NextRequest, NextResponse } from "next/server";
import { parseTapEvent, assureAdminAuth } from "@atproto/tap";
import { AtUri } from "@atproto/syntax";
import {
  upsertAccount,
  insertStatus,
  deleteStatus,
  deleteAccount,
} from "@/lib/db/queries";
import * as xyz from "@/src/lexicons/xyz";

const TAP_ADMIN_PASSWORD = process.env.TAP_ADMIN_PASSWORD;

export async function POST(request: NextRequest) {
  // Verify request is from our TAP server
  if (TAP_ADMIN_PASSWORD) {
    const authHeader = request.headers.get("Authorization");
    if (!authHeader) {
      return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
    }
    try {
      assureAdminAuth(TAP_ADMIN_PASSWORD, authHeader);
    } catch {
      return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
    }
  }

  const body = await request.json();
  const evt = parseTapEvent(body);

  // Handle account/identity changes
  if (evt.type === "identity") {
    if (evt.status === "deleted") {
      await deleteAccount(evt.did);
    } else {
      await upsertAccount({
        did: evt.did,
        handle: evt.handle,
        active: evt.isActive ? 1 : 0,
      });
    }
  }

  // Handle status record changes
  if (evt.type === "record") {
    const uri = AtUri.make(evt.did, evt.collection, evt.rkey);

    if (evt.action === "create" || evt.action === "update") {
      let record: xyz.statusphere.status.Main;
      try {
        record = xyz.statusphere.status.$parse(evt.record);
      } catch {
        return NextResponse.json({ success: false });
      }

      await insertStatus({
        uri: uri.toString(),
        authorDid: evt.did,
        status: record.status,
        createdAt: record.createdAt,
        indexedAt: new Date().toISOString(),
        current: 1,
      });
    } else if (evt.action === "delete") {
      await deleteStatus(uri);
    }
  }

  return NextResponse.json({ success: true });
}
```

Tap sends webhook events when records change anywhere on the network. Requests are validated using a shared secret; for `identity` events, you update the account cache, and for `record` events, you check if the record matches your Lexicon and insert/update/delete it in your database.

Update `app/page.tsx` again to fetch and pass the current user's status:

```tsx
import { getAccountStatus } from "@/lib/db/queries";

// In Home function:
const accountStatus = session ? await getAccountStatus(session.did) : null;

// Add to the StatusPicker element:
<StatusPicker currentStatus={accountStatus?.status} />
```

This way, the `StatusPicker` will highlight the user's current status when the page loads.

Now, in another terminal, install and run `tap` from its source repository:

```bash
go install github.com/bluesky-social/indigo/cmd/tap
tap run --webhook-url=http://localhost:3000/api/webhook --collection-filters=xyz.statusphere.status
```

Ctrl+C your running app. Then add your data repository to Tap for tracking (you'll add other data sources later):

```bash
# Replace with your own DID below
curl -H 'Content-Type: application/json' -d '{"dids":["DID"]}' http://localhost:2480/repos/add
```

Restart your app with `pnpm dev`, complete the login flow, and you should find your saved status highlighted:

<Container>
  <Image src={tap} alt="" className="w-full max-w-md mx-auto" />
</Container>

## Part 5: Displaying Handles and Feeds

Finally, you'll add functionality to display user handles and a feed of statuses from all users.

First, add some additional imports to `lib/db/queries.ts`:

```typescript
import { getHandle } from "@atproto/common-web";
import { getTap } from "@/lib/tap";
```

Then, add additional functions to fetch account handles and statuses:

```typescript
export async function getAccountHandle(did: string): Promise<string | null> {
  const db = getDb();
  // if we've tracked to the account through Tap and gotten their account info, we'll load from there
  const account = await db
    .selectFrom("account")
    .select("handle")
    .where("did", "=", did)
    .executeTakeFirst();
  if (account) return account.handle;
  // otherwise we'll resolve the accounts DID through Tap which provides identity caching
  try {
    const didDoc = await getTap().resolveDid(did);
    if (!didDoc) return null;
    return getHandle(didDoc) ?? null;
  } catch {
    return null;
  }
}

export async function getRecentStatuses(limit = 5) {
  const db = getDb();
  return db
    .selectFrom("status")
    .innerJoin("account", "status.authorDid", "account.did")
    .selectAll()
    .orderBy("createdAt", "desc")
    .limit(limit)
    .execute();
}

export async function getTopStatuses(limit = 10) {
  const db = getDb();
  return db
    .selectFrom("status")
    .select(["status", db.fn.count("uri").as("count")])
    .where("current", "=", 1)
    .groupBy("status")
    .orderBy("count", "desc")
    .limit(limit)
    .execute();
}
```

Next, update `app/page.tsx` to display the full feed with top statuses, handles, and timestamps:

```tsx
import { getSession } from "@/lib/auth/session";
import {
  getAccountStatus,
  getRecentStatuses,
  getTopStatuses,
  getAccountHandle,
} from "@/lib/db/queries";
import { LoginForm } from "@/components/LoginForm";
import { LogoutButton } from "@/components/LogoutButton";
import { StatusPicker } from "@/components/StatusPicker";

export default async function Home() {
  const session = await getSession();
  const [statuses, topStatuses, accountStatus, accountHandle] =
    await Promise.all([
      getRecentStatuses(),
      getTopStatuses(),
      session ? getAccountStatus(session.did) : null,
      session ? getAccountHandle(session.did) : null,
    ]);

  return (
    <div className="flex min-h-screen items-center justify-center bg-zinc-50 dark:bg-zinc-950">
      <main className="w-full max-w-md mx-auto p-8">
        <div className="text-center mb-8">
          <h1 className="text-3xl font-bold text-zinc-900 dark:text-zinc-100 mb-2">
            Statusphere
          </h1>
          <p className="text-zinc-600 dark:text-zinc-400">
            Set your status on the Atmosphere
          </p>
        </div>

        {session ? (
          <div className="bg-white dark:bg-zinc-900 rounded-lg border border-zinc-200 dark:border-zinc-800 p-6 mb-6">
            <div className="flex items-center justify-between mb-4">
              <p className="text-sm text-zinc-500 dark:text-zinc-400">
                Signed in as @{accountHandle ?? session.did}
              </p>
              <LogoutButton />
            </div>
            <StatusPicker currentStatus={accountStatus?.status} />
          </div>
        ) : (
          <div className="bg-white dark:bg-zinc-900 rounded-lg border border-zinc-200 dark:border-zinc-800 p-6 mb-6">
            <LoginForm />
          </div>
        )}

        {topStatuses.length > 0 && (
          <div className="bg-white dark:bg-zinc-900 rounded-lg border border-zinc-200 dark:border-zinc-800 p-6 mb-6">
            <h3 className="text-sm font-medium text-zinc-500 dark:text-zinc-400 mb-3">
              Top Statuses
            </h3>
            <div className="flex flex-wrap gap-2">
              {topStatuses.map((s) => (
                <span
                  key={s.status}
                  className="inline-flex items-center gap-1 px-3 py-1 rounded-full bg-zinc-100 dark:bg-zinc-800 text-sm"
                >
                  <span className="text-lg">{s.status}</span>
                  <span className="text-zinc-500 dark:text-zinc-400">
                    {String(s.count)}
                  </span>
                </span>
              ))}
            </div>
          </div>
        )}

        <div className="bg-white dark:bg-zinc-900 rounded-lg border border-zinc-200 dark:border-zinc-800 p-6">
          <h3 className="text-sm font-medium text-zinc-500 dark:text-zinc-400 mb-3">
            Recent
          </h3>
          {statuses.length === 0 ? (
            <p className="text-zinc-500 dark:text-zinc-400 text-sm">
              No statuses yet. Be the first!
            </p>
          ) : (
            <ul className="space-y-3">
              {statuses.map((s) => (
                <li key={s.uri} className="flex items-center gap-3">
                  <span className="text-2xl">{s.status}</span>
                  <span className="text-zinc-600 dark:text-zinc-400 text-sm">
                    @{s.handle}
                  </span>
                  <span className="text-zinc-400 dark:text-zinc-500 text-xs ml-auto">
                    {timeAgo(s.createdAt)}
                  </span>
                </li>
              ))}
            </ul>
          )}
        </div>
      </main>
    </div>
  );
}

function timeAgo(dateString: string): string {
  const now = Date.now();
  const then = new Date(dateString).getTime();
  const seconds = Math.floor((now - then) / 1000);
  if (seconds < 60) return "just now";
  const minutes = Math.floor(seconds / 60);
  if (minutes < 60) return `${minutes}m`;
  const hours = Math.floor(minutes / 60);
  if (hours < 24) return `${hours}h`;
  const days = Math.floor(hours / 24);
  return `${days}d`;
}
```

Now logged-in users will see "Signed in as @theirhandle" instead of just "Signed in", along with a feed of recent statuses from all users, and a "Top Statuses" section showing the most popular current statuses.

You can stop and restart your app with `pnpm dev` again.

One last thing ‚Äî remember, you only added a single data source to `tap` earlier. To see statuses from across the entire network, stop and re-run `tap` with the `--signal-collection` flag:

```bash
tap run \
  --webhook-url=http://localhost:3000/api/webhook \
  --collection-filters=xyz.statusphere.status \
  --signal-collection=xyz.statusphere.status
```

... and you'll start seeing statuses from other users on the network!

<Container>
  <Image src={feed} alt="" className="w-full max-w-md mx-auto" />
</Container>

## Conclusion

You've now built a complete Atproto app that uses OAuth to sign in, a custom Lexicon to define status records, and Tap to synchronize records in real-time.

This gives you all the tools you need to continue building on AT Protocol. You can find more guides and tutorials in our [Guides](/guides) section, and more example apps in the [Cookbook](https://github.com/bluesky-social/cookbook/) repository. Happy building!


---
atproto/src/app/[locale]/guides/streaming-data/en.mdx
---
export const header = {
  title: 'Streaming Data',
  description: 'Working with the Firehose and Jetstream',
}

## Streaming data

One of the core primitives of the AT Protocol is the *firehose*. It is an authenticated stream of events used to efficiently sync user updates (posts, likes, follows, handle changes, etc).

Many AT applications that need to stream incoming data will utilize the firehose ‚Äî from feed generators to labelers, to bots and search engines. In the AT ecosystem, there are many different endpoints that seed these firehose APIs. Each PDS serves a stream of all of the activity on the repos it is responsible for. From there, Relays aggregate the streams of each participating PDS into a single unified stream ‚Äî the firehose.

## Firehose

Anyone can connect to the firehose without authentication ‚Äî this is a core feature of the protocol. To get started, open a WebSocket connection to any provider of the `com.atproto.sync.subscribeRepos` endpoint:

```shell 
$ websocat wss://bsky.network/xrpc/com.atproto.sync.subscribeRepos
```

From here, you would need to read each message as it comes in, and decode the associated data. Our [Go](https://github.com/bluesky-social/indigo) SDK is currently the most feature-complete for interacting with the firehose directly.

Bear in mind that firehose output format is one of the more complex parts of AT, involving decoding binary [CBOR](/specs/data-model) data and [CAR](/specs/repository#car-file-serialization) files. Additionally, the volume of data has increased rapidly as the network has grown. The full synchronization firehose is core network infrastructure, but for end users such as [feed developers](/guides/feeds), we provide an alternative streaming solution called **Jetstream.**

## Jetstream

Jetstream has a few key advantages:

- simple JSON encoding
- reduced bandwidth, and compression
- ability to filter by collection (NSID) or repo (DID)

A Jetstream server consumes from the firehose and fans out to many subscribers. It is [open source](https://github.com/bluesky-social/jetstream), implemented in Go, simple to self-host. There is an official client library included (in Go), and [community client libraries](https://skyware.js.org/guides/jetstream/introduction/getting-started/) have been developed.

You can work with Jetstream like any other websocket. Just provide a Jetstream endpoint like `wss://jetstream2.us-east.bsky.network/subscribe`, and a `wantedCollections` parameter, like `?wantedCollections=app.bsky.feed.post`.

Here is a Python example:

```python
import asyncio
import websockets

uri = "wss://jetstream2.us-east.bsky.network/subscribe?wantedCollections=app.bsky.feed.post"

async def listen_to_websocket():
  async with websockets.connect(uri) as websocket:
    while True:
      try:
        message = await websocket.recv()
        print(message)
      except websockets.ConnectionClosed as e:
        print(f"Connection closed: {e}")
        break
      except Exception as e:
        print(f"Error: {e}")

asyncio.get_event_loop().run_until_complete(listen_to_websocket())
```

## Further Reading and Resources

- [Sync](/guides/sync)
- [Backfilling](/guides/backfilling)
- [Feeds](/guides/feeds)
- [Repository spec](/specs/repository)
- [Event Stream spec](/specs/event-stream)
- [Sync spec](/specs/sync)
- The [Microcosm](https://www.microcosm.blue/) community project maintains tools for working with AT records at scale without local mirroring.

---
atproto/src/app/[locale]/guides/subscriptions/en.mdx
---
import {Container} from "@/components/Container"

export const header = {
  title: 'Subscriptions',
  description: 'Subscribing to a labeler service',
}

## Labeler Subscriptions

Labeler services each have a service identity, meaning a DID document. This is the DID that appears in the label source (**`src`**) field.

To include labels from a given labeler, set the `atproto-accept-labelers` header on an XRPC request to a comma-separated list of the DIDs of the labelers you want to include, like so:

```bash
curl -H "atproto-accept-labelers: did:plc:vmt7o7y6titkqzzxav247zrn, did:plc:vmt7o7y6titkqzzxav247zrn"
```

You can also add labelers using the TypeScript SDK:

```tsx
import { Client } from '@atproto/lex'

// Global app-level labelers
Client.configure({
  appLabelers: ['did:plc:labeler1', 'did:plc:labeler2'],
})

// Client-specific labelers
const client = new Client(session, {
  labelers: ['did:plc:labeler3'],
})

// Add labelers dynamically
client.addLabelers(['did:plc:labeler4'])

// Replace all labelers
client.setLabelers(['did:plc:labeler5'])

// Clear labelers
client.clearLabelers()
```

## Use labels in your app

Labels may be placed on accounts or records. Apps can interprets targets in different ways:

- On an account: has account-wide effects
- On a profile: affects only the profile record (e.g. user avatar) and never hides any content, including the account in listings.
- On content: affects the content specifically (a post record, a list, a feed, etc.)

## Reporting

To send a report to a Labeler, use the `com.atproto.moderation.createReport` procedure. Users may send reports to any of their labelers.

To specify which labeler should receive the report, set the `atproto-proxy` header with the DID of the labeler and the service key of `atproto_labeler`. In the TypeScript SDK, it looks like this:

```typescript
import { AtpAgent } from '@atproto/api'

const agent = new AtpAgent({
    service: 'https://bsky.social'
})
await agent.login({
    identifier: 'alex.bsky.team',
    password: 'your-app-password'
})

agent
  .withProxy('atproto_labeler', 'did:web:my-labeler.com')
  .createModerationReport({
    reasonType: 'com.atproto.moderation.defs#reasonViolation',
    reason: 'They were being such a jerk to me!',
    subject: { did: 'did:web:bob.com' },
  })
```

## Further Reading and Resources

- [Moderation](/guides/moderation)
- [Labels](/guides/labels)
- [Creating a labeler](/guides/creating-a-labeler)
- [Label Specs](/specs/label)
- [Using Ozone](/guides/using-ozone)

---
atproto/src/app/[locale]/guides/sync/en.mdx
---
export const header = {
  title: 'Sync',
  description: 'Synchronizing AT Protocol data',
}

## Handles and DIDs

In AT, handles are resolved through [DIDs](/specs/did), also known as [Decentralized Identifiers](https://en.wikipedia.org/wiki/Decentralized_identifier). Currently, two different DID prefixes are supported:
- `did:web`, which is based on the W3C DID standard.
- `did:plc`, which is a DID method developed and implemented by Bluesky. See the [did-method-plc](https://github.com/did-method-plc/did-method-plc) repository for details.

When you sign up for a new account using an Atmosphere app, that app will communicate with a PDS to register a new handle for you, using its domain name (for example, `*.bsky.social`).

## Data Repositories

A data repository is a collection of data published by a single user. Repositories are self-authenticating data structures, meaning each update is signed and can be verified by anyone.

Multiple types of identifiers are used within a data repository:

- [**Decentralized IDs (DIDs)**](https://w3c.github.io/did/) identify data repositories. They are broadly used as user IDs, but since every user has one data repository then a DID can be considered a reference to a data repository.
- [**Namespaced Identifiers (NSIDs)**](/specs/nsid) identify the Lexicon type for groups of records within a repository.
- [**Record Keys**](/specs/record-key) ("rkeys") identify individual records within a collection in a given repository. The format is specified by the collection Lexicon.
- [**Content IDs (CIDs)**](https://github.com/multiformats/cid) identify content using a fingerprint hash. They are used throughout the repository to reference the objects (nodes) within it.

Take, for example, a single AT record:

```
at://did:plc:vmt7o7y6titkqzzxav247zrn/app.bsky.feed.post/3m72rq2hgss2a
```

This can be broken down as `at://DID/NSID/rkey` , or `at://repository/collection/record`. It has a corresponding CID of `bafyreidgbehqwweghrrddfu6jgj7lyr6fwhzgazhirnszdb5lvr7iynkiy`.

When making a write request to an AT API endpoint ‚Äî such as when creating a new post ‚Äî this full repository path will be returned as a response:

```json
{
  "uri": "at://did:plc:u5cwb2mwiv2bfq53cjufe6yn/app.bsky.feed.post/3k4duaz5vfs2b",
  "cid": "bafyreibjifzpqj6o6wcq3hejh7y4z4z2vmiklkvykc57tw3pcbx3kxifpm"
}
```

This is how individual records are uniquely identified and stored within AT data repositories.

## PDS

A PDS, or Personal Data Server, is a server that hosts users' individual data repositories, such as posts, followers, and profile information. The PDS handles account lifecycle features, identity management, and key resolution for hosted user accounts.

You can [host your own PDS](/guides/self-hosting), and [migrate individual accounts](/guides/account-migration) from one PDS to another. Refer to [The AT Stack](/guides/the-at-stack#pds) for more information.

## Further Reading and Resources

There are many ways of synchronizing and streaming AT data. You can use [Jetstream](/guides/streaming-data) to stream real-time updates, or locally [Backfill](/guides/backfilling) the data that already exists on the network.

- [Streaming data](/guides/streaming-data)
- [Backfilling](/guides/backfilling)
- [Feeds](/guides/feeds)
- [Repository spec](/specs/repository)
- [Event Stream spec](/specs/event-stream)
- [Sync spec](/specs/sync)
- The [Microcosm](https://www.microcosm.blue/) community project maintains tools for working with AT records at scale without local mirroring.

---
atproto/src/app/[locale]/guides/the-at-stack/en.mdx
---
import {Container} from "@/components/Container"
import crawling from "./crawling.png"
import did from "./did.png"
import feeds from "./feeds.png"

export const header = {
  title: 'The AT Stack',
  description:
    'Components of the AT Protocol Stack',
}

## The Authenticated Transfer Protocol

The AT Protocol (Authenticated Transfer Protocol) is a standard for public conversation and an open-source framework for building social apps.

It creates a standard format for user identity, follows, and data on social apps, allowing apps to interoperate and users to move across them freely. It is a federated network with account portability.

## Decentralized Architecture

AT syncs repositories in a federated networking model. Federation ensures the network is convenient to use and reliably available. Repository data is synchronized between servers over standard web technologies (HTTP and WebSockets).

[**Personal data servers**, or PDSes](/guides/the-at-stack#pds), are your home in the cloud. They host your data, distribute it, manage your identity, and orchestrate requests to other services to give you your views.

**Relays** handle all of your events, like retrieving large-scale metrics (likes, reposts, followers), content discovery (algorithms), and user search.

<Container>
  <Image src={crawling} alt="" className="w-full max-w-md mx-auto" />
</Container>

The AT Protocol is architected in a ‚Äúbig world with small world fallbacks‚Äù way, modeled after the open web itself. With the web, individual computers upload content to the network, and then all of that content is then broadcasted back to other computers. Similarly, with the AT Protocol, we're sending messages to a much smaller number of big aggregators, which then broadcast that data to personal data servers across the network.

Technically, you can implement an AT application with just a single PDS ‚Äî Relays are not a necessary part of the specification. They enable the ecosystem to scale for big-world social networks.

## PDS

PDS instances host accounts for users, which require account management and lifecycle controls similar to any network server. While AT identities (DIDs and handles) can in theory be entirely separate from the PDS, in practice the PDS is expected to help manage the user's identity. 

You can [host your own PDS](/guides/self-hosting).

User data is stored in signed data repositories and verified by DIDs. Signed data repositories are like Git repos but for database records, and DIDs are essentially registries of user certificates, similar in some ways to the TLS certificate system. They are expected to be secure, reliable, and independent of the user's PDS.

<Container>
  <Image src={did} alt="" className="w-full max-w-md mx-auto" />
</Container>

The PDS handles:

- account lifecycle: signup, deletion, migration
- account security: email verification, password reset flow, change email flow
- AT identity resolution: DIDs, handles. [Read more](https://fusectore.dev/2025/01/04/atproto-identity.html) about how identity resolution.
- storage of preferences and private state
- ability to takedown or suspend accounts, and handle moderation contacts
- secret key management: AT signing key, and PLC rotation key
- [PLC](https://web.plc.directory/) identity operations and endpoints: change handle; validate, sign, and submit PLC ops
- email delivery: for reset flows, as well as forwarding moderation mail
- output `#identity` and `#account` (coming soon) events on repo event stream
- usually the ability to manage a default handle namespace (base domain)
- OAuth client flows, including web sign-in pages, and optional MFA

You can migrate an individual account's data repository freely from one PDS to another. This involves the import and export of repositories as CAR files. There are some community tools that make this easier:

- https://atproto.at/, which lets you browse all the data stored in your own repository (and what PDS it's currently hosted on)
- https://pdsmoover.com/, a collection of tools for PDS migration
- https://boat.kelinci.net/, a set of interfaces for repository import and export
- https://github.com/bluesky-social/goat, our Go AT command-line tool.

## Tap

Tap is used for syncing existing records from the AT network and then continuing to receive new records from the firehose.

Tap simplifies AT sync by handling the firehose connection, verification, backfill, and filtering. Your application connects to a Tap and receives simple JSON events for only the repos and collections you care about, no need to worry about binary formats for validating cryptographic signatures. Tap features:

- verifies repo structure, MST integrity, and identity signatures
- automatic backfill: fetches full repo history from PDS when adding new repos
- filtered output: by DID list, by collection, or full network mode
- ordering guarantees: live events wait for historical backfill to complete
- delivery modes: WebSocket with acks, fire-and-forget, or webhook
- single Go binary
- SQLite or Postgres backend
- designed for moderate scale (millions of repos, 30k+ events/sec)

There is a Typescript library for working with Tap at [@atproto/tap](https://www.npmjs.com/package/@atproto/tap).

## App Views and Relay

**Relays** handle all of your events, like retrieving large-scale metrics (likes, reposts, followers), content discovery (algorithms), and user search. The Relay also handles [rate limiting](https://docs.bsky.app/docs/advanced-guides/rate-limits) of individual PDSes to ensure that content can be indexed from across the network.

An App View is the piece that actually assembles your feed and all the other data you see in the app, and is downstream from a Relay's firehose of data. This is a semantically-aware service that produces aggregations across the network and views over some subset of the network. For example, the Relay might crawl to grab data such as a certain post's likes and reposts, and the app view will output the count of those metrics.

There can be an ecosystem of App Views for each lexicon deployed on the network. For example, *Bluesky* currently supports a micro-blogging mode: the `app.bsky` Lexicon. Developers who create new lexicons could deploy a corresponding App View that understands their Lexicon to service their users. Other lexicons could include [video](https://stream.place/), or [long-form blogging](https://leaflet.pub/), or [collaborative coding](https://tangled.org/). By bootstrapping off of an existing Relay, data collation will already be taken care of for these new applications. They can benefit from the critical mass of users already posting on Bluesky and populating their repositories with other Lexicons; they only need to provide the indexing behaviors necessary for their application.

App Views and Relays are both application-level tooling, rather than inherent parts of the AT protocol. Our App View implementation responds to [XRPC](/specs/xrpc) calls ‚Äî our HTTP API.

You can find a well-documented example of other App View and Relay implementations in [The Blacksky Architecture](https://blackskyweb.xyz/overview/#blacksky-architecture). Different Atmosphere applications may make different implementation choices.

## DID PLC

[DID PLC](https://web.plc.directory/) is a self-authenticating DID which is strongly-consistent, recoverable, and allows for key rotation. Each identity is made up of a series of signed operations that each reference the preceding operation in a chain back to the genesis operation. Any observer can verify the chain of operations in order to verify the current state of the identity.

PLC operations are stored in a directory (currently run at [plc.directory](http://plc.directory)) which is a centralized service that validates operations, stores them, and serves those operations as well as computed views of the identities derived from the operations. The trust model for the directory is fairly limited as it is unable to modify identities without a signed operation on their behalf.

For guidance on PLC mirroring, refer to [this developer blog](https://dholms.leaflet.pub/3m6zswymcqk2p).

## Feeds

As with Web search engines, users are free to select their aggregators. Feeds, search indices, and entire app views can be provided by independent third parties, with requests routed by the PDS based on user configuration.

<Container>
  <Image src={feeds} alt="" className="w-full max-w-md mx-auto" />
</Container>

Custom feeds, or feed generators, are services that provide custom algorithms to users through the AT Protocol. This allows users to choose their own timelines, whether it's an algorithmic For You page or a feed of entirely cat photos.

Custom feeds work straightforwardly: the server receives a request from a user's server and returns a list of post URIs with some optional metadata attached. Those posts are then hydrated into full views by the requesting server and sent back to the client.

A Feed Generator service can host one or more algorithms. The service itself is identified by DID, while each algorithm that it hosts is declared by a record in the repo of the account that created it. For instance, feeds offered by Bluesky will likely be declared in `@bsky.app`'s repo. Therefore, a given algorithm is identified by the at-uri of the declaration record. This declaration record includes a pointer to the service's DID along with some profile information for the feed.

[Browse](https://blueskydirectory.com/feeds/all) custom feeds.

## Osprey and Ozone

Moderation is handled by two parts of our stack: [*Osprey*](https://github.com/roostorg/osprey), an event stream decisions engine and analysis UI designed to investigate and take automatic action; and [*Ozone*](/guides/using-ozone), a labeling service and web frontend for making moderation decisions.

## Tools and SDKs

Most AT tooling is implemented in Go and TypeScript.

- The [indigo](https://github.com/bluesky-social/indigo) repository contains most of our Go tooling, including the code for the [Relay](https://github.com/bluesky-social/indigo/blob/main/cmd/relay/README.md) and the Go SDK.
- There's also https://github.com/bluesky-social/goat, our Go AT command-line tool.
- The https://github.com/bluesky-social/atproto repository contains most of our TypeScript tooling, including the code for the [PDS](https://github.com/bluesky-social/atproto/tree/main/packages/pds) and the TypeScript SDK.
- There are many other community tools that we use every day. For example, the [Microcosm](https://www.microcosm.blue/) tooling, written in Rust, provides a set of APIs to help build query layers without requiring dedicated AppViews! Refer to https://constellation.microcosm.blue/ for details.

---
atproto/src/app/[locale]/guides/tutorials/en.mdx
---
import {Container} from "@/components/Container"

export const header = {
  title: 'Tutorials',
  // description: 'AT Protocol Tutorials',
}

These tutorials will guide you through building applications using the AT Protocol. Each tutorial focuses on a specific use case or feature, providing step-by-step instructions and code examples.

Our tutorials are organized to be completed in order, but each tutorial is self-contained and can be followed independently.

## Build-a-Bot

Create an agent that can interact with the AT Protocol, post content, and respond to user actions.

[**Build a Bot Tutorial**](/guides/bot-tutorial)

## Custom Feed Algorithms

Learn how to create and deploy custom feeds to curate content for users based on specific criteria.

[**Custom Feed Tutorial**](/guides/custom-feed-tutorial)

## OAuth with NextJS

Build a Next.js app where users can log in with their AT Protocol identity using OAuth.

See a demo version running at: [https://nextjs-oauth-tutorial.up.railway.app/](https://nextjs-oauth-tutorial.up.railway.app/).

[**OAuth with NextJS Tutorial**](/guides/oauth-tutorial)

## Statusphere Example App

Build an app that lets you broadcast a "status" message using custom Lexicons, and see a live feed of other statuses.

This is a revision of our original Statusphere tutorial.

[**Statusphere Example App Tutorial**](/guides/statusphere-tutorial)

---
atproto/src/app/[locale]/guides/understanding-atproto/en.mdx
---
import { Card, CardParagraph, CardTitle } from '@/components/Card'

export const header = {
  title: 'Understanding Atproto',
  description: 'A closer look into how it all works',
}

## TL;DR

Atproto is big-world, open social. Users publish JSON records into repositories. The changestreams of those records then sync across the network to drive applications.

We recommend these fantastic articles by community member [Dan Abramov](https://bsky.app/profile/danabra.mov):

- [**Open Social**](https://overreacted.io/open-social/) - The protocol is the API.
- [**Where it's at://**](https://overreacted.io/where-its-at/) - From handles to hosting.
- [**A Social Filesystem**](https://overreacted.io/a-social-filesystem/) - Formats over apps.

## Core primitives

- **User repos.** The public per-user databases.
- **User handles.** Usernames, which are DNS records. Our account is [@atproto.com](https://bsky.app/profile/atproto.com).
- **User DIDs.** The permanent IDs of users.

## Data models

- **Records**. JSON, categorized into collections.
- **Blobs**. The [images and videos](/guides/images-and-video).
- **Lexicon**. The schema language.
- **Lexicon RPC (XRPC).** It's HTTPS but with the routes defined by Lexicons.

## The stack

There are different kinds of services which you can [self-host](/guides/self-hosting):

- **Personal Data Servers (PDS).** They host user accounts.
- **Relays**. They collect and rebroadcast user write events.
- **Applications**. They aggregate user data to produce app experiences.
- **Labelers**. Publish moderation decisions as label metadata.

## Making it happen

Let's assume you're building an application. Your users will sign in with [OAuth](/guides/auth), which hands your application the address of their PDS. Your application then [reads & writes](/guides/reads-and-writes) records by contacting their PDS.

To listen for activity across the network, you [sync from relays](/guides/sync). You could sync directly from the users' PDSes, but it's more convenient to use the relays. Since all data is signed, you can confirm the relay's stream is accurate.

The [Lexicons](/guides/lexicon) help identify the data being published on the network. You use them to validate data as you read it, just like you validate incoming HTTP requests.

Finally, you handle [moderation](/guides/moderation) by subscribing to (or running) labelers which receive user reports and publish labels on user content.

## Deeper reads

We also recommend these deep-dive articles by the team:

- [**Atproto for distributed systems engineers**](/articles/atproto-for-distsys-engineers). An explainer for backend thinkers.
- [**The Atproto Ethos**](/articles/atproto-ethos). The principles that underpin the design of atproto.



---
atproto/src/app/[locale]/guides/using-ozone/en.mdx
---
import {Container} from "@/components/Container"
import acknowledged from "./acknowledged.png"
import confirm from "./confirm.png"
import credentials from "./credentials.png"
import eventstream from "./event-stream.png"
import labeler from "./labeler-account-view.png"
import login from "./login.png"
import actions from "./moderation-actions.png"
import reported from "./ozone-post-reported.png"
import report from "./report-post.png"
import reporting from "./reporting-interface.png"
import repositories from "./search-repositories.png"
import welcome from "./welcome.png"
import plc from "./plc-update.png"

export const header = {
  title: 'Using Ozone',
  description: 'Working with the Ozone moderation interface',
}

## Getting started

Ozone is a web-based moderation interface that allows you to run your own Labeler service, and review and content reports efficiently. This guide will walk you through installing and using Ozone.

## Installing Ozone

Ozone should be run on a dedicated VPS, from a cloud provider like [OVHcloud](https://us.ovhcloud.com/vps/), [DigitalOcean](https://digitalocean.com/), or [Vultr](https://vultr.com/). 1-2GB of memory and 1-2 CPU cores is sufficient to get started.

**To install Ozone**, refer to [HOSTING.md](https://github.com/bluesky-social/ozone/blob/main/HOSTING.md) in the [Ozone GitHub repository](https://github.com/bluesky-social/ozone).

<Note>
Before setting up your Ozone service you should create a new account on the network, separate from your main account. This is the account that subscribers to your labeler will interact with: accounts for labelers appear differently in Bluesky than normal accounts.
</Note>

After completing the server-side installation, you can access the Ozone web interface by navigating to your server's IP address or domain name in a web browser:

<Container>
  <Image src={login} alt="" className="w-full max-w-md mx-auto" />
</Container>

From here, you can begin configuring your Labeler service.

## Configuring Ozone

First, provide the credentials of the account you created for your labeler service:

<Container>
  <Image src={credentials} alt="" className="w-full max-w-md mx-auto" />
</Container>

Make sure to provide your full handle, including the `.bsky.social` suffix (or your custom domain if applicable).

Next, you'll be prompted to update your account registration with the [PLC](https://web.plc.directory/), to mark this account as a moderation service. This will trigger an email verification. Verify that you are using the correct account and domain name, and then provide the verification code:

<Container>
  <Image src={plc} alt="" className="w-full max-w-md mx-auto" />
</Container>

Finally, confirm that you will be using this account as a labeler service. This publishes an [AT Record](https://atproto.at/viewer?uri=did:plc:wwjhjtgw5ksxrzdswtvnsnjs/app.bsky.labeler.service/self) of the type `app.bsky.labeler.service` to your account.

<Container>
  <Image src={confirm} alt="" className="w-full max-w-md mx-auto" />
</Container>

Now your Ozone instance is set up and ready to use!

<Container>
  <Image src={welcome} alt="" className="w-full max-w-md mx-auto" />
</Container>

## Submitting and managing reports

Now, you'll create and action an example moderation report. In Bluesky or another application, sign back in to a regular (non-labeler) account. Then, navigate to the account page for your labeler service.

<Container>
  <Image src={labeler} alt="" className="w-full max-w-md mx-auto" />
</Container>

Click the "Subscribe to Labeler" button to subscribe your account to the labeler service. This allows your account to submit reports to this labeler, and receive labels from it.

Next, navigate to a post you wish to report, and select "Report Post":

<Container>
  <Image src={report} alt="" className="w-full max-w-md mx-auto" />
</Container>

Because this is just a tutorial, we're going to report a post from Bluesky's own team for being too enthusiastic. This would not normally be a reason to report a post, but it works for our example!

<Container>
  <Image src={reporting} alt="" className="w-full max-w-md mx-auto" />
</Container>

Fill out the reporting interface as desired. Make sure select your labeler as the moderation service to send the report to. Then, submit the report.

Now, return to the Ozone interface. You should see the new report listed in the *Reports* section:

<Container>
  <Image src={reported} alt="" className="w-full max-w-md mx-auto" />
</Container>

Click on the report to view its details. From here, you can review the reported content and take appropriate moderation actions.

<Container>
  <Image src={actions} alt="" className="w-full max-w-md mx-auto" />
</Container>

In this case, the most straightforward action is to acknowledge the report without taking further action. Select "Ackonwledge" from the drop-down, provide a note, then submit the action. You can use keyboard shortcuts to speed up moderation actions ‚Äî for example, `S` for Submit.

<Container>
  <Image src={acknowledged} alt="" className="w-full max-w-md mx-auto" />
</Container>

If you had wanted to mute the user for subscribers to your labeler, you could have selected "Mute" from the action drop-down instead. Refer to [Ozone Concepts](https://github.com/bluesky-social/ozone/blob/main/docs/userguide.md#concepts) for an explanation of each action.

You can review the event stream of reports and actions in the *Events* section:

<Container>
  <Image src={eventstream} alt="" className="w-full max-w-md mx-auto" />
</Container>

Or, use the *Repositories* section to search reports by user, content, or action:

<Container>
  <Image src={repositories} alt="" className="w-full max-w-md mx-auto" />
</Container>

## Next steps

You now have an understanding of the Ozone moderation flow. Ozone also allows you to create additional user accounts with different permissions, to help manage your moderation workload. To aggregate the data from your Ozone instance, see the [#OzoneInsights](https://bsky.app/search?q=%23ozoneinsights) feed for solutions.

## Further Reading and Resources

- [Moderation](/guides/moderation)
- [Labels](/guides/labels)
- [Creating a labeler](/guides/creating-a-labeler)
- [Subscriptions](/guides/subscriptions)
- [Label Specs](/specs/label)

---
atproto/src/app/[locale]/guides/video-handling/en.mdx
---
export const header = {
  title: 'Video Handling',
  description: 'Handling video uploads and processing',
}

## Video uploads

Video uploads differ slightly from image uploads, since they are typically much larger files and have a significantly long processing time. Individual Atmosphere apps or CDNs may impose additional limits on video uploads.

For this reason, video uploads are typically handled by a side-car service before the final file(s) are uploaded to the PDS as blobs. For example, Bluesky has the `https://video.bsky.app` service for handling the upload and transcoding of videos before they are written to the PDS as a blob.

## Aspect ratios

Depending on the requirements of your application, you may need to supply an aspect ratio along with video uploads. This metadata can be tricky to compute. If you're developing in a browser, you can load the video into a `<video>` and observe the dimensions when loaded. If you're in a native app, you'll most likely be able to get it via the media picker APIs. Alternatively, you can use a tool like `ffprobe` :

```jsx
import { ffprobe } from "https://deno.land/x/fast_forward@0.1.6/ffprobe.ts";

export async function getAspectRatio(fileName: string) {
  const { streams } = await ffprobe(fileName, {});
  const videoSteam = streams.find((stream) => stream.codec_type === "video");
  return {
    width: videoSteam.width,
    height: videoSteam.height,
  };
}
```

## Transcoding

Videos will need time to transcode using a tool like `ffmpeg` on the server side before they are available from the firehose. This can lead to UX gaps ‚Äî if a video blob is accompanying a new post, the video service only knows about the video once the post appears in the firehose. This means that people will be able to see the post before processing is complete, which will show a missing video for several seconds.

It is possible to design an application endpoint to allow submitting videos to a separate microservice for processing before creating a post. This method also allows for better UX, since you can show the processing state to the user and let them know if the processing job fails.

There are client-side code samples for both methods here:

- [Simple Method](https://tangled.sh/strings/did:plc:p2cp5gopk7mgjegy6wadk3ep/3lw74be4dr422)
- [Video Service](https://tangled.sh/strings/did:plc:p2cp5gopk7mgjegy6wadk3ep/3lw743ejno722)

## Further Reading and Resources

- [Images and Video](/guides/images-and-video)
- [Blob lifecycle](/guides/blob-lifecycle)
- [Blob security](/guides/blob-security)
- [Blob Specs](/specs/blob)
- [Streamplace](https://stream.place/docs/) provides an implementation and related Lexicon for video streaming on ATProto.


---
atproto/src/app/[locale]/guides/writing-data/en.mdx
---
export const header = {
  title: 'Writing Data',
  description: 'Write AT Protocol Records',
}

## Writing Data

Writing data ‚Äî for example, by posting to a Bluesky feed ‚Äî works similarly to [reading data](/guides/reading-data), but with different methods.

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
To create a new record, call `client.create()` with an imported Lexicon, along with any required fields:

```tsx
import * as app from './lexicons/app.js'

const result = await client.create(app.bsky.feed.post, {
  text: 'Hello, world!',
  createdAt: new Date().toISOString(),
})
```

`text` and `createdAt` are required values under the [`app.bsky.feed.post` Lexicon](https://atproto.blue/en/latest/atproto/atproto_client.models.app.bsky.feed.post.html).

<Note>
This will post using the currently-authenticated user session. If you want to post as another user, reuse [`agent.login()`](/guides/reads-and-writes#creating-a-client) to get a new user session.
</Note>

The API will respond with the `at://` URI of the post and a content hash of the post as the `cid`:

```tsx
console.log(result.uri) // at://did:plc:...
console.log(result.cid)
```
    </TabPanel>
    <TabPanel value="go">
To create a new record, call `RepoCreateRecord()` with your target Lexicon and any required fields:

```go
now := time.Now().UTC().Format(time.RFC3339)

post := &appbsky.FeedPost{
    Text:      "Hello from Go!",
    CreatedAt: now,
}

resp, _ := comatproto.RepoCreateRecord(ctx, client, &comatproto.RepoCreateRecord_Input{
    Repo:       client.Auth.Did,         // handle or DID
    Collection: "app.bsky.feed.post",    // lexicon
    Record:     &lexutil.LexiconTypeDecoder{Val: post},
})
```
`Text` and `CreatedAt` are required values under the [`app.bsky.feed.post` Lexicon](https://atproto.blue/en/latest/atproto/atproto_client.models.app.bsky.feed.post.html).

<Note>
This will post using the currently-authenticated user session. If you want to post as another user, reuse [`ServerCreateSession()`](/guides/sdk-auth#creating-a-client) to get a new user session.
</Note>

The API will respond with the `at://` URI of the post and a content hash of the post as the `cid`:

```go
fmt.Printf("Created post: %s (CID: %s)\n", resp.Uri, resp.Cid)
```
    </TabPanel>
  </TabPanels>
</TabGroup>    

```
{
  "uri": "at://did:plc:u5cwb2mwiv2bfq53cjufe6yn/app.bsky.feed.post/3k4duaz5vfs2b",
  "cid": "bafyreibjifzpqj6o6wcq3hejh7y4z4z2vmiklkvykc57tw3pcbx3kxifpm"
}
```

If you are building a client or another interface that both reads and writes data, it is important to ensure that you read your own writes using this response. 

## Read-after-Write

Depending on how an AT app frontend is designed, a user may take some action (such as updating their profile), rapidly refresh the view, and find that their recent change is not immediately reflected in the updated response.

This is because services such as the App View do not have transactional writes from a user's PDS. Therefore the views that they calculate are eventually consistent. In other words, a user may have created a record on their PDS that is not yet reflected in the API responses provided by the App View.

Because all requests from the application are sent to the user's PDS, the PDS is in a position to smooth over this behavior. [Our PDS distribution](https://github.com/bluesky-social/pds) provides some basic read-after-write behaviors by looking at response headers from the App View, determining if there are any new records that are not in the response, and modifying the response to reflect those new records.

The App View communicates the current state of its indices by setting the `Atproto-Repo-Rev` response header. This is set by the `rev` of the most recent commit that's been indexed from the requesting user's repository. If the PDS sees this header on a response, it will search for all records that it has locally that are more recent than the provided rev and determine if they affect the App View's response.

This read-after-write behavior *only* applies to records from the user making the request. Records from other users that happen to be on the same PDS will not affect the requesting user's response.

## Updating Records

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
To update an existing record, like a user's profile, use `client.put()`:

```bash
lex install app.bsky.actor.profile
lex build
```

```tsx
import * as app from './lexicons/app.js'

await client.put(app.bsky.actor.profile, {
  displayName: 'New Name',
  description: 'Updated bio',
})
```
    </TabPanel>
    <TabPanel value="go">
To update an existing record, like a user's profile, use `RepoPutRecord()`:

```go
newProfile := &appbsky.ActorProfile{
  DisplayName: ptrString("New Display Name"),
  Description: ptrString("Updated bio"),
}

resp, _ := comatproto.RepoPutRecord(ctx, client, &comatproto.RepoPutRecord_Input{
  Repo:       client.Auth.Did,                // handle or DID
  Collection: "app.bsky.actor.profile",       // lexicon
  Rkey:       "self",                         // rkey
  Record:     &lexutil.LexiconTypeDecoder{Val: newProfile},
})

fmt.Printf("Updated: %s (CID: %s)\n", resp.Uri, resp.Cid)
```

`RepoPutRecord` is idempotent ‚Äî it'll create the record if it doesn't exist, or overwrite it if it does.
    </TabPanel>
  </TabPanels>
</TabGroup>

## Deleting Records

<TabGroup defaultValue="ts">
  <TabList>
    <Tab value="ts">TypeScript</Tab>
    <Tab value="go">Go</Tab>
  </TabList>
  <TabPanels>
    <TabPanel value="ts">
To delete a record, like an individual post, use `client.delete()`:

```bash
lex install app.bsky.actor.post
lex build
```

```tsx
import * as app from './lexicons/app.js'

await client.delete(app.bsky.feed.post, {
  rkey: '3jxf7z2k3q2',
})
```
    </TabPanel>
    <TabPanel value="go">
    To delete a record, like an individual post, use `RepoDeleteRecord():`
```go
comatproto.RepoDeleteRecord(ctx, client, &comatproto.RepoDeleteRecord_Input{
    Repo:       client.Auth.Did,      // handle or DID
    Collection: "app.bsky.feed.post", // lexicon
    Rkey:       "3mbl5xdw4yk2r",      // rkey
})
```
    </TabPanel>
  </TabPanels>
</TabGroup>

## Further Reading and Resources

- [Reads and Writes](/guides/reads-and-writes)
- [Reading data](/guides/reading-data)
- [Accounts and deletions](/guides/account-lifecycle)
- [Social graph](/guides/social-graph)
- [Record Key spec](/specs/record-key)
- [URI scheme](/specs/at-uri-scheme)

---
